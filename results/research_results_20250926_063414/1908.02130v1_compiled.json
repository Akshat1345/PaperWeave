{
  "metadata": {
    "title": "Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning",
    "authors": [
      "Aras R. Dargazany"
    ],
    "abstract": "The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately.",
    "published": "2019-07-30T16:57:38Z",
    "arxiv_id": "1908.02130v1",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02130v1",
    "pdf_file": "data/pdfs/1908.02130v1.pdf",
    "pdf_filename": "1908.02130v1.pdf"
  },
  "processing_info": {
    "processed_at": "2025-09-26T06:33:40.094376",
    "is_large_pdf": false,
    "sections_found": 2,
    "tables_found": 0,
    "images_found": 2
  },
  "sections_text": {
    "Abstract": "The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artiﬁcial cortical column ultimately. Past: Deep learning inspirations Deep learning horizon, landscape and research roadmap in nutshell is presented in this ﬁgure . The historical development and timeline of deep learning & neural network is separately illustrated Figure 1: Deep learning research landscape & roadmap: past, present, future. The future is highlighted as deep cortical learning. in ﬁgure . The Origin of neural nets [ WR17 ] is thoroughly reviewed in terms of the evolutionary history of deep learning models. Vernon Mountcastle discovery of cortical columns in somatosensory cortex [ Mou97 ] was a breakthrough in brain science. The big bang was the discovery of Hubel & Wiesel of simple cells and complex cell in visual cortex [ HW59 ] which won the Nobel prize for this discovery in 1981. This work was heavily founded on Vernon Mountcastle discovery of cortical columns in somatosensory cortex [ Mou97 ]. After the discovery of Hubel & Wiesel, Fukushima proposed a pattern recognition architecture based on the simple cell and complex cell discovery, known as NeoCognitron [ FM82 ]. In this work, a deep neural network was proposed using simple cell layer and complex cell layer repeatedly. In 80s and maybe a bit earlier backpropagation have been proposed by multiple people but the ﬁrst time it was well-explained and applied for learning neural nets was done by Hinton and his colleagues in [ RHW86 ]. arXiv:1908.02130v1 [cs.NE] Jul Figure 2: Neural nets origin, timeline & history made by Favio Vazquez Present: Deep learning by LeCun, Bengio and Hinton Convolutional nets was invented by LeCun [ LBD + ] which led to deep learning conspiracy which also started by the three founding fathers of the ﬁeld: LeCun, Bengio and Hinton [ LBH15 ]. The main hype in deep learning happened in when the state-of-the-art result in Imagenet classiﬁcation and TIMIT speech recognition task were dramatically reduced using an end-to-end deep convolutional network [ KSH12 ] and deep belief net [ HDY + ]. The power of deep learning is scalability and the ability to learn in an end-to-end fashion. In this sense, deep learning architectures are capable of learning big datasets such as Imagenet [ KSH12 , GDG + ] and TIMIT using multiple GPUs in an end-to-end fashion meaning directly from raw inputs, all the way the desired outputs. Alexnet [ KSH12 ] used two GPUs for Imagenet classiﬁcation which is a very big dataset of images, almost 1. million images of size 215x215. Kaiming He et al. [ GDG + ] proposed a highly scalable approach for training on Image using",
    "256 GPUs for almost an hour which shows an amazingly powerful approach based stochastic": "gradient descent for applying big cluster of GPUs on huge datasets. Very many application domains have been revolutionized using deep learning architectures such as image classiﬁcations [ KSH12 ], machine translation [ WSC + , JSL + ], speech recognition [ HDY + ], and robotics [ MKS + ]. The Nobel Prize in Physiology or Medicine was given to John O’Keefe, May-Britt Moser and Edvard I. Moser “for their discoveries of cells that constitute a positioning system in the brain.” [ Bur14 ]. This study of cognitive neuroscience shed light on how the world is represented within the brain. Hinton’s Capsule network [ SFH17 ] and Hawkins’ cortical learning algorithm [ HAD11 ] are highly inspired by this Nobel-prize winning work [ Bur14 ]. Future: Brain-plausible deep learning & cortical learning algorithms The main direction and inclination in the deep learning for future is the ability to bridge the gap between the cortical architecture and deep learning architectures, speciﬁcally convolutional nets. In this quest, Hinton proposed capsule network [ SFH17 ] as an eﬀort to get rid of pooling layers and replace it with capsules which are highly inspired bu cortical mini-columns in cortical columns and layers and include the location information or pose information of parts. Another important quest in deep learning is understanding the biological root of learning in our brain, speciﬁcally in our cortex. Backpropagation is not biologically inspired and plausible. Hinton and the other founding fathers of deep learning have been trying to understand how backprop might be feasible biologically in brain. Feedback alignment [ LCTA16 ] and spike time-dependent plasticity or STDP-based backprop [ BSR + ] are some of the works which have been done by Timothy Lillicrap, Blake Richards, and Hinton in order to model backprop biologically based on the pyramidal neuron in the cortex. In the far future, the main goal should be the merge of two very independent quest to build cortical structure in our brain: The ﬁrst one is heavily target by the big and active deep learning community; The second one is targeted independently and neuroscientiﬁcally by Numenta and Geoﬀ Hawkins [ HAD11 ]. These people argue that the cortical structure and our neocortex is the main source of our intelligence and for building a true intelligent machine, we should be able to reconstruct the cortex and to do so, we should ﬁrst focus more on the cortex and understand what cortex is made out of. Finale: Deep cortical learning as the merge of deep learning and cortical learning By merging deep learning and cortical learning, a very more focused and detailed architectures, named deep cortical learning might be created. We might be able to understand and reconstruct the cortical structure with much more accuracy and have a better idea what the true intelligence is and how artiﬁcial general intelligence or AGI might be reproducible. Deep cortical learning might be the algorithm behind one cortical column in the neocortex."
  },
  "sections_summary": {
    "Abstract": "The past and future of deep learning are connected to the concept of cortical columns in the brain, which were first discovered by Vernon Mountcastle. The development of deep learning models was influenced by this discovery, with notable milestones including:\n\n- The proposal of simple cell layers and complex cell layers by Fukushima (NeoCognitron)\n- The use of backpropagation in neural networks by Hinton and colleagues\n- The invention of convolutional nets by LeCun\n\nThe present state of deep learning is characterized by its scalability and ability to learn from large datasets, with notable achievements including:\n\n- State-of-the-art results in ImageNet classification and TIMIT speech recognition tasks\n- The use of end-to-end deep convolutional networks and multiple GPUs for training on large datasets\n\nThe future of deep learning is predicted to be the convergence of deep learning and cortical learning, resulting in artificial cortical columns.",
    "256 GPUs for almost an hour which shows an amazingly powerful approach based stochastic": "Gradient descent can apply big clusters of GPUs to large datasets. Deep learning has revolutionized many domains, including image classification, machine translation, and speech recognition. Nobel Prize winners John O'Keefe and May-Britt Moser discovered brain cells that represent spatial positioning. Future directions in deep learning aim to bridge the gap between cortical architecture and deep networks, with proposed solutions like capsule networks and feedback alignment. A new goal is to merge deep learning with cortical learning to create \"deep cortical learning,\" potentially leading to more accurate understanding of intelligence and artificial general intelligence (AGI)."
  },
  "tables": [],
  "images": [
    "processed/images/1908.02130v1_page1_img0.png",
    "processed/images/1908.02130v1_page2_img0.png"
  ],
  "status": "completed"
}