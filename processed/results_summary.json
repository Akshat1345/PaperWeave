{
  "results": [
    {
      "metadata": {
        "title": "Implementing Performance Portability of High Performance Computing   Programs in the New Golden Age of Chip Architecture",
        "authors": [
          "Weifeng Liu",
          "Linping Wu",
          "Xiaowen Xu",
          "Yuren Wang"
        ],
        "abstract": "As an important goal of high-performance computing, the concept of performance portability has been around for many years. As the failure of Moore's Law, it is no longer feasible to improve computer performance by simply increasing the number of existing hardware. The innovation of high performance computer is imperative, which makes high-performance computers with multiple architectures coexist in the production environment. For example, current high-performance computing nodes often use co-accelerators such like general-purpose GPUs and Intel Xeon Phis to accelerate general-purpose processors. With the flourishing of deep learning, dedicated neural network acceleration chips are also arising. The emergence of co-accelerators with different architectures and their wide application in high-performance computers have challenged the performance portability of programs between high-performance computers with different architectures. This article summarizes the current performance portability technology from the programming model, serial code automatic parallelization, parallel code automatic conversion, etc. at the end of the article, it also summarizes how to use scientific computing function libraries to improve performance and performance portability of a program. Different application scenarios need different implementation technologies to get performance portability. Program developers choose performance portability solutions for their programs. In fact, they balance programming efficiency and optimization effects under various constraints.",
        "published": "",
        "arxiv_id": "2308.13802v1",
        "categories": [
          "cs.AR"
        ],
        "pdf_url": "http://arxiv.org/pdf/2308.13802v1",
        "pdf_file": "data/pdfs/2308.13802v1.pdf",
        "pdf_filename": "2308.13802v1.pdf"
      },
      "processing_info": {
        "processed_at": "2025-11-11T10:29:14.918978",
        "is_large_pdf": true,
        "sections_found": 1,
        "tables_found": 2,
        "images_found": 9
      },
      "sections_text": {
        "Abstract": "\u5916\u5316\u5185\u4e0d\u5316\uff0c\u4e58\u7269\u4ee5\u6e38\u5fc3\uff1a\u5728\u82af\u7247\u4f53\u7cfb\u7ed3\u6784\u7684\u65b0\u9ec4\u91d1\u65f6\u4ee3\u5b9e\u73b0\u9ad8\u6027 \u80fd\u8ba1\u7b97\u7a0b\u5e8f\u7684\u6027\u80fd\u53ef\u79fb\u690d \u5218\u4f1f\u5cf0 \u6b66\u6797\u5e73 \u5f90\u5c0f\u6587 \u738b\u6631\u4eba \u4e2d\u56fd\u5de5\u7a0b\u7269\u7406\u7814\u7a76\u9662 \u5317\u4eac\u5e94\u7528\u7269\u7406\u4e0e\u8ba1\u7b97\u6570\u5b66\u7814\u7a76\u6240 1) \u5f15\u8a00 1.1\u53d1\u5c55\u80cc\u666f\u4ecb\u7ecd \u5e84\u5b50\u5728\u300a\u77e5\u5317\u6e38\u300b\u548c\u300a\u4eba\u95f4\u4e16\u300b\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u751f\u4ef7\u503c\u89c2\uff1a\u201c\u5916\u5316\u800c\u5185\u4e0d\u5316\uff0c\u4e58\u7269\u4ee5 \u6e38\u5fc3\u201d\u3002\u8fd9\u662f\u8bf4\u4e3a\u4eba\u5904\u4e8b\uff0c\u5e94\u8be5\u4fdd\u6301\u5185\u5fc3\u7684\u5e84\u91cd\u548c\u79c9\u6301\uff0c\u53c8\u80fd\u5e73\u9759\u6e29\u548c\u7684\u987a\u5e94\u81ea\u5df1\u6240\u5904\u7684 \u73af\u5883\uff0c\u53ea\u6709\u8fd9\u6837\u624d\u80fd\u591f\u5b9e\u73b0\u7cbe\u795e\u7684\u81ea\u7531\u548c\u89e3\u653e\u3002\u5e84\u5b50\u7684\u8fd9\u53e5\u8bdd\u5bf9\u4e8e\u7b97\u6cd5\u6216\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u6765 \u8bf4\u540c\u6837\u6709\u7740\u91cd\u8981\u610f\u4e49\u3002\u4e00\u4e2a\u7b97\u6cd5\u6216\u7a0b\u5e8f\u5982\u679c\u5728\u8de8\u8d8a\u591a\u79cd\u7cfb\u7edf\u65f6\u4ecd\u80fd\u81ea\u52a8\u9002\u5e94\u7cfb\u7edf\u67b6\u6784\uff0c\u5145 \u5206\u53d1\u6325\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u6027\u80fd\u7684\u53ef\u79fb\u690d\uff0c\u90a3\u4e48\u5b83\u4eec\u5c31\u66f4\u6709\u53ef\u80fd\u83b7\u5f97\u5e7f\u6cdb\u5e94\u7528\uff0c\u521b\u9020\u51fa\u66f4\u5927 \u7684\u4ef7\u503c \u3002\u9ad8\u6027\u80fd\u8ba1\u7b97\u5df2\u7ecf\u7ecf\u5386\u4e86\u6570\u4e2a\u9636\u6bb5\uff0c\u4f5c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u4e00\u4e2a\u91cd\u8981\u76ee\u6807\uff0c\u6027\u80fd\u53ef\u79fb \u690d\u7684\u6982\u5ff5\u5df2\u7ecf\u51fa\u73b0\u4e86\u5f88\u591a\u5e74\u3002\u65e9\u57281970\u5e74\u4ee3\uff0c\u90a3\u65f6\u7684\u7a0b\u5e8f\u5458\u5c31\u5df2\u7ecf\u5f00\u59cb\u9762\u5bf9\u5c06\u987a\u5e8f\u7a0b\u5e8f\u4ece CDC-6600\u548cCDC-7600\u8ba1\u7b97\u673a\u79fb\u690d\u5230\u77e2\u91cf\u8ba1\u7b97\u673aCray-1\u7684\u6311\u6218\uff0c\u800c\u8fd9\u6700\u7ec8\u4fc3\u6210\u4e86OpenMP\u7f16\u7a0b \u6807\u51c6 \u7684\u51fa\u73b0\u3002\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u67b6\u6784\u7684\u8ba1\u7b97\u8282\u70b9\u6765\u800c\u8a00\uff0cOpenMP\u53ef\u4ee5\u5e2e\u52a9\u7a0b\u5e8f\u5728\u8de8\u8d8a\u591a\u79cd \u64cd\u4f5c\u7cfb\u7edf\u3001\u5904\u7406\u5668\u67b6\u6784\u4ee5\u53ca\u7f16\u8bd1\u5668\u65f6\u83b7\u5f97\u826f\u597d\u7684\u6027\u80fd\u3002\u63a5\u4e0b\u6765\u768490\u5e74\u4ee3\uff0c\u7a0b\u5e8f\u5458\u5728\u7ecf\u5386\u4e86 \u5de8\u5927\u7684\u52aa\u529b\u540e\u5b8c\u6210\u4e86\u5c06\u7a0b\u5e8f\u4ece\u5171\u4eab\u5185\u5b58\u67b6\u6784\u8ba1\u7b97\u673a\u5411\u673a\u7fa4\u67b6\u6784\u8ba1\u7b97\u673a\u7684\u8fc1\u79fb\uff0c\u800c\u8fd9\u6700\u7ec8\u4fc3 \u6210\u4e86MPI\uff08\u6d88\u606f\u4f20\u9012\u63a5\u53e3\uff09\u6807\u51c6 \u7684\u51fa\u73b0\u3002\u8fdb\u5165\u65b0\u4e16\u7eaa\u540e\uff0c\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u8282\u70b9\u9010\u6e10\u5f00\u59cb\u5927 \u8303\u56f4\u4f7f\u7528\u591a\u6838\u5904\u7406\u5668\uff0c\u7531\u4e8e\u8ba1\u7b97\u6838\u5fc3\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u5229\u7528\u6bd4\u4f20\u9012\u6d88\u606f\u6548\u7387\u66f4\u9ad8\uff0c\u6df7\u5408\u7f16\u7a0b\u6280 \u672fM+X \uff08\u6b64\u5904\u7684M\u901a\u5e38\u662f\u6307MPI\uff0cX\u53ef\u4ee5\u4e3aOpenMP \u6216\u8005Cilk \u7b49\uff09\u53d8\u5f97\u91cd\u8981\u3002\u76f4\u5230\u6b64 \u65f6\uff0c\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4ecd\u7136\u53ef\u4ee5\u7528\u540c\u4e00\u79cd\u7f16\u7a0b\u6a21\u578b\u65b9\u4fbf\u7684\u5b9e\u73b0\u7a0b\u5e8f\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u7684\u79fb \u690d\u3002\u4ece\u4ee5\u4e0a\u89d2\u5ea6\u770bOpenMP\u4ee5\u53caMPI\u662f\u6027\u80fd\u79fb\u690d\u7684\u4f7f\u80fd\u6280\u672f\u3002\u7efc\u4e0a\uff0c\u53ef\u4ee5\u53d1\u73b0\u8d85\u7ea7\u8ba1\u7b97\u673a\u4f53 \u7cfb\u7ed3\u6784\u7684\u53d1\u5c55\u5bfc\u81f4\u4e86\u7f16\u7a0b\u6a21\u5f0f\u7684\u6539\u53d8\uff0c\u7f16\u7a0b\u6a21\u5f0f\u5fc5\u987b\u4e0d\u65ad\u8fdb\u5316\u4ee5\u9002\u5e94\u65b0\u7684\u4f53\u7cfb\u7ed3\u6784\u3002 \u5982\u679c\u53ef\u4ee5\u7ee7\u7eed\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u6838\u5fc3\u6570\u91cf\u7684\u65b9\u5f0f\u63d0\u9ad8\u5904\u7406\u5668\u6027\u80fd\uff0c\u90a3\u4e48\u7b80\u5355\u3001\u76f4\u89c2\u7684M+X \u6df7\u5408\u7f16\u7a0b\u6a21\u578b\u662f\u5b9e\u73b0\u6027\u80fd\u79fb\u690d\u7684\u6700\u4f73\u9009\u62e9\u3002\u7136\u800c\uff0c\u968f\u7740Dennard\u7f29\u653e\u5b9a\u5f8b\u4e0e\u6469\u5c14\u5b9a\u5f8b\u7684\u7ec8 \u7ed3\uff0c\u5f53\u524d\u901a\u7528\u5904\u7406\u5668\u7684\u53d1\u5c55\u9047\u5230\u4e86\u4e25\u91cd\u7684\u74f6\u9888\uff0c\u5f88\u96be\u901a\u8fc7\u7ee7\u7eed\u589e\u52a0\u590d\u6742\u7684\u8ba1\u7b97\u6838\u5fc3\uff0c\u4ee5\u53ca \u63d0\u9ad8\u8ba1\u7b97\u6838\u5fc3\u9891\u7387\u6765\u63d0\u5347\u6027\u80fd\u3002\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\u662f\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u9886\u57df\u5b9a\u5236\u4f53\u7cfb\u7ed3\u6784\uff0c\u4e3a\u8be5\u9886 \u57df\u63d0\u4f9b\u663e\u8457\u7684\u6027\u80fd\u548c\u80fd\u6548\u6536\u76ca\uff0c\u8fd9\u4e5f\u88ab\u79f0\u4e4b\u4e3a\u201c\u9886\u57df\u7279\u5b9a\u67b6\u6784\u201d\uff08DSA\uff09\u3002DSA\u7684\u4f8b\u5b50\u5305\u62ec GPU\u3001\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5668\u3001\u4ee5\u53caFPGA\u7b49\u3002\u7531\u4e8e\u80fd\u66f4\u6709\u6548\u7684\u5229\u7528\u7279\u5b9a\u9886\u57df\u7684\u5e76\u884c\u5f62\u5f0f\u4ee5\u53ca\u5185\u5b58 \u5c42\u6b21\u7ed3\u6784\uff0c\u4e0e\u901a\u7528CPU\u76f8\u6bd4\uff0cDSA\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u548c\u66f4\u9ad8\u7684\u80fd\u6548\u3002\u4ee5GPU\u4e3a\u4f8b\uff0c\u5176\u5728\u7247 \u4e0a\u96c6\u6210\u4e86\u4f17\u591a\u6d41\u5904\u7406\u5668\u6838\u5fc3\uff0c\u8fd9\u4e9b\u6838\u5fc3\u4e0d\u5305\u542b\u4e71\u5e8f\u6267\u884c\u3001cache\u63a7\u5236\u7b49\u8017\u8d39\u8d44\u6e90\u7684\u6a21\u5757\uff0c \u56e0\u6b64\u7ed3\u6784\u7b80\u5355\u80fd\u8017\u8f83\u4f4e\u3002GPU\u501f\u52a9SIMD\u5e76\u884c\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5feb\u901f\u7684\u4efb\u52a1\u5207\u6362\u9690\u85cf\u4e00\u7ec4\u8ba1\u7b97\u4efb \u52a1\u7684\u8bbf\u5b58\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u8fdc\u9ad8\u4e8e\u901a\u7528\u5904\u7406\u5668\u7684\u8ba1\u7b97\u541e\u5410\u91cf\u3002GPU\u7684\u7279\u6b8a\u67b6\u6784\u4f7f\u5176\u7279\u522b\u9002\u7528\u4e8e \u5927\u89c4\u6a21\u6570\u636e\u5e76\u884c\u7684\u8ba1\u7b97\u4efb\u52a1\u3002\u7531\u4e8eDSA\u53ea\u4f1a\u52a0\u901f\u67d0\u7c7b\u7a0b\u5e8f\uff0c\u56e0\u6b64\u5b83\u4eec\u901a\u5e38\u88ab\u79f0\u4e3a\u52a0\u901f\u5668\u3002 \u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u5c06\u901a\u7528\u5904\u7406\u5668\u4e0e\u4e00\u4e2a\u6216\u591a\u4e2a\u534f\u52a0\u901f\u5668\u5728\u4e3b\u677f\u6216\u7247\u4e0a\u76f8\u8fde\uff0c\u6210\u4e3a\u4e86\u589e\u5f3a\u5355\u8282\u70b9 \u8ba1\u7b97\u80fd\u529b\u7684\u666e\u904d\u9009\u62e9\uff1a\u901a\u7528\u5904\u7406\u5668\u4f5c\u4e3a\u63a7\u5236\u8bbe\u5907\uff0c\u8d1f\u8d23\u8ba1\u7b97\u4efb\u52a1\u7684\u63a7\u5236\u4ee5\u53ca\u8c03\u5ea6\uff1b\u534f\u52a0\u901f \u5668\u5219\u8d1f\u8d23\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u4efb\u52a1\u6216\u7279\u5b9a\u9886\u57df\u7684\u8ba1\u7b97\u4efb\u52a1\u3002 \u88681.2019\u5e7411\u6708\u8d85\u7ea7\u8ba1\u7b97\u673aTOP500\u699c\u5355\u524d10\u7684\u7cfb\u7edf \u7cfb\u7edf/\u578b\u53f7 \u8ba1\u7b97\u8282\u70b9\u7ed3\u6784 \u5b89\u88c5\u5730\u70b9 Linpack \u503c/PFLOPS \u503c/PFLO Summit IBM POWER9 22C 3.1GHz, NVIDIA Volta GV100 ORNL 148. 187. Sierra IBM POWER9 22C 3.1GHz, NVIDIA Volta GV100 LLNL 94. \u795e\u5a01\u592a\u6e56\u4e4b Sunway SW26010 260C 1.45GHz \u56fd\u5bb6\u8d85\u7b97\u65e0\u9521 93. 125. Tianhe-2A Intel Xeon E5-2692v2 12C 2.2GHz, TH Express-2, Matrix\u56fd\u5bb6\u8d85\u7b97\u5e7f\u5dde 61. 100. Frontera Intel Xeon Platinum 28C 2.7GHz \u5fb7\u514b\u8428\u65af\u5dde\u9ad8 \u7ea7\u8ba1\u7b97\u4e2d\u5fc3 23. 38. Piz Daint Intel Xeon E5-2690v3 12C 2.6GHz, NVIDIA Tesla P100 \u745e\u58eb\u5362\u52a0\u8bfa\u56fd \u5bb6\u8d85\u7b97\u4e2d\u5fc3 21. 27. Trinity Intel Xeon E5-2698v3 16C 2.3GHz, Intel Xeon Phi 68C 1.4GHz LANL 20. 41. ABCI Intel Xeon Gold 20C 2.4GHz, NVIDIA Tesla V100 SXM2 \u4e1c\u4eac\u5927\u5b66 19. 32. SuperMUC-NG Intel Xeon Platinum 24C 3.1GHz \u83b1\u5e03\u5c3c\u5179\u8ba1\u7b97 19. 26. Lassen IBM POWER9 22C 3.1GHz, NVIDIA Tesla V100 LLNL 18. 23. \u4ee5\u88681\u4e2d2019\u5e7411\u6708\u8d85\u7ea7\u8ba1\u7b97\u673aTOP500 \u699c\u5355\u4e0a\u6392\u540d\u524d10\u7684\u7cfb\u7edf\u4e3a\u4f8b\uff0c\u53ef\u4ee5\u53d1\u73b0\u8fd9\u4e9b \u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u8ba1\u7b97\u8282\u70b9\u666e\u904d\u91c7\u7528\u901a\u7528\u5904\u7406\u5668\u52a0\u534f\u5904\u7406\u5668\u7684\u5f02\u6784\u67b6\u6784\uff0c\u4ec5\u6709\u7b2c5\u548c\u7b2c9\u4f4d\u7684\u4e24 \u53f0\u8d85\u7ea7\u8ba1\u7b97\u673a\u91c7\u7528\u4e86\u65e0\u534f\u52a0\u901f\u5668\u7684\u540c\u6784\u8282\u70b9\u3002\u5177\u4f53\u5230\u8ba1\u7b97\u8282\u70b9\u4f7f\u7528\u7684\u901a\u7528\u5904\u7406\u5668\uff0c\u4e3b\u8981\u6709 Intel Xeon\u3001IBM Power\u3001\u4ee5\u53ca\u65e0\u9521\u6c5f\u5357\u8ba1\u7b97\u6280\u672f\u7814\u7a76\u6240\u8bbe\u8ba1\u7684\u7533\u5a01\uff1b\u800c\u534f\u52a0\u901f\u5668\u5219\u4e3b\u8981 \u6709Nvidia GPU\u3001Intel Xeon Phi\u3001\u4ee5\u53ca\u4e2d\u56fd\u56fd\u9632\u79d1\u5b66\u6280\u672f\u5927\u5b66\u7684Matrix-2000\u3002FPGA\u7531\u4e8e \u81ea\u8eab\u9762\u4e34\u7684\u6311\u6218\uff08\u76f8\u5bf9\u7a00\u5c11\u7684\u7247\u4e0a\u8d44\u6e90\u4ee5\u53ca\u8f83\u4f4e\u7684\u65f6\u949f\u9891\u7387\uff09\uff0c\u5728\u5f53\u524d\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df \u5e94\u7528\u5e76\u4e0d\u5e7f\u6cdb\uff0c\u540c\u65f6\u7531\u4e8e\u8d85\u7ea7\u8ba1\u7b97\u673a\u5f80\u5f80\u9762\u5411\u4e0d\u540c\u9886\u57df\u7684\u7a0b\u5e8f\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709Top10\u7684\u7cfb\u7edf \u5b89\u88c5\u4e13\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u82af\u7247\u3002 1.2\u5728\u4f53\u7cfb\u7ed3\u6784\u65b0\u9ec4\u91d1\u65f6\u4ee3\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u7684\u65b9\u5f0f \u7531\u4e8e\u5f53\u524d\u540c\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0e\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u540c\u65f6\u5b58\u5728\uff0c\u4e14\u4e0d\u540c\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u91c7\u7528\u7684 \u534f\u5904\u7406\u5668\u7684\u67b6\u6784\u4e5f\u5b58\u5728\u5dee\u5f02\uff0c\u5982\u679c\u5f00\u53d1\u4eba\u5458\u5728\u8bbe\u8ba1\u7a0b\u5e8f\u65f6\u6ca1\u6709\u5c06\u6027\u80fd\u53ef\u79fb\u690d\u7eb3\u5165\u8003\u8651\uff0c\u5219 \u5f53\u7a0b\u5e8f\u9700\u8981\u8fd0\u884c\u4e8e\u91c7\u7528\u5176\u5b83\u67b6\u6784\u7684\u8ba1\u7b97\u673a\u4e0a\u65f6\uff0c\u9700\u8981\u5bf9\u5176\u4ee3\u7801\u8fdb\u884c\u5927\u91cf\u91cd\u5199\u3002\u56e0\u6b64\uff0c\u8003\u8651 \u7b97\u6cd5\u6216\u7a0b\u5e8f\u7684\u6027\u80fd\u53ef\u79fb\u690d\uff0c\u4ee5\u4fbf\u65b0\u7684\u8d85\u7ea7\u8ba1\u7b97\u673a\u5728\u5b83\u4eec\u53ef\u7528\u7684\u90a3\u4e00\u523b\u5c31\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\uff0c \u662f\u5f53\u524d\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002\u7a0b\u5e8f\u6027\u80fd\u53ef\u79fb\u690d\u6280\u672f\u6db5\u76d6\u8303\u56f4\u975e\u5e38\u5e7f\u6cdb\uff0c\u56fe 1\u5bf9\u5f53\u524d\u51e0\u4e2a\u91cd\u8981\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6280\u672f\u7684\u5e94\u7528\u573a\u666f\u8fdb\u884c\u4e86\u603b\u7ed3:\u57fa\u4e8e\u7f16\u7a0b\u6a21\u578b\uff0c\u7f16\u5199\u6ee1\u8db3\u6027\u80fd \u53ef\u79fb\u690d\u8981\u6c42\u7684\u4ee3\u7801\uff1b\u5bf9\u4e8e\u7b26\u5408\u8981\u6c42\u7684\u4e32\u884c\u4ee3\u7801\uff0c\u4f7f\u7528\u7f16\u8bd1\u7cfb\u7edf\u81ea\u52a8\u4ece\u4ee3\u7801\u4e2d\u53d1\u73b0\u5e76\u884c\u6027\uff0c \u5e76\u9762\u5411\u4e0d\u540c\u7684\u76ee\u6807\u8ba1\u7b97\u673a\u751f\u6210\u76f8\u5e94\u4ee3\u7801\uff1b\u5bf9\u4e8e\u5df2\u7ecf\u9488\u5bf9\u67d0\u79cd\u67b6\u6784\u7684\u8ba1\u7b97\u673a\u5b8c\u6210\u5f00\u53d1\u7684\u4ee3 \u7801\uff0c\u4f7f\u7528\u6e90\u7801\u8f6c\u6362\u5de5\u5177\u5c06\u5176\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u4ee5\u5728\u5176\u5b83\u67b6\u6784\u7684\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u7684\u4ee3\u7801\u3002 \u56fe1. \u6027\u80fd\u53ef\u79fb\u690d\u6280\u672f\u7684\u5e94\u7528\u573a\u666f. \u672c\u6587\u7b2c\u4e8c\u7ae0\u5c06\u5bf9\u6ee1\u8db3\u6027\u80fd\u53ef\u79fb\u690d\u8981\u6c42\u7684\u6a21\u578b\u8fdb\u884c\u91cd\u70b9\u4ecb\u7ecd\uff1b\u7b2c\u4e09\u7ae0\u5c06\u5bf9\u5f53\u524d\u7684\u4e32\u884c\u4ee3 \u7801\u81ea\u52a8\u5e76\u884c\u5316\u6280\u672f\u8fdb\u884c\u63cf\u8ff0\uff0c\u5e76\u4ecb\u7ecd\u4e00\u4e9b\u4f7f\u7528\u8fd9\u4e9b\u6280\u672f\u8fdb\u884c\u4ee3\u7801\u79fb\u690d\u7684\u4f8b\u5b50\uff1b\u7b2c\u56db\u7ae0\u5c06\u603b \u7ed3\u5f53\u524d\u7684\u4e00\u4e9b\u4e3b\u6d41\u5e76\u884c\u4ee3\u7801\u8f6c\u6362\u6280\u672f\u3002\u672c\u6587\u6700\u540e\u8fd8\u5c06\u4ecb\u7ecd\u4e00\u4e9b\u5408\u7406\u8c03\u7528\u79d1\u5b66\u8ba1\u7b97\u5e93\u51fd\u6570\uff0c \u4ee5\u63d0\u5347\u7a0b\u5e8f\u6027\u80fd\u7684\u6280\u672f\u3002 2\u3001\u6ee1\u8db3\u6027\u80fd\u53ef\u79fb\u690d\u8981\u6c42\u7684\u5e76\u884c\u7f16\u7a0b\u6a21\u578b \u5728\u540c\u6784\u4e0e\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u5e76\u5b58\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u7f16\u5199\u7a0b\u5e8f\uff0c\u662f\u5b9e\u73b0\u7a0b\u5e8f\u6027\u80fd\u53ef\u79fb\u690d\u9996\u5148\u8981 \u89e3\u51b3\u7684\u95ee\u9898\u3002\u4f34\u968f\u7740\u5f02\u6784\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\uff0c\u539f\u6709\u7684\u540c\u6784\u8ba1\u7b97\u673a\u5e76\u884c\u7f16\u7a0b\u6a21\u578b(OpenMP\uff0cCilk \u7b49)\u5df2\u4e0d\u518d\u9002\u7528\uff0c\u7531\u6b64\u63a8\u52a8\u4e86\u5927\u91cf\u4e0e\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u76f8\u5173\u7684\u7814\u7a76\u3002\u4e00\u4e2a\u597d\u7684\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u9700 \u80fd\u63d0\u4f9b\u5408\u7406\u6613\u7528\u7684\u8ba1\u7b97\u8bbe\u5907\u62bd\u8c61\uff0c\u4f7f\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u5728\u4e0d\u9677\u5165\u590d\u6742\u7684\u786c\u4ef6\u7ec6\u8282\u4e2d\u7684\u60c5\u51b5\u4e0b\uff0c \u5b8c\u6210\u4efb\u52a1\u5212\u5206\u3001\u4efb\u52a1\u6620\u5c04\u3001\u6570\u636e\u5206\u5e03\u3001\u8ba1\u7b97\u540c\u6b65\u3001\u4ee5\u53ca\u6570\u636e\u901a\u4fe1\u7b49\u4efb\u52a1\uff1b\u540c\u65f6\uff0c\u7f16\u8bd1\u4f18\u5316 \u7cfb\u7edf\u4f5c\u4e3a\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u7684\u5de5\u5177\u5c42\uff0c\u5c06\u5f00\u53d1\u4eba\u5458\u7f16\u5199\u7684\u7a0b\u5e8f\u8fdb\u884c\u4f18\u5316\u7f16\u8bd1\uff0c\u751f\u6210\u53ef\u5728\u76ee\u6807\u8ba1 \u7b97\u8bbe\u5907\u4e0a\u6267\u884c\u7684\u6587\u4ef6\u3002\u5982\u88682\u6240\u793a\uff0c\u5f53\u524d\u652f\u6301\u6027\u80fd\u53ef\u79fb\u690d\u7684\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u901a\u5e38\u4ee5\u7f16\u7a0b\u63a5 \u53e3\u3001\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u3001\u4ee5\u53ca\u7f16\u7a0b\u8bed\u8a00\u7684\u5f62\u5f0f\u4e3a\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u8ba1\u7b97\u8bbe\u5907\u62bd\u8c61\u3002\u672c\u8282\u5c06\u5206\u522b \u5bf9\u4ee5\u4e0a\u4e09\u7c7b\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u8fdb\u884c\u68b3\u7406\u603b\u7ed3\u3002 \u88682. \u5f53\u524d\u4e3b\u6d41\u5e76\u884c\u7f16\u7a0b\u6a21\u578b \u7f16\u7a0b\u6a21\u578b \u540e\u7aef\u5e76\u884c\u65b9\u5f0f\u53ca\u652f\u6301\u7684\u8ba1\u7b97\u8bbe \u652f\u6301\u6027\u80fd\u53ef \u79fb\u690d\uff08\u5426\u3001 \u7f16\u7a0b\u63a5\u53e3 OpenCL CPU\u3001Xeon Phi\u3001GPU\u7b49 \u7f16\u7a0b\u63a5\u53e3 SkelCL OpenCL \u7f16\u7a0b\u63a5\u53e3 Boost.compute OpenCL \u7f16\u7a0b\u63a5\u53e3 Bolt OpenCL \u7f16\u7a0b\u63a5\u53e3 SYCL CPU\u3001Xeon Phi\u3001GPU\u7b49 \u7f16\u7a0b\u63a5\u53e3 DPC++ CPU\u3001Xeon Phi\u3001GPU\u3001FPGA\u7b49 \u7f16\u7a0b\u63a5\u53e3 C++ AMP CPU\u3001GPU\u7b49 \u7f16\u7a0b\u63a5\u53e3 Kokkos OpenMP\u3001CUDA\u3001OpenCL \u7f16\u7a0b\u63a5\u53e3 RAJA OpenMP\u3001CUDA\u3001OpenCL \u7f16\u7a0b\u63a5\u53e3 OP2 MPI\u3001OpenMP\u3001CUDA\u3001OpenCL [14, \u6307\u5bfc\u8bed\u53e5 OpenACC CPU\u3001Xeon Phi\u3001GPU\u7b49 \u6307\u5bfc\u8bed\u53e5 OpenMP 4.X CPU\u3001Xeon Phi\u3001GPU\u7b49 \u6307\u5bfc\u8bed\u53e5 OpenHMPP CPU\u3001Xeon Phi\u3001GPU\u7b49 \u6307\u5bfc\u8bed\u53e5 Mint GPU Chapel CPU\u3001GPU [20, X10 CPU\u3001GPU Halide CPU\u3001GPU\u7b49 TVM CPU\u3001GPU\u7b49 XLA CPU\u3001GPU\u7b49 2. \u57fa\u4e8e\u7f16\u7a0b\u63a5\u53e3\u7684\u5e76\u884c\u7f16\u7a0b\u6a21\u578b \u4e3a\u65b9\u4fbf\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u57fa\u4e8e\u73b0\u6709\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u7a0b\u5e8f\u7684\u6027\u80fd\u53ef\u79fb\u690d\uff0c\u5f53\u524d\u6709\u8f83\u591a\u5de5\u4f5c\u5c06\u5bf9 \u8ba1\u7b97\u8bbe\u5907\u8fdb\u884c\u7684\u64cd\u4f5c\u4ee5\u7f16\u7a0b\u63a5\u53e3\u7684\u5f62\u5f0f\u63d0\u4f9b\u7ed9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\uff0c\u4ece\u800c\u65b9\u4fbf\u5f00\u53d1\u4eba\u5458\u57fa\u4e8e\u73b0\u6709 \u8bed\u8a00\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u3002 2.1. \u6807\u51c6\u7f16\u7a0b\u63a5\u53e3 \u53d7\u652f\u6301\u4e0d\u540c\u7f51\u7edc\u7c7b\u578b\u7684MPI\u6d88\u606f\u4f20\u9012\u63a5\u53e3\u542f\u53d1\uff0c\u5f53\u524d\u6709\u8bb8\u591a\u7ec4\u7ec7\u4ee5\u53ca\u673a\u6784\u5c1d\u8bd5\u4e3a\u591a\u79cd \u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u63d0\u4f9b\u7edf\u4e00\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u4ece\u800c\u5b9e\u73b0\u7a0b\u5e8f\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u7684\u6027\u80fd\u53ef\u79fb\u690d\u3002 \u4f34\u968f\u7740NVIDIA GPU\u5728\u8d85\u7b97\u9886\u57df\u7684\u5e94\u7528\uff0cNVIDIA\u4e8e2007\u5e74\u63a8\u51fa\u4e86CUDA \u5f02\u6784\u7f16\u7a0b\u6807 \u51c6\u4ee5\u5e2e\u52a9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4f7f\u7528GPU\u5bf9\u4ee3\u7801\u8fdb\u884c\u52a0\u901f\uff0c\u7136\u800cCUDA\u4ec5\u80fd\u5728\u4ee5NVIDIA\u7cfb\u5217GPU \u52a0\u901f\u5668\u7684\u5f02\u6784\u7cfb\u7edf\u4e2d\u4f7f\u7528\uff0c\u65e0\u6cd5\u5e2e\u52a9\u7a0b\u5e8f\u5728\u4e0d\u540c\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u3002\u4e3a \u6b64\uff0cKhronos \u7ec4\u7ec7\u4e8e2008\u5e74\u63d0\u51fa\u4e86OpenCL \u7f16\u7a0b\u6807\u51c6\uff0c\u5c1d\u8bd5\u5c06\u5404\u79cd\u5904\u7406\u5668\u62bd\u8c61\u4e3a\u7edf\u4e00\u7684 \u6a21\u578b\uff0cIntel\u7684Xeon Phi\u3001NVIDIA GPU\u3001AMD GPU\u3001\u4ee5\u53ca\u795e\u5a01\u592a\u6e56\u4e4b\u5149\u8d85\u7ea7\u8ba1\u7b97\u673a\u4f7f\u7528\u7684 SW2601\u5904\u7406\u5668\u90fd\u5bf9OpenCL\u8fdb\u884c\u4e86\u652f\u6301 [26-29] \u3002OpenCL \u7684\u7f16\u7a0b\u63a5\u53e3\u8f83\u4e3a\u5e95\u5c42\uff0c\u9020\u6210\u7a0b\u5e8f\u5f00 \u53d1\u6548\u7387\u964d\u4f4e\u3002\u4e3a\u4e86\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0cSkelCL \u3001Bolt \u3001\u4ee5\u53caBoost.compute \u7b49\u4ee5\u6a21\u677f \u5e93\u7684\u65b9\u5f0f\u5c01\u88c5\u5e76\u7b80\u5316\u4e86OpenCL\u7684\u6570\u636e\u4e0e\u4efb\u52a1\u7ba1\u7406\u64cd\u4f5c\uff0c\u5176\u4e2dBolt \u4ee5\u53ca Boost.compute \u8fd8\u57fa\u4e8eOpenCL\u5b9e\u73b0\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u5e76\u884c\u7b97\u6cd5\u4f9b\u7f16\u7a0b\u4eba\u5458\u4f7f\u7528\u3002Khronos \u7ec4 \u7ec7\u4e8e2014\u5e74\u63d0\u51fa\u4e86 SYCL \u7f16\u7a0b\u6807\u51c6\uff0c \u4f5c\u4e3aOpenCL\u4e4b\u4e0a\u7684\u62bd\u8c61\u5c42\uff0cSYCL \u6807\u51c6\u652f\u6301\u9ad8\u7ef4 \u6570\u7ec4\uff0c\u7b80\u5316\u4e86OpenCL\u4e2d\u7684\u6570\u636e\u4e0e\u4efb\u52a1\u7ba1\u7406\u64cd\u4f5c\u3002DPC++ \u662fIntel\u57fa\u4e8eSYCL\u8bbe\u8ba1\u7684\u7f16\u7a0b\u6807 \u51c6\uff0c\u5176\u5bf9SYCL\u8fdb\u884c\u4e86\u9002\u5f53\u6269\u5c55\uff0c\u652f\u6301\u8de8CPU\u548c\u534f\u52a0\u901f\u5668\u7684\u6570\u636e\u5e76\u884c\u7f16\u7a0b\uff0c\u540c\u65f6\u8fd8\u652f\u6301\u9762\u5411 \u7279\u5b9a\u7684\u534f\u52a0\u901f\u5668\u8fdb\u884c\u8c03\u4f18\u3002C++ AMP \u662f\u5fae\u8f6f\u63d0\u51fa\u7684\u7f16\u7a0b\u6807\u51c6\uff0c\u5176\u6bd4OpenCL\u62e5\u6709\u66f4\u9ad8\u5c42 \u6b21\u7684\u8ba1\u7b97\u8bbe\u5907\u62bd\u8c61\uff0c\u56e0\u6b64\u4ee3\u7801\u975e\u5e38\u7b80\u6d01\uff0c\u53ea\u9700\u8981\u5728for\u5faa\u73af\u4e2d\u6307\u5b9a\u8ba1\u7b97\u57df\u4ee5\u53ca\u5185\u5d4c\u7684 lambda\u51fd\u6570\u5c31\u80fd\u5b8c\u6210\u5e76\u884c\u8ba1\u7b97\u3002C++ AMP\u7684\u652f\u6301\u591a\u7ef4\u6570\u7ec4\u4ee5\u53ca\u5185\u5b58\u6570\u636e\u5e03\u5c40\uff08Row/column major, AOS, SOA\uff09\uff0c\u4f9d\u6258\u4e8eVisual Studio\u7684\u652f\u6301\uff0c\u4ee5\u53ca\u5fae\u8f6f\u4e3aC++ AMP\u4e13\u95e8\u5f00\u53d1\u7684\u5e76\u884c \u7b97\u6cd5\u5e93\uff0c\u5176\u4e3aWindows\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u652f\u6301\u3002 2.1. \u9002\u914d\u4e0d\u540c\u6807\u51c6\u7f16\u7a0b\u63a5\u53e3\u7684\u6a21\u677f\u51fd\u6570 \u6bcf\u4e00\u79cd\u5e76\u884c\u7f16\u7a0b\u6807\u51c6\u90fd\u6709\u5bf9\u5176\u8fdb\u7a0b\u652f\u6301\u7684\u8ba1\u7b97\u8bbe\u5907\uff0c\u4e3a\u4e86\u80fd\u591f\u4f7f\u7528\u5c3d\u53ef\u80fd\u591a\u7684\u8ba1\u7b97\u8bbe \u5907\uff0c\u5f53\u524d\u6709\u4e00\u4e9b\u7f16\u7a0b\u6a21\u578b\u901a\u8fc7\u7528\u7edf\u4e00\u7684\u63a5\u53e3\u9002\u914d\u4e0d\u540c\u7684\u5e76\u884c\u7f16\u7a0b\u6807\u51c6\uff0c\u8fbe\u5230\u4f7f\u7a0b\u5e8f\u5728\u4e0d\u540c \u67b6\u6784\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u6027\u80fd\u53ef\u79fb\u690d\u7684\u76ee\u6807\u3002 Kokkos \u662f\u4e00\u5957\u5e2e\u52a9\u7a0b\u5e8f\u5b9e\u73b0\u5728\u4e0d\u540c\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u6027\u80fd\u53ef\u79fb\u690d\u7684C++\u6a21\u677f \u5e93\uff0c\u5176\u6838\u5fc3\u662f\u6570\u7ec4\u7684\u62bd\u8c61\u5bb9\u5668view\u4ee5\u53ca\u6570\u636e\u5e76\u884c\u64cd\u4f5c\u7684\u62bd\u8c61\u7c7bfunctor\u3002\u901a\u8fc7\u5bf9\u4e0d\u540c\u7684\u8bbe \u5907\u5185\u5b58\u8fdb\u884c\u62bd\u8c61\uff0cview\u4e0d\u4ec5\u652f\u6301\u591a\u7ef4\u6570\u7ec4\uff0c\u8fd8\u628a\u5185\u5b58\u5bf9\u9f50\u3001\u4e0b\u6807\u6620\u5c04\u3001\u5185\u5b58\u6570\u636e\u5e03\u5c40\u3001\u4ee5 \u53ca\u8bbf\u95ee\u63a7\u5236\u7b49\u5c01\u88c5\u8d77\u6765\u3002functor\u7c7b\u5305\u542b\u7a0b\u5e8f\u5458\u5b9a\u4e49\u7684\u6570\u636e\u64cd\u4f5c\u51fd\u6570\u4ee5\u53ca\u8fd9\u4e9b\u51fd\u6570\u9700\u8981\u64cd \u4f5c\u7684\u6570\u7ec4\u5bb9\u5668\uff0c\u5176\u4e3b\u8981\u4f5c\u7528\u662f\u5c06\u8bbe\u5907\u7684\u6267\u884c\u7a7a\u95f4\u4e0e\u5b58\u50a8\u7a7a\u95f4\u8fde\u63a5\u8d77\u6765\u3002Kokkos\u76ee\u524d\u652f\u6301\u7684 \u7f16\u7a0b\u6807\u51c6\u5305\u62ecOpenMP\uff0cPthreads\u3001CUDA\u3001\u4ee5\u53caOpenCL\u7b49\u3002\u5f53\u524d\u8457\u540d\u7684\u5e76\u884c\u8ba1\u7b97\u57fa\u7840\u5e93 Trilinos \uff0c\u5176\u4e2d\u5e76\u884c\u7ebf\u6027\u4ee3\u6570\u5bb9\u5668Tpetra\u7684\u5e95\u5c42\u5c31\u662f\u57fa\u4e8eKokkos\u5b9e\u73b0\u7684\u3002 RAJA \u540c\u6837\u662f\u4e00\u5957\u5e2e\u52a9\u7a0b\u5e8f\u5b9e\u73b0\u5728\u4e0d\u540c\u67b6\u6784\u8ba1\u7b97\u8bbe\u5907\u95f4\u6027\u80fd\u53ef\u79fb\u690d\u7684C++\u6a21\u677f\u5e93\uff0c \u5176\u53c2\u8003\u4e86\u52b3\u4f26\u65af\u5229\u7269\u83ab\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u7684\u4e00\u7cfb\u5217\u7ed3\u6784\u4ee5\u53ca\u975e\u7ed3\u6784\u7f51\u683c\u7a0b\u5e8f\u7684\u7279\u70b9\uff0c\u8ba4\u4e3a\u8fd9\u4e9b\u7a0b \u5e8f\u4e2d\u5bf9\u7f51\u683c\u8fdb\u884c\u904d\u5386\u64cd\u4f5c\u7684for\u5faa\u73af\u7684\u5404\u5faa\u73af\u5b9e\u4f8b\u4e4b\u95f4\u4e0d\u5b58\u5728\u6570\u636e\u4f9d\u8d56\uff0c\u53ef\u4ee5\u5e76\u884c\u6267 \u884c\u3002RAJA\u5c06\u8fd9\u4e9bfor\u5faa\u73af\u62bd\u8c61\u6210\u4e3a\u6a21\u677f\u51fd\u6570\uff0c\u6a21\u677f\u51fd\u6570\u7684\u53c2\u6570\u5305\u62ec\u7528\u6237\u6307\u5b9a\u7684\u5faa\u73af\u4f53\u6267\u884c \u7b56\u7565\u4ee5\u53ca\u76ee\u6807\u7f16\u7a0b\u6807\u51c6\u7b49\u3002RAJA\u8fd8\u5f15\u5165\u4e86Indexsets\u7684\u6982\u5ff5\uff0c\u53ef\u4ee5\u81ea\u52a8\u5c06\u5faa\u73af\u7a7a\u95f4\u5212\u5206\u6210 \u4e3a\u4e0d\u540c\u7684\u5206\u6bb5\uff0c\u6700\u7ec8\u5728\u7f16\u8bd1\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u4e3a\u6bcf\u4e00\u7c7b\u5206\u6bb5\u5206\u522b\u751f\u6210\u4ee3\u7801\u3002RAJA\u76ee\u524d\u652f\u6301\u7684\u7f16\u7a0b \u6807\u51c6\u5305\u62ecOpenMP\u3001Pthreads\u3001\u4ee5\u53caCUDA\u7b49\u3002 OP2 [14, 15] \u662f\u4e13\u95e8\u9488\u5bf9\u975e\u7ed3\u6784\u7f51\u683c\u7a0b\u5e8f\u8bbe\u8ba1\u7684\u7f16\u7a0b\u6a21\u677f\uff0c\u5176\u524d\u8eab\u662fOplus [32, 33] \u3002OP2\u5c06\u975e\u7ed3\u6784\u7f51\u683c\u7a0b\u5e8f\u5212\u5206\u4e3a4\u90e8\u5206\uff1a\u96c6\u5408\uff08\u8fb9\u7684\u96c6\u5408\uff0c\u7f51\u683c\u7684\u96c6\u5408\u7b49\uff09\u3001\u96c6\u5408\u76f8\u5173 \u7684\u6570\u636e\u3001\u96c6\u5408\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff08\u8fb9\u4e0e\u7f51\u683c\u7684\u5bf9\u5e94\u5173\u7cfb\uff09\u3001\u4ee5\u53ca\u5bf9\u96c6\u5408\u7684\u64cd\u4f5c\u3002\u5176\u8ba4\u4e3a\u6240\u6709 \u975e\u7ed3\u6784\u7f51\u683c\u7a0b\u5e8f\u90fd\u662f\u5bf9\u76f8\u5173\u96c6\u5408\u8fdb\u884c\u7b97\u6570\u64cd\u4f5c\uff0c\u4f53\u73b0\u5728\u4ee3\u7801\u4e0a\u5c31\u662f\u4f7f\u7528for\u5faa\u73af\u5bf9\u96c6\u5408\u7684 \u5143\u7d20\u8fdb\u884c\u904d\u5386\uff0c\u76f4\u63a5\u6216\u901a\u8fc7\u6620\u5c04\u5173\u7cfb\u95f4\u63a5\u8bbf\u95ee\u5e76\u64cd\u4f5c\u96c6\u5408\u7684\u6570\u636e\u3002\u4f7f\u7528OP2\u53ef\u4ee5\u5b9e\u73b0\u7a0b\u5e8f \u7684\u8282\u70b9\u95f4\u4ee5\u53ca\u8282\u70b9\u5185\u5e76\u884c\uff0c\u5176\u4e2d\u8282\u70b9\u95f4\u5e76\u884c\u57fa\u4e8eMPI\u7f16\u7a0b\u6807\u51c6\u5b9e\u73b0\uff0c\u8282\u70b9\u5185\u5e76\u884c\u6839\u636e\u540e\u7aef \u8ba1\u7b97\u8bbe\u5907\u53ef\u4ee5\u57fa\u4e8ePthread\u3001OpenMP\u3001CUDA\u3001OpenCL\u7b49\u7f16\u7a0b\u6807\u51c6\u5b9e\u73b0\u3002\u5728\u5b9e\u73b0\u8282\u70b9\u95f4\u5e76\u884c \u65f6\uff0cOP2\u6846\u67b6\u4f7f\u7528ParMETISs \u4ee5\u53caPTScotch \u5bf9\u6574\u4e2a\u8ba1\u7b97\u57df\u7684\u7f51\u683c\u8fdb\u884c\u5212\u5206\u4ee5\u786e\u4fdd\u8282 \u70b9\u4e4b\u95f4\u7684\u8d1f\u8f7d\u5e73\u8861\uff1b\u5728\u5b9e\u73b0\u8282\u70b9\u5185\u5e76\u884c\u65f6\uff0cOP2\u91c7\u7528\u56fe\u7740\u8272\u7b97\u6cd5\u5b9e\u73b0\u8282\u70b9\u5185\u5904\u7406\u5355\u5143\u7684\u4efb \u52a1\u5206\u914d\u53ca\u8d1f\u8f7d\u5e73\u8861\u3002OP2\u6846\u67b6\u8fd8\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u540e\u7aef\u8ba1\u7b97\u8bbe\u5907\u7684\u7279\u70b9\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u6570\u636e\u5e03\u5c40 \u65b9\u5f0f\uff08AOS\u6216SOA\uff09\u3002\u65e9\u671f\u7684OP2\u6846\u67b6\u57fa\u4e8eROSE\u6e90\u7801\u7f16\u8bd1\u5668\u5b9e\u73b0\u6e90\u7801\u7f16\u8bd1\uff0c\u800c\u6700\u65b0\u7684OP2\u6846\u67b6 \u91c7\u7528LLVM-Clang\u91cd\u65b0\u5b9e\u73b0\u4e86\u6e90\u7801\u7f16\u8bd1\u529f\u80fd\u3002 2.2\u57fa\u4e8e\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u7684\u7f16\u7a0b\u6a21\u578b \u5bf9\u4e8e\u6570\u91cf\u5e9e\u5927\u7684\u9057\u4ea7\u4ee3\u7801\uff0c\u4f7f\u7528\u63d2\u5165\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u7684\u65b9\u5f0f\u53ef\u4ee5\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u4f7f\u7528\u4e0d\u540c \u8ba1\u7b97\u8bbe\u5907\u5bf9\u5176\u8fdb\u884c\u52a0\u901f\u3002OpenACC \u662f\u7531Cray\u3001PGI\u3001\u4ee5\u53ca\u82f1\u4f1f\u8fbe\u53d1\u8d77\u7684\u4e00\u4e2a\u7f16\u7a0b\u6807\u51c6\uff0c \u4e0eOpenMP\u7c7b\u4f3c\uff0c\u5176\u5141\u8bb8\u5c06\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u63d2\u5165Fortran,C\u548cC++\u7a0b\u5e8f\u7684\u4ee3\u7801\u4e2d\uff0c\u4ee5\u5e2e\u52a9\u7f16\u8bd1\u5668 \u5c06\u8ba1\u7b97\u4efb\u52a1\u8c03\u5ea6\u5230\u534f\u52a0\u901f\u5668\u4e0a\u3002\u76ee\u524d\u5bf9OpenACC\u8fdb\u884c\u652f\u6301\u7684\u8bbe\u5907\u5305\u62ecNVIDIA GPU, AMD GPU, SW26010, Intel Xeon Phi,FPGA\u7b49\u3002OpenACC\u7684\u51fa\u73b0\u6781\u5927\u7684\u65b9\u4fbf\u4e86\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4f7f\u7528 \u534f\u5904\u7406\u5668\u52a0\u901f\u6570\u91cf\u5e9e\u5927\u7684\u9057\u4ea7\u4ee3\u7801\u3002\u4e0eOpenACC\u7c7b\u4f3c\uff0cHMPP \u662f\u7531CAPS\u7ec4\u7ec7\u53d1\u8d77\u7684\u4e00\u79cd \u57fa\u4e8e\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u7684\u7f16\u7a0b\u6807\u51c6\uff0c\u5176\u652f\u6301C\u548cFortran\u4e24\u79cd\u8bed\u8a00\u3002\u76ee\u524dNIVIDA \u7cfb\u5217GPU\u5bf9HMPP \u8fdb\u884c\u4e86\u652f\u6301\u3002HMPP\u7f16\u8bd1\u5668\u53ef\u4ee5\u6839\u636e#pragma\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u751f\u6210\u5728\u76f8\u5e94\u8ba1\u7b97\u8bbe\u5907\u4e0a\u6267\u884c\u7684\u4e8c \u8fdb\u5236\u6587\u4ef6\u3002\u81ea\u4eceOpenMP 4.X \u5f00\u59cb\uff0cOpenMP\u4e2d\u4e5f\u5f15\u5165\u4e86\u7528\u4e8e\u534f\u5904\u7406\u5668\u52a0\u901f\u7684\u6307\u5bfc\u8bed\u53e5\uff0c \u76ee\u524dNVIDIA GPU, AMD GPU, Intel Xeon Phi\u3001\u4ee5\u53caIBM Power\u5904\u7406\u5668\u90fd\u5bf9OpenMP 4.X\u8fdb\u884c \u4e86\u652f\u6301\u3002OpenMP\u5bf9\u5e95\u5c42\u7684\u62bd\u8c61\u6bd4OpenACC\u8981\u590d\u6742\uff0c\u5bfc\u81f4\u5176\u4f7f\u7528\u96be\u5ea6\u8981\u9ad8\u4e8e OpenACC\u3002Mint \u662f\u4e3a\u6ca1\u6709CUDA\u7f16\u7a0b\u7ecf\u9a8c\u7684\u4eba\u5458\u8bbe\u8ba1\u7684\u7f16\u7a0b\u6a21\u578b, \u5176\u53ef\u4ee5\u5e2e\u52a9\u4ed6\u4eec\u4f7f\u7528 \u7b80\u5355\u76845\u6761\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u5c06\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u4e32\u884cStencil\u4ee3\u7801\u8f6c\u6362\u4e3aCUDA\u4ee3\u7801\u3002Mint \u57fa\u4e8eROSE\u6e90\u7801\u7f16\u8bd1\u5668\u5b9e\u73b0\u4e86\u4ee5\u4e0a\u6e90\u7801\u8f6c\u6362\uff0c\u5728\u8f6c\u6362\u7684\u8fc7\u7a0b\u4e2d\u8fd8\u96c6\u6210\u4e86\u8bbf\u5b58\u4f18\u5316\u7b56\u7565\uff0c\u76ee\u524d Mint\u53ea\u80fd\u751f\u6210CUDA\u4ee3\u7801\uff0c\u4f46\u5176\u7ecf\u8fc7\u7b80\u5355\u6269\u5c55\uff0c\u5c31\u53ef\u4ee5\u5b9e\u73b0\u5bf9OpenCL\u7684\u652f\u6301\u3002Niklas\u5728\u4ed6\u7684 \u5de5\u4f5c\u4e2d\u4f7f\u7528LLVM\u524d\u7aefClang\u91cd\u65b0\u5b9e\u73b0\u4e86Mint \u3002 2. \u57fa\u4e8e\u8bed\u8a00\u7684\u7f16\u7a0b\u6a21\u578b \u5f53\u524d\u8fd8\u6709\u8bb8\u591a\u5de5\u4f5c\u8bbe\u8ba1\u65b0\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u8fd9\u4e9b\u7f16\u7a0b\u8bed\u8a00\u5bf9\u5e76\u884c\u7f16\u7a0b\u6240\u9700\u7684\u5e38\u7528\u64cd\u4f5c\u8fdb\u884c \u603b\u7ed3\uff0c\u4ece\u800c\u7b80\u5316\u5e76\u884c\u7a0b\u5e8f\u7684\u5f00\u53d1\u96be\u5ea6\u3002\u8fc7\u53bb\u7684\u5f88\u957f\u4e00\u6bb5\u65f6\u95f4\uff0c\u7a0b\u5e8f\u4e2d\u63cf\u8ff0\u7b97\u6cd5\u7684\u903b\u8f91\u88ab\u6027 \u80fd\u4f18\u5316\u7b56\u7565\u63a9\u76d6\u3002\u9488\u5bf9\u7279\u5b9a\u8ba1\u7b97\u8bbe\u5907\u8fdb\u884c\u6027\u80fd\u4f18\u5316\u540e\u7684\u4ee3\u7801\u57fa\u672c\u88ab\u56fa\u5b9a\uff0c\u4e0d\u518d\u5177\u6709\u826f\u597d\u7684 \u53ef\u7ef4\u62a4\u6027\uff0c\u4ee3\u7801\u5728\u8fdb\u884c\u8de8\u8ba1\u7b97\u8bbe\u5907\u79fb\u690d\u65f6\u9700\u8981\u5927\u8303\u56f4\u91cd\u5199\u3002\u56e0\u6b64\uff0c\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u5f80\u5f80\u91c7\u7528 \u4fdd\u5b88\u7684\u65b9\u6848\uff0c\u628a\u6027\u80fd\u4f18\u5316\u7559\u5230\u6700\u540e\u4e00\u6b65\u3002\u4e3a\u4e86\u63d0\u9ad8\u4ee3\u7801\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u6709\u7814\u7a76\u5c06\u7b97\u6cd5\u63cf \u8ff0\u548c\u4ee3\u7801\u751f\u6210\u89e3\u8026\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u8bed\u6cd5\u5bf9\u7b97\u6cd5\u8fdb\u884c\u63cf\u8ff0\uff0c\u7136\u540e\u518d\u9488\u5bf9\u5177\u4f53\u7684\u8ba1\u7b97\u8bbe\u5907\u751f\u6210\u76f8 \u5e94\u7684\u4f18\u5316\u4ee3\u7801\uff0c\u8fd9\u6837\u5728\u4e0d\u540c\u7684\u8ba1\u7b97\u8bbe\u5907\u95f4\u79fb\u690d\u7b97\u6cd5\u65f6\u53ea\u9700\u8981\u5bf9\u7b97\u6cd5\u7684\u4ee3\u7801\u751f\u6210\u6a21\u5757\u8fdb\u884c\u66ff \u6362\u3002\u672c\u8282\u5c06\u5206\u522b\u4ecb\u7ecd\u7b97\u6cd5\u63cf\u8ff0\u4e0e\u4f18\u5316\u7b56\u7565\u6df7\u5408\u4ee5\u53ca\u89e3\u8026\u7684\u7f16\u7a0b\u8bed\u8a00\u3002 2.3.1\u7b97\u6cd5\u5b9e\u73b0\u4e0e\u4f18\u5316\u6df7\u5408\u7684\u7f16\u7a0b\u8bed\u8a00 \u8fc7\u53bb\u4e00\u6bb5\u65f6\u95f4\uff0c\u6709\u4e00\u4e9b\u9762\u5411\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u7684\u7a0b\u5e8f\u8bed\u8a00\u88ab\u8bbe\u8ba1\u51fa\u6765\uff0c\u7ecf\u8fc7\u9002\u5f53\u7684\u6269 \u5c55\uff0c\u8fd9\u4e9b\u7f16\u7a0b\u8bed\u8a00\u76ee\u524d\u4e5f\u5bf9\u534f\u52a0\u901f\u5668\u63d0\u4f9b\u652f\u6301\u3002\u8fd9\u4e9b\u7f16\u7a0b\u8bed\u8a00\u53ef\u4ee5\u7528\u4e8e\u5b9e\u73b0\u4efb\u4f55\u7b97\u6cd5\uff0c\u4f46 \u7531\u4e8e\u8fd9\u4e9b\u7b97\u6cd5\u5f80\u5f80\u8f83\u4e3a\u590d\u6742\uff0c\u56e0\u6b64\u8fd9\u4e9b\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u8a00\u8f83\u96be\u505a\u5230\u5c06\u7b97\u6cd5\u63cf\u8ff0\u4ee5\u53ca\u4f18\u5316\u5b8c\u5168\u89e3 X10 \u662fIBM\u5728\u7f8e\u56fdDARPA\u7684HPCS\uff08High Productivity Computing Systems\uff09\u9879\u76ee\u4e2d \u63d0\u51fa\u7684\u4e00\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5176\u57fa\u4e8eJAVA\u8fdb\u884c\u6269\u5c55\uff0c\u4f7f\u7528\u5f02\u6b65\u5212\u5206\u5730\u5740\u7a7a\u95f4\u6a21\u578b(APGAS Model) \u7ba1\u7406CPU\u4ee5\u53caGPU\uff0c\u5220\u9664\u4e86JAVA\u4e2d\u7684\u5e76\u884c\u63a7\u5236\u90e8\u5206\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u5e76\u53d1\u63a7\u5236\u5e93\u3002X10\u901a\u8fc7\u7b80\u5355 \u6269\u5c55\u540e\u4f7f\u7528\u81ea\u8eab\u7684\u8bed\u4e49\u66ff\u4ee3CUDA\u7684\u76f8\u5173\u8bed\u4e49\uff0c\u7b80\u5316\u4e86GPU\u7f16\u7a0b \u3002Chapel [21, 39] \u662f Cray\u5728HPCS\u9879\u76ee\u4e2d\u63d0\u51fa\u7684\u4e00\u79cd\u5e76\u884c\u7f16\u7a0b\u8bed\u8a00\uff0c\u5176\u521b\u9020\u6027\u7684\u63d0\u51fa\u4e86\u5168\u5c40\u89c6\u89d2\u7684\u5e76\u884c\u7f16\u7a0b\u6a21 \u5f0f\u3001\u6570\u636e\u5b9a\u4e49\u4e0e\u7b97\u6cd5\u5b9e\u73b0\u5206\u79bb\u3001\u4ee5\u53ca\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u3002\u4e3a\u4e86\u5b9e\u73b0\u76f8\u540c \u4ee3\u7801\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u4e0a\u6267\u884c\uff0cChapel\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u4f9b\u9884\u5b9a\u4e49\u7684\u63a5\u53e3 [40, 41] \uff0c\u901a\u8fc7\u5b9e\u73b0\u8fd9 \u4e9b\u63a5\u53e3\uff0c\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u6307\u660e\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\u7684\u6570\u636e\u7ba1\u7406\u65b9\u5f0f\u4ee5\u53ca\u5faa\u73af\u4e2d\u5404\u5faa\u73af\u5b9e\u4f8b\u7684\u5e76 \u884c\u65b9\u5f0f\u3002\u901a\u8fc7\u4ee5\u4e0a\u65b9\u5f0f\uff0cChapel\u7a0b\u5e8f\u76ee\u524d\u53ef\u4ee5\u4f7f\u7528\u591a\u6838CPU\u4ee5\u53caNVIDIA GPU \u8fdb\u884c\u6267 2.3.2\u7b97\u6cd5\u5b9e\u73b0\u4e0e\u4f18\u5316\u89e3\u8026\u7684\u7f16\u7a0b\u8bed\u8a00 \u76ee\u524d\uff0c\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u67d0\u4e9b\u7279\u6b8a\u9886\u57df\uff0c\u5df2\u7ecf\u6709\u67d0\u4e9b\u4e13\u7528\u8bed\u8a00\u53ef\u4ee5\u5c06\u7a0b\u5e8f\u7684\u7b97\u6cd5\u63cf\u8ff0\u548c \u4ee3\u7801\u751f\u6210\u5b8c\u5168\u89e3\u8026\u3002\u8be5\u7c7b\u9886\u57df\u4e13\u7528\u8bed\u8a00\u80fd\u591f\u5e2e\u52a9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u5bf9\u9886\u57df\u7b97\u6cd5\u8fdb\u884c\u63cf\u8ff0\uff0c\u5e76\u5728 \u4e0d\u5173\u5fc3\u5e95\u5c42\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u901a\u8fc7\u8c03\u7528\u4f18\u5316\u63a5\u53e3\u5c31\u5b8c\u6210\u7b97\u6cd5\u4f18\u5316\u4ee5\u53ca\u4ee3\u7801\u751f\u6210\u3002 \u4fe1\u53f7\u5904\u7406\u9886\u57df\u7684SPIRAL\u6846\u67b6 \uff0c\u5b9e\u73b0\u4e86\u7b97\u6cd5\u63cf\u8ff0\u4e0e\u4ee3\u7801\u751f\u6210\u4f18\u5316\u5206\u79bb\uff0c\u4e3a\u4fe1\u53f7\u5904\u7406 \u7b97\u6cd5\u63d0\u4f9b\u4e86\u4ece\u9876\u5c42\u7b97\u6cd5\u63cf\u8ff0\u5230\u5e95\u5c42\u4ee3\u7801\u5b9e\u73b0\u7684\u6620\u5c04\u3002\u5177\u4f53\u7684\uff0cSPIRAL\u6846\u67b6\u53ef\u4ee5\u5212\u5206\u4e3a\u7b97\u6cd5 \u63cf\u8ff0\u5c42\u3001\u4ee3\u7801\u751f\u6210\u5c42\u3001\u4ee5\u53ca\u6d4b\u8bd5\u4f18\u5316\u5c423\u5c42\u3002\u5176\u7b97\u6cd5\u63cf\u8ff0\u5c42\u4f7f\u7528\u4fe1\u53f7\u5904\u7406\u8bed\u8a00SPL\u5bf9\u4fe1\u53f7\u5904 \u7406\u7b97\u6cd5\u8fdb\u884c\u63cf\u8ff0\u3002\u4ee3\u7801\u751f\u6210\u5c42\u8d1f\u8d23\u5c06SPL\u8bed\u8a00\u63cf\u8ff0\u7684\u7b97\u6cd5\u8f6c\u5316\u4e3aC\u6216Fortran\u4ee3\u7801\uff0c\u5728\u4ee3\u7801 \u8f6c\u5316\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u6d89\u53ca\u5927\u91cf\u4f18\u5316\u53c2\u6570\u7684\u9009\u62e9\uff0c\u5982\u5faa\u73af\u5c55\u5f00\u5c42\u6570\u4ee5\u53ca\u5faa\u73af\u5206\u5757\u5927\u5c0f\u7b49\uff0c\u4f18\u5316\u7a7a \u95f4\u975e\u5e38\u5de8\u5927\uff0c\u4f8b\u5982\u4f7f\u7528SPL\u63cf\u8ff0\u7684DCT-264\u7b97\u6cd5\u5c31\u5b58\u5728\u9ad8\u8fbe\u79cd\u8f6c\u5316\u9009\u62e9\u3002\u5982\u679c\u4f7f\u7528\u7a77\u4e3e\u7684\u65b9 \u6cd5\u4ece\u4f18\u5316\u7a7a\u95f4\u4e2d\u9009\u62e9\u6700\u4f18\u7684\u4ee3\u7801\u8f6c\u5316\u65b9\u6848\uff0c\u5219\u65f6\u95f4\u5f00\u9500\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002\u4e3a\u6b64\uff0cSPIRAL\u6846\u67b6 \u7684\u6d4b\u8bd5\u4f18\u5316\u5c42\u91c7\u7528\u4e86\u81ea\u52a8\u4f18\u5316\u7b56\u7565\uff0c\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u6216\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u63a7\u5236\u4ee3\u7801\u751f\u6210\u5c42\u751f\u6210\u4ee3 \u7801\u3002SPIRAL\u6846\u67b6\u76ee\u524d\u53ef\u4ee5\u4f7f\u7528\u591a\u79cd\u4e0d\u540c\u67b6\u6784\u7684\u591a\u6838\u5904\u7406\u5668\uff0c\u6682\u4e0d\u652f\u6301GPU\u7b49\u52a0\u901f\u8bbe\u5907\u3002 \u56fe\u50cf\u5904\u7406\u9886\u57df\u7684Halide \u8bed\u8a00\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u7684\u63cf\u8ff0\u4e0e\u4f18\u5316\u7684\u5206\u79bb\u3002\u4e00\u4e2a Halide\u7a0b\u5e8f\u7531\u7b97\u6cd5\u63cf\u8ff0\u4ee5\u53ca\u8c03\u5ea6\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u7b97\u6cd5\u63cf\u8ff0\u90e8\u5206\u53ef\u4ee5\u9ad8\u6548\u5b9e\u73b0\u5e38\u89c1\u7684\u56fe\u50cf\u5904\u7406 \u7b97\u6cd5\uff0c\u5982\u53bb\u9664\u7ea2\u773c\u3001\u67d4\u5316\u9634\u5f71\u6216\u589e\u52a0\u9971\u548c\u5ea6\u7b49\uff0c\u8c03\u5ea6\u4f18\u5316\u90e8\u5206\u5219\u6839\u636e\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\u7684\u67b6\u6784 \u6307\u5b9a\u4e00\u4e9b\u4f18\u5316\u7b56\u7565\uff0c\u5982\u8ba1\u7b97\u5206\u5757\u3001\u5e76\u884c\u65b9\u5f0f\u3001\u5411\u91cf\u5316\u65b9\u5f0f\u7b49\u3002\u4f7f\u7528Halide\u8bed\u8a00\u7684\u7a0b\u5e8f\u5f00\u53d1 \u4eba\u5458\u5982\u679c\u60f3\u8981\u5c06\u7a0b\u5e8f\u79fb\u690d\u5230\u91c7\u7528\u4e0d\u540c\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u4e0a\uff0c\u53ea\u9700\u5bf9\u8c03\u5ea6\u4f18\u5316\u90e8\u5206\u8fdb\u884c\u53d8\u66f4\u3002 \u4e0d\u540c\u7684\u8ba1\u7b97\u8bbe\u5907\u6709\u7740\u4e0d\u540c\u7684\u4f18\u5316\u65b9\u6cd5\uff0cHalide\u8ba4\u4e3a\u6240\u6709\u7684\u4f18\u5316\u65b9\u6cd5\u5f52\u6839\u7ed3\u5e95\u90fd\u662f\u5bf9\u5b58\u50a8\u6216 \u8ba1\u7b97\u987a\u5e8f\u7684\u63a7\u5236\uff0c\u56e0\u6b64\u5176\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8c03\u5ea6\u4f18\u5316\u63a5\u53e3\uff0c\u63a7\u5236\u7b97\u6cd5\u6267\u884c\u8fc7\u7a0b\u4e2d\u8bbf\u5b58\u548c\u8ba1\u7b97\u987a \u5e8f\uff0c\u5e2e\u52a9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u63cf\u8ff0\u6027\u80fd\u4f18\u5316\u65b9\u6848\u3002\u4e00\u65e6\u8c03\u5ea6\u4f18\u5316\u7b56\u7565\u786e\u5b9a\uff0cHalide\u5c06\u81ea\u52a8\u7ed3\u5408\u7b97 \u6cd5\u63cf\u8ff0\u3001\u8c03\u5ea6\u4f18\u5316\u7b56\u7565\u3001\u4ee5\u53ca\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\uff0c\u751f\u6210\u76f8\u5e94\u7684\u4ee3\u7801\u3002Halide\u76ee\u524d\u53ef\u4ee5\u4f7f\u7528\u7684\u8ba1 \u7b97\u8bbe\u5907\u5305\u62ecCPU\u3001GPU\u3001DSP\uff0cFPGA\u3001\u4ee5\u53caASIC\u3002Google\u7684Pixel 2\u624b\u673a\u9879\u76ee\u4f7f\u7528Halide\u5b9e\u73b0 \u4e86HDR+\u7b97\u6cd5\uff0c\u4f7f\u5176\u8fd0\u884c\u4e8eIPU\u82af\u7247\u4e0a\u3002\u4f7f\u7528Halide\u63d0\u4f9b\u7684\u8c03\u5ea6\u4f18\u5316\u63a5\u53e3\uff0c\u7a0b\u5e8f\u5458\u53ef\u4ee5\u65b9\u4fbf \u7684\u5c1d\u8bd5\u4e0d\u540c\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u53d1\u73b0\u4e00\u4e9b\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4f46\u662f\u7531\u4e8e\u9700\u8981\u4eba\u5de5\u4e0d\u65ad\u5c1d\u8bd5\uff0c\u56e0\u6b64 \u6548\u7387\u5e76\u4e0d\u9ad8\u3002\u4e3a\u4e86\u63d0\u9ad8\u4f18\u5316\u6548\u7387\uff0cHailde\u501f\u9274PolyMage\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u4f18\u5316\u7b56\u7565\u3002 \u56fe1: \u4f7f\u7528TVM\u89e3\u8026\u77e9\u9635\u4e58\u6cd5\u7b97\u6cd5\u63cf\u8ff0\u4ee5\u53ca\u7b97\u6cd5\u4f18\u5316 \u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u7814\u7a76\u5341\u5206\u6d3b\u8dc3\uff0c\u6bcf\u5929\u90fd\u6709\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff08\u5c42\uff09\u88ab\u63d0\u51fa\u6765\uff0c\u4ee5 \u671f\u671b\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u7cbe\u786e\u5ea6\u3002\u540c\u65f6\uff0c\u7531\u4e8e\u8d8a\u6765\u8d8a\u591a\u7684\u5382\u5546\u5f00\u59cb\u8bbe\u8ba1\u5236\u4f5c\u795e\u7ecf\u7f51\u7edc\u82af\u7247\uff0c \u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u65f6\u4f1a\u6709\u8d8a\u6765\u8d8a\u591a\u7684\u540e\u7aef\u8ba1\u7b97\u8bbe\u5907\u53ef\u4f9b\u9009\u62e9\u3002\u7531\u4e8e\u65e2\u8981\u652f\u6301\u5c42\u51fa\u4e0d\u7a77\u7684\u795e \u7ecf\u7f51\u7edc\u7b97\u5b50\uff08\u5c42\uff09\uff0c\u53c8\u8981\u4fdd\u8bc1\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff08\u5c42\uff09\u53ef\u4ee5\u8fd0\u884c\u4e8e\u4e0d\u540c\u7684\u540e\u7aef\u8ba1\u7b97\u8bbe\u5907\u4e4b \u4e0a\uff0c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u9762\u4e34\u7740\u6781\u5927\u7684\u6311\u6218\uff0c\u7531\u6b64\u5bfc\u81f4\u4e86\u4f17\u591a\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\u9879\u76ee\u7684\u51fa \u73b0\u3002TVM \u662f\u4e00\u5957\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u6846\u67b6\uff0c\u5176\u5305\u62ec\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u56fe\u4f18\u5316\u5c42\u4ee5\u53ca\u7b97\u5b50 \u4f18\u5316\u5c42\u3002\u56fe\u4f18\u5316\u5c42\u4f1a\u6267\u884c\u7b97\u5b50\u5408\u5e76\u7b49\u64cd\u4f5c\u4ee5\u5bf9\u8ba1\u7b97\u56fe\u8fdb\u884c\u4f18\u5316\uff1b\u7b97\u5b50\u4f18\u5316\u5c42\u4f1a\u9488\u5bf9\u7279\u5b9a\u540e \u7aef\u8ba1\u7b97\u8bbe\u5907\uff0c\u4e3a\u8ba1\u7b97\u56fe\u4e2d\u7684\u7b97\u5b50\u751f\u6210\u4f18\u5316\u7684\u4ee3\u7801\u3002\u7b97\u5b50\u4f18\u5316\u5c42\u501f\u9274\u4e86Halide\u8bed\u8a00\u7684\u601d\u60f3\uff0c \u5c06\u7b97\u5b50\u63cf\u8ff0\u548c\u4ee3\u7801\u751f\u6210\u89e3\u8026\uff0c\u4f7f\u7528\u4e00\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7684\u5f20\u91cf\u8bed\u8a00\u63cf\u8ff0\u7b97\u5b50\uff0c\u540c\u65f6\u4ece\u4e0d\u540c\u7684\u540e\u7aef \u8ba1\u7b97\u8bbe\u5907\u62bd\u8c61\u51fa\u4e86\u4e00\u6574\u5957\u4f18\u5316\u8c03\u5ea6\u4f18\u5316\u63a5\u53e3\u3002\u56fe1\u63cf\u8ff0\u4e86\u4f7f\u7528\u5f20\u91cf\u8bed\u8a00\u5bf9\u77e9\u9635\u4e58\u7b97\u6cd5\u8fdb\u884c \u63cf\u8ff0\uff0c\u4ee5\u53ca\u901a\u8fc7\u8c03\u5ea6\u4f18\u5316\u63a5\u53e3\u5bf9\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002\u5f53\u786e\u5b9a\u4e86\u540e\u7aef\u8ba1\u7b97\u8bbe\u5907\u4e4b\u540e\uff0c\u7b97\u5b50\u4f18\u5316\u5c42 \u91c7\u7528\u81ea\u52a8\u4f18\u5316\u7684\u7b56\u7565\uff0c\u4e0d\u65ad\u5728\u7528\u6237\u6307\u5b9a\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u641c\u7d22\u8c03\u5ea6\u4f18\u5316\u7b56\u7565\u3002TVM\u76ee\u524d\u53ef\u4ee5\u4f7f \u7528\u7684\u8ba1\u7b97\u8bbe\u5907\u5305\u62ecCPU\u3001GPU\u3001DSP\uff0cFPGA\u7b49\u3002 \u4e0eTVM\u7c7b\u4f3c\u7684\u8fd8\u6709Google\u5f00\u53d1\u7684XLA\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\uff0cXLA\u7684\u4f18\u5316\u6d41\u7a0b\u53ef\u4ee5\u5206\u6210\u76ee\u6807\u65e0 \u5173\u4f18\u5316\u548c\u76ee\u6807\u76f8\u5173\u4f18\u5316\u3002\u5176\u76ee\u6807\u65e0\u5173\u4f18\u5316\u5c42\u5bf9HLO\u63cf\u8ff0\u7684Tensorflow\u8ba1\u7b97\u56fe\u8fdb\u884c\u4f18\u5316\uff1b\u800c \u76ee\u6807\u76f8\u5173\u4f18\u5316\u5c42\u5219\u5c06Tensorflow\u7684\u7b97\u5b50\u7f16\u8bd1\u5230\u4e0d\u540c\u7684\u8ba1\u7b97\u8bbe\u5907\u4e0a\uff08Nvidia GPU\uff0c\u5f20\u91cf\u5904\u7406 \u5668\u5355\u5143TPU\u7b49\uff09\u3002 2. \u7f16\u7a0b\u6a21\u578b\u5c0f\u7ed3 \u5f53\u524d\u5f02\u6784\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u901a\u5e38\u4ee5\u7f16\u7a0b\u63a5\u53e3\u3001\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u3001\u4ee5\u53ca\u7f16\u7a0b\u8bed\u8a00\u7684\u5f62\u5f0f\u4e3a\u7a0b\u5e8f \u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u8ba1\u7b97\u8bbe\u5907\u62bd\u8c61\uff0c\u4f7f\u7a0b\u5e8f\u80fd\u591f\u5145\u5206\u5229\u7528\u5404\u79cd\u8ba1\u7b97\u8bbe\u5907\u7684\u8d44\u6e90\u3002 \u5728\u7f16\u7a0b\u63a5\u53e3\u65b9\u9762\uff0cOpenCL\u4f5c\u4e3a\u8f83\u65e9\u88ab\u63d0\u51fa\u7684\u6807\u51c6\u7f16\u7a0b\u63a5\u53e3\uff0c\u5df2\u7ecf\u5f97\u5230\u4e86\u8f83\u4e3a\u5e7f\u6cdb\u7684\u5e94 \u7528\u3002\u7531\u4e8eOpenCL\u7f16\u7a0b\u63a5\u53e3\u8f83\u4e3a\u5e95\u5c42\uff0c\u4e3a\u4e86\u63d0\u9ad8\u7a0b\u5e8f\u5f00\u53d1\u6548\u7387\uff0c\u4e00\u4e9b\u5de5\u4f5c\u5c06OpenCL\u4e0e\u5e95\u5c42\u7ec6 \u8282\u76f8\u5173\u7684\u64cd\u4f5c\u8fdb\u884c\u4e86\u7b80\u5316\uff0c\u5e76\u628a\u4e00\u4e9b\u5e38\u7528\u7684\u7b97\u6cd5\u7528OpenCL\u8fdb\u884c\u4e86\u5b9e\u73b0\u3002Kokkos\u3001RAJA\u3001\u4ee5 \u53caOP2\u5219\u53ef\u4ee5\u4f7f\u7528\u7edf\u4e00\u7684\u7f16\u7a0b\u63a5\u53e3\u9002\u914d\u4e0d\u540c\u7684\u5e76\u884c\u7f16\u7a0b\u6807\u51c6\uff0c\u4ece\u800c\u5e2e\u52a9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4f7f\u7528 \u66f4\u591a\u7684\u8ba1\u7b97\u8bbe\u5907\u3002 \u5728\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u65b9\u9762\uff0cOpenACC\u3001HMPP\u3001OpenMP 4.X\u53ef\u4ee5\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u5feb\u901f\u4f7f\u7528\u4e0d\u540c \u534f\u52a0\u901f\u5668\u5bf9\u4ee3\u7801\u8fdb\u884c\u52a0\u901f\u3002Mint\u5219\u53ef\u4ee5\u5e2e\u52a9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4f7f\u7528\u7b80\u5355\u76845\u6761\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u5c06 \u4e32\u884cStencil\u4ee3\u7801\u8f6c\u6362\u4e3aCUDA\u4ee5\u53caOpenCL\u4ee3\u7801\uff0c\u4ece\u800c\u5b9e\u73b0\u4ee3\u7801\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u95f4\u7684\u79fb\u690d\u3002 \u7136\u800c\uff0c\u7531\u4e8e\u7f16\u8bd1\u6307\u5bfc\u8bed\u53e5\u80fd\u591f\u63d0\u4f9b\u7ed9\u7f16\u8bd1\u5668\u7684\u4fe1\u606f\u4e0d\u591f\u4e30\u5bcc\uff0c\u56e0\u6b64\u5176\u4ee3\u7801\u52a0\u901f\u6548\u679c\u5f80\u5f80\u4e0d \u5982\u5e95\u5c42\u7f16\u7a0b\u63a5\u53e3\u597d\u3002 \u5728\u7f16\u7a0b\u8bed\u8a00\u65b9\u9762\uff0cChapel\u4ee5\u53caX10\u7b49\u5e76\u884c\u8ba1\u7b97\u8bed\u8a00\u7ecf\u8fc7\u7b80\u5355\u6269\u5c55\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528\u591a\u79cd\u4e0d \u540c\u7684\u8ba1\u7b97\u8bbe\u5907\u3002\u540c\u65f6\u5728\u67d0\u4e9b\u7279\u5b9a\u9886\u57df\uff0c\u9886\u57df\u4e13\u7528\u8bed\u8a00\u5df2\u7ecf\u53ef\u4ee5\u5c06\u7a0b\u5e8f\u7684\u7b97\u6cd5\u63cf\u8ff0\u548c\u4ee3\u7801\u751f \u6210\u89e3\u8026\uff0c\u8be5\u7c7b\u8bed\u8a00\u80fd\u591f\u5e2e\u52a9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u5728\u4e0d\u5173\u5fc3\u5e95\u5c42\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8c03\u7528\u4f18\u5316\u63a5\u53e3 \u5b8c\u6210\u7b97\u6cd5\u4f18\u5316\u4ee5\u53ca\u4ee3\u7801\u751f\u6210\uff0c\u8f83\u597d\u7684\u5e73\u8861\u4e86\u7f16\u7a0b\u6548\u7387\u4ee5\u53ca\u4f18\u5316\u6548\u679c\u3002 3\u3001 \u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316 \u4e00\u76f4\u4ee5\u6765\uff0c\u5e76\u884c\u8ba1\u7b97\u9886\u57df\u7684\u5723\u676f\u662f\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u53ea\u9700\u8981\u7f16\u5199\u5b8c\u5168\u4e32\u884c\u7684\u4ee3\u7801\uff0c\u7531\u7f16\u8bd1 \u7cfb\u7edf\u6216\u8fd0\u884c\u65f6\u73af\u5883\u81ea\u52a8\u4ece\u4ee3\u7801\u4e2d\u53d1\u73b0\u5e76\u884c\u6027\uff0c\u5e76\u9762\u5411\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\u751f\u6210\u76f8\u5e94\u4ee3\u7801\uff0c\u901a\u8fc7\u4e32 \u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u76f4\u63a5\u5b9e\u73b0\u4ee3\u7801\u7684\u6027\u80fd\u79fb\u690d\u3002 3.1\u57fa\u4e8e\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u7684\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316 \u5bf9\u4e8e\u67d0\u4e9b\u7b26\u5408\u8981\u6c42\u7684\u4ee3\u7801\uff0c\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u5df2\u7ecf\u53ef\u4ee5\u5c06\u5176\u81ea\u52a8\u5e76\u884c\u5316\u3002\u591a\u9762\u4f53\u7f16\u8bd1\u6280 \u672f\u4e2d\u7684\u591a\u9762\u4f53\u6307\u7684\u662f\u5728\u5faa\u73af\u8fb9\u754c\u7684\u7ebf\u6027\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u5404\u5faa\u73af\u5b9e\u4f8b\u88ab\u5305\u56f4\u5728\u4e00\u4e2a\u7a7a\u95f4\u51f8\u591a\u9762 \u4f53\u5185\uff0c\u5728\u4e0d\u8fdd\u53cd\u4f9d\u8d56\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u5404\u5faa\u73af\u5b9e\u4f8b\u7684\u7a7a\u95f4\u4f4d\u7f6e\u8fdb\u884c\u91c7\u7528\u76f8\u540c\u7684\u8c03\u5ea6\u4f18\u5316\u7b56 \u7565\uff08\u4eff\u5c04\u53d8\u6362\uff09\u540e\uff0c\u5404\u4e2a\u5faa\u73af\u5b9e\u4f8b\u4ecd\u7136\u88ab\u88ab\u5305\u56f4\u5728\u4e00\u4e2a\u7a7a\u95f4\u51f8\u591a\u9762\u4f53\u5185\u3002 \u591a\u9762\u4f53\u7f16\u8bd1\u5de5\u5177\u4e3b\u8981\u7531\u62bd\u8c61\u5206\u6790\u3001\u8c03\u5ea6\u4f18\u5316\u3001\u4ee5\u53ca\u4ee3\u7801\u751f\u6210\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210\u3002\u62bd\u8c61\u5206\u6790 \u90e8\u5206\u4f5c\u4e3a\u591a\u9762\u4f53\u7f16\u8bd1\u5de5\u5177\u7684\u524d\u7aef,\u5176\u9996\u5148\u4ece\u8f93\u5165\u7684\u5faa\u73af\u4ee3\u7801\u4e2d\u8bc6\u522b\u8fed\u4ee3\u7a7a\u95f4\u548c\u8bbf\u5b58\u6620\u5c04;\u7136 \u540e,\u6839\u636e\u8fed\u4ee3\u7a7a\u95f4\u548c\u8bbf\u5b58\u6620\u5c04\uff0c\u8ba1\u7b97\u5faa\u73af\u5b9e\u4f8b\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u8c03\u5ea6\u53d8\u6362\u4f5c\u4e3a\u591a\u9762\u4f53\u7f16\u8bd1 \u5de5\u5177\u7684\u4e2d\u95f4\u4f18\u5316\u5c42,\u5176\u901a\u8fc7\u8c03\u7528\u7ebf\u6027\u6574\u6570\u89c4\u5212\u51fd\u6570\uff0c\u5728\u6ee1\u8db3\u5faa\u73af\u5b9e\u4f8b\u4f9d\u8d56\u5173\u7cfb\u7684\u524d\u63d0\u4e0b,\u7ed3 \u5408\u540e\u7aef\u8ba1\u7b97\u8bbe\u5907\u7684\u67b6\u6784\u7279\u70b9\uff0c\u6c42\u4e00\u4e2a\u5145\u5206\u6316\u6398\u4ee3\u7801\u5e76\u884c\u6027\u4ee5\u53ca\u6570\u636e\u5c40\u90e8\u6027\u7684\u4f18\u5316\u7b56\u7565\uff08\u901a \u8fc7\u4eff\u5c04\u53d8\u6362\u6539\u53d8\u5faa\u73af\u5b9e\u4f8b\u7684\u6267\u884c\u987a\u5e8f\uff09\u3002\u4ee3\u7801\u751f\u6210\u90e8\u5206\u4f5c\u4e3a\u591a\u9762\u4f53\u7f16\u8bd1\u5de5\u5177\u7684\u540e\u7aef, \u5176\u4ee5 \u8fed\u4ee3\u7a7a\u95f4\u3001\u4f9d\u8d56\u5173\u7cfb\u3001\u4ee5\u53ca\u8c03\u5ea6\u4f18\u5316\u7b56\u7565\u4f5c\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u62bd\u8c61\u8bed\u6cd5\u6811\u5e76\u6700\u7ec8\u5c06\u62bd\u8c61\u8bed\u6cd5\u6811 \u8f6c\u53d8\u6210\u4ee3\u7801\u3002 \u56fe2: \u57fa\u4e8e\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u7684\u4e00\u822c\u7f16\u8bd1\u6d41\u7a0b \u5f53\u524d\u7684\u4e3b\u6d41\u7f16\u8bd1\u5668\u5df2\u9010\u6e10\u96c6\u6210\u4e86\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\uff0c\u5982 GCC \u96c6\u6210\u4e86Graphite \u6846\u67b6 [46, 47] \uff0cLLVM \u96c6\u6210\u4e86Polly\u6846\u67b6 \uff0c\u4ece\u800c\u9488\u5bf9\u7279\u5b9a\u8ba1\u7b97\u8bbe\u5907\u6539\u8fdb\u7f16\u8bd1\u7ed3\u679c\u7684\u5e76\u884c\u6027\u4ee5\u53ca\u8bbf \u5b58\u6027\u80fd\u3002\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u7f16\u8bd1\u6a21\u5757\u5d4c\u5165\u5230\u901a\u7528\u7f16\u8bd1\u5668\u4e2d,\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u72ec \u7acb\u7684\u7f16\u8bd1\u5de5\u5177, \u5b9e\u73b0\u4ece\u6e90\u7a0b\u5e8f\u5230\u6e90\u7a0b\u5e8f\u7684\u7ffb\u8bd1\u6d41\u7a0b\u3002\u9762\u5411\u5171\u4eab\u5185\u5b58\u7684\u591a\u6838\u67b6\u6784\uff0c\u591a\u9762\u4f53\u7f16 \u8bd1\u5de5\u5177Pluto \u53ef\u4ee5\u5c06\u4e32\u884c\u5faa\u73af\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u6210\u4e3aOpenMP\u4ee3\u7801\u3002\u9762\u5411GPU\u7b49\u8ba1\u7b97\u8bbe\u5907\uff0c Konstantinidis\u7b49\u4eba\u5f00\u53d1\u7684\u591a\u9762\u4f53\u7f16\u8bd1\u5de5\u5177 \u53ef\u4ee5\u5c06\u4e32\u884c\u4ee3\u7801\u8f6c\u6362\u6210CUDA\u4ee3\u7801,\u8be5\u5de5\u4f5c \u540e\u88abPPCG \u8d85\u8d8a\uff0cPPCG\u80fd\u591f\u652f\u6301Pencil\u8bed\u8a00 ,\u57fa\u4e8ePPCG+Pencil\u7684\u7ec4\u5408\uff0c\u7a0b\u5e8f\u5f00\u53d1\u4eba \u5458\u53ef\u4ee5\u91c7\u7528\u201c\u7f16\u8bd1\u5236\u5bfc+\u591a\u9762\u4f53\u7f16\u8bd1\u201d\u7684\u6a21\u5f0f\u5bf9\u7a0b\u5e8f\u8fdb\u884c\u4f18\u5316\u3002\u6b64\u5916\uff0cPolly-ACC \u57fa\u4e8e Polly\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece\u4e2d\u95f4\u8bed\u8a00\u5230\u4e2d\u95f4\u8bed\u8a00\u7684\u7ffb\u8bd1,\u5e76\u6700\u7ec8\u53ef\u4ee5\u751f\u6210OpenCL\u6216CUDA\u76ee\u6807\u4ee3\u7801\u3002 \u7531\u4e8ePolly-ACC\u7684\u64cd\u4f5c\u5bf9\u8c61\u4e3a\u6e90\u4ee3\u7801\u7f16\u8bd1\u540e\u7684IR\u4e2d\u95f4\u8bed\u8a00\uff0c\u5176\u53ef\u4ee5\u5bf9\u591a\u79cd\u8bed\u8a00\u7f16\u5199\u7684\u6e90\u4ee3 \u7801\u63d0\u4f9b\u4f18\u5316\u3002 \u7531\u4e8e\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u53ef\u4ee5\u81ea\u52a8\u53d1\u6398\u7a0b\u5e8f\u5e76\u884c\u6027\u548c\u6570\u636e\u5c40\u90e8\u6027\uff0c\u6781\u5927\u964d\u4f4e\u4e86\u7a0b\u5e8f\u5f00\u53d1\u4eba \u5458\u4f7f\u7528\u4e0d\u540c\u67b6\u6784\u8ba1\u7b97\u8bbe\u5907\u7684\u95e8\u69db\uff0c\u5176\u5f00\u59cb\u5f97\u5230\u4e86\u8d8a\u6765\u8d8a\u5e7f\u6cdb\u7684\u5173\u6ce8\u3002\u4f46\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u5e76 \u4e0d\u662f\u673a\u5668\u732b\u7684\u795e\u5947\u53e3\u888b\uff0c\u5176\u5e76\u4e0d\u80fd\u81ea\u52a8\u4e3a\u6240\u6709\u7684\u5faa\u73af\u4ee3\u7801\u627e\u51fa\u5e76\u884c\u4f18\u5316\u65b9\u6848\u3002\u9996\u5148\uff0c\u591a\u9762 \u4f53\u7f16\u8bd1\u6280\u672f\u4e25\u683c\u8981\u6c42\u8f93\u5165\u7a0b\u5e8f\u7684\u5faa\u73af\u8fb9\u754c\u4e3a\u5404\u5faa\u73af\u53d8\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\uff1b\u5176\u6b21\uff0c\u591a\u9762\u4f53\u7f16\u8bd1\u6280 \u672f\u5bf9\u4ee3\u7801\u7684\u63a7\u5236\u6d41\u7a0b\u6709\u8f83\u9ad8\u8981\u6c42\uff0c\u5bf9\u590d\u6742\u6761\u4ef6\u5224\u65ad\u8bed\u53e5\u7684\u652f\u6301\u4e0d\u8db3\uff1b\u6700\u540e\uff0c\u591a\u9762\u4f53\u7f16\u8bd1\u6280 \u672f\u7684\u4f9d\u8d56\u5206\u6790\u90e8\u5206\u8981\u6c42\u6570\u7ec4\u4e0b\u6807\u9700\u8981\u4e3a\u5faa\u73af\u53d8\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u5bf9\u590d\u6742\u4e0b\u6807\u7684\u652f\u6301\u4e0d\u8db3\u3002\u7531 \u4e8e\u4ee5\u4e0a\u8bf8\u591a\u9650\u5236\uff0c\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u76ee\u524d\u5e38\u88ab\u7528\u4e8e\u5bf9\u7b26\u5408\u8981\u6c42\u7684Stencil\u4ee3\u7801\u8fdb\u884c\u4f18\u5316\u3002 \u7531\u4e8e\u76ee\u524d\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff08\u5c42\uff09\u5f80\u5f80\u53ef\u4ee5\u7528\u7b26\u5408\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u8981\u6c42 \u7684Stencil\u4ee3\u7801\u63cf\u8ff0\uff0c\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u5df2\u7ecf\u5f00\u59cb\u88ab\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u7b97\u5b50\u4f18\u5316\u3002TVM\u4ee5\u53ca XLA\u7684\u7b97\u5b50\u4f18\u5316\u5c42\u5bf9\u4e8e\u6709\u4f53\u7cfb\u7ed3\u6784\u80cc\u666f\u77e5\u8bc6\u7684\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6848\uff0c\u4f46\u5bf9 \u4e8e\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8bbe\u8ba1\u4eba\u5458\u6765\u8bf4\u5374\u6709\u7740\u4e00\u5b9a\u7684\u95e8\u69db\u3002\u9488\u5bf9\u4ee5\u4e0a\u95ee\u9898\uff0cFacebook\u5f00\u53d1\u4e86 Tensor Comprehensions\uff08TC\uff09 , \u4ee5\u5e2e\u52a9\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8bbe\u8ba1\u4eba\u5458\u66f4\u65b9\u4fbf\u7684\u5c06\u4ee3\u7801\u90e8\u7f72 \u5230\u4e0d\u540c\u7684\u540e\u7aef\u8ba1\u7b97\u8bbe\u5907\u4e0a\u3002\u4e0eTVM\u548cXLA\u4e00\u6837\uff0cTC\u540c\u6837\u5b9a\u4e49\u4e86\u4e00\u79cd\u5f20\u91cf\u8868\u8fbe\u8bed\u8a00\uff0c\u4ee5\u65b9\u4fbf\u7b97 \u6cd5\u8bbe\u8ba1\u4eba\u5458\u5bf9\u7b97\u5b50\u7684\u8ba1\u7b97\u8fdb\u884c\u63cf\u8ff0\uff0c\u4e0eTVM\u548cXLA\u4e0d\u540c\u7684\u662f\uff0cTC\u628a\u7b97\u6cd5\u4f18\u5316\u90e8\u5206\u7528\u591a\u9762\u4f53\u7f16 \u8bd1\u6280\u672f\u5b9e\u73b0\u4e86\uff0c\u7528\u6237\u4e0d\u9700\u8981\u7f16\u5199\u4efb\u4f55\u4e0e\u4f18\u5316\u76f8\u5173\u7684\u8bed\u53e5\u3002\u4ee5GPU\u4e3a\u4f8b\uff0cTC\u53ef\u4ee5\u57fa\u4e8e\u591a\u9762\u4f53 \u7f16\u8bd1\u6280\u672f\u5b9e\u73b0\u5faa\u73af\u7684\u878d\u5408\u3001\u5206\u88c2\u3001\u5206\u5757\u3001\u4ee5\u53ca\u81ea\u52a8\u5e76\u884c\uff0c\u540c\u65f6\u8fd8\u786e\u4fdd\u6570\u636e\u5728\u590d\u6742\u5b58\u50a8\u5c42\u6b21 \u4e2d\u6b63\u786e\u79fb\u52a8\u3002\u4e3a\u4e86\u5bfb\u627e\u6700\u4f18\u7684\u8c03\u5ea6\u4f18\u5316\u7b56\u7565\uff0cTC\u91c7\u7528\u8fdb\u5316\u641c\u7d22\u7b97\u6cd5\u5bf9\u6d77\u91cf\u65b9\u6848\u8fdb\u884c\u8bc4\u4f30\uff0c \u5e76\u4ece\u4e2d\u9009\u62e9\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6848\u3002\u76ee\u524dTC\u80fd\u591f\u4f7f\u7528\u7684\u8ba1\u7b97\u8bbe\u5907\u4e3b\u8981\u4e3a\u4f17\u6838CPU\u4ee5\u53caGPU\u3002 3.2\u57fa\u4e8e\u9057\u4f20\u7f16\u7a0b\u7684\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316 Koza\u4e8e1992\u5e74\u63d0\u51fa\u4e86\u9057\u4f20\u7f16\u7a0b [55, 56] \uff0c\u9057\u4f20\u7f16\u7a0b\u5c5e\u4e8e\u8fdb\u5316\u7b97\u6cd5\uff0c\u5176\u7ee7\u627f\u4e86\u9057\u4f20\u7b97\u6cd5 \u7684\u57fa\u672c\u601d\u60f3\uff0c\u5373\u4ece\u57fa\u6570\u8f83\u4e3a\u5e9e\u5927\u7684\u539f\u59cb\u3001\u7c97\u7cd9\u7684\u7a0b\u5e8f\u79cd\u7fa4\u4e2d\u901a\u8fc7\u8bc4\u4f30\u9002\u5e94\u6027\u9009\u62e9\u7236\u7cfb\uff0c\u7136 \u540e\u8fdb\u884c\u53d8\u5f02\u3001\u4ea4\u53c9\u7b49\u9057\u4f20\u64cd\u4f5c\u751f\u6210\u65b0\u4e00\u4ee3\u7a0b\u5e8f\u79cd\u7fa4\uff0c\u518d\u5224\u65ad\u7ec8\u6b62\u6761\u4ef6\u51b3\u5b9a\u662f\u5426\u9700\u8981\u7ee7\u7eed\u8fed \u4ee3\u3001\u751f\u6210\u4e0b\u4e00\u4ee3\u79cd\u7fa4\u3002Koza\u5728\u8bba\u6587\u4e2d\u4f7f\u7528\u8bed\u6cd5\u6811\u8868\u793a\u7a0b\u5e8f\u4ee3\u7801\uff0c\u7a0b\u5e8f\u4ee3\u7801\u7684\u53d8\u5f02\u64cd\u4f5c\u901a\u8fc7 \u6539\u53d8\u8bed\u6cd5\u6811\u8282\u70b9\u4e0a\u7684\u64cd\u4f5c\u3001\u589e\u52a0\u6216\u5220\u9664\u5206\u652f\u3001\u6216\u7528\u4e00\u68f5\u5168\u65b0\u7684\u6811\u6765\u66ff\u6362\u5176\u5b50\u6811\u5b8c\u6210\uff1b\u800c\u7a0b \u5e8f\u4ee3\u7801\u7684\u4ea4\u53c9\u53ef\u4ee5\u901a\u8fc7\u7528\u4e00\u68f5\u8bed\u6cd5\u6811\u7684\u5206\u652f\u53d6\u4ee3\u53e6\u4e00\u68f5\u8bed\u6cd5\u6811\u7684\u5206\u652f\u5b8c\u6210\u3002\u9057\u4f20\u7f16\u7a0b\u5728\u751f \u6210\u590d\u6742\u63a7\u5236\u4ee3\u7801\u65b9\u9762\u53d6\u5f97\u4e86\u8f83\u5927\u7684\u6210\u529f\u3002\u6587\u6cd5\u8fdb\u5316\u7b97\u6cd5 \u662f\u57fa\u4e8e\u6587\u6cd5\u7684\u9057\u4f20\u7f16\u7a0b\uff0c\u5176\u901a \u8fc7\u9884\u5148\u6784\u5efa\u7684\u7f16\u7a0b\u8bed\u8a00\u7684BNF\u8303\u5f0f\uff0c\u5c06\u7a0b\u5e8f\u4ee3\u7801\u7684\u57fa\u56e0\u578b\uff08\u4e8c\u8fdb\u5236\u4e32\uff09\u4e0e\u8868\u73b0\u578b\uff08\u8bed\u6cd5 \u6811\uff09\u5bf9\u5e94\u4e86\u8d77\u6765\uff0c\u901a\u8fc7\u7b80\u5355\u66ff\u6362\u8bed\u8a00\u7684BNF\u8303\u5f0f\uff0c\u5c31\u53ef\u4ee5\u7528\u76f8\u540c\u7684\u7b97\u6cd5\u751f\u6210\u4e0d\u540c\u8bed\u8a00\u7684\u4ee3 \u7801\u3002\u56fe3\u5bf9\u4f7f\u7528BNF\u8303\u5f0f\u5c06\u7a0b\u5e8f\u4ee3\u7801\u57fa\u56e0\u578b\u4e0e\u8868\u73b0\u578b\u8fdb\u884c\u5bf9\u5e94\u7684\u8fc7\u7a0b\u505a\u4e86\u63cf\u8ff0\u3002Langdon\u7b49 \u4eba\u5728\u4ed6\u4eec\u7684\u5de5\u4f5c\u4e2d \u4f7f\u7528\u6587\u6cd5\u8fdb\u5316\u7b97\u6cd5\uff0c\u5c06gzip\u7a0b\u5e8f\u4e2d\u88ab\u9891\u7e41\u8c03\u7528\u7684\u4e32\u884c\u51fd\u6570 longest_match\u8f6c\u6362\u6210CUDA\u4ee3\u7801\u3002\u8be5\u5de5\u4f5c\u57fa\u4e8eCUDA\u63d0\u4f9b\u7684\u793a\u4f8b\u7a0b\u5e8fscan_naive_kernel.cu \u6784\u9020BNF\u8303\u5f0f\uff0c\u5e76\u4ee5\u6784\u9020\u7684BNF\u8303\u5f0f\u4e3a\u57fa\u7840\u4e0d\u65ad\u8fdb\u5316\u51fa\u65b0\u7684CUDA\u4ee3\u7801\u3002\u8fdb\u5316\u751f\u6210\u7684\u4ee3\u7801\u88ab\u4f7f \u7528nvcc\u7f16\u8bd1\u5668\u7f16\u8bd1\u5e76\u94fe\u63a5\u5230gzip\uff0c\u7136\u540e\u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u9a8c\u8bc1\u751f\u6210\u7684\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3002\u4f7f\u7528\u8be5\u65b9 \u6cd5\uff0c\u6700\u7ec8\u8fdb\u5316\u51fa\u4e86\u80fd\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\u7528\u4f8b\u7684CUDA\u4ee3\u7801\uff0c\u8be5CUDA\u4ee3\u7801\u540e\u671f\u5728\u4e0a\u767e\u4e07\u6b21\u8fd0\u884c\u8fc7\u7a0b \u4e2d\u5168\u90e8\u90fd\u80fd\u4ea7\u751f\u6b63\u786e\u7ed3\u679c\u3002\u4f7f\u7528\u540c\u6837\u7684\u65b9\u6cd5\uff0cGopinath\u7b49\u4eba\u5728\u4ed6\u4eec\u7684\u5de5\u4f5c\u4e2d [59-62] \u5c06\u4e32 \u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u6210\u8fd0\u884c\u4e8e\u591a\u6838\u8ba1\u7b97\u8bbe\u5907\u7684OpenMP\u4ee3\u7801\uff0c\u540c\u65f6\u4ed6\u4eec\u8fd8\u5c06\u4ee3\u7801\u8fdb\u5316\u7684\u8fc7\u7a0b\u5e76\u884c \u5316\uff0c\u4ee5\u52a0\u5feb\u4ee3\u7801\u751f\u6210\u901f\u5ea6\u3002\u4f7f\u7528\u9057\u4f20\u7f16\u7a0b\u9762\u4e34\u7684\u6700\u4e3b\u8981\u95ee\u9898\u662f\u6700\u7ec8\u8fdb\u5316\u51fa\u7684\u4ee3\u7801\u7684\u53ef\u89e3\u91ca \u6027\u8f83\u5dee\uff0c\u56e0\u6b64\u65e0\u6cd5\u786e\u4fdd\u4ee3\u7801\u5728\u6240\u6709\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u90fd\u80fd\u8f93\u51fa\u6b63\u786e\u7684\u7ed3\u679c\u3002 \u56fe3: \u6587\u6cd5\u8fdb\u5316\u7b97\u6cd5\u4f7f\u7528BNF\u8303\u5f0f\u5c06\u7a0b\u5e8f\u4ee3\u7801\u7684\u57fa\u56e0\u578b\u4e0e\u8868\u73b0\u578b\u5bf9\u5e94\u7684\u8fc7\u7a0b\u3002\u57fa\u56e0\u578b\u4e8c\u8fdb\u5236\u4e32\u88ab\u8f6c\u6362\u6210 \u6574\u578b\u4e32\uff0c\u6bcf\u4e2a\u6574\u578b\u5bf9\u5e94\u57fa\u56e0\u578b\u4e8c\u8fdb\u5236\u4e32\u4e2d\u76848\u4e2abit\u3002\u7b97\u6cd5\u4f9d\u6b21\u4f7f\u7528\u6574\u578b\u4e32\u4e2d\u7684\u6570\u5b57\u9009\u62e9BNF\u7684\u751f\u6210\u89c4\u5219\uff0c \u5e76\u5bf9\u8d77\u59cb\u7b26\u53f7\u4e0d\u65ad\u5e94\u7528\u8fd9\u4e9b\u9009\u4e2d\u7684\u89c4\u5219 3.3\u57fa\u4e8e\u7f16\u8bd1\u4fe1\u606f\u7684\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316 Nawata\u7b49\u4eba\u5f00\u53d1\u7684APTCC \u91c7\u7528\u6e90\u7801\u5206\u6790\u6280\u672f\uff0c\u5c06\u53ef\u5e76\u884c\u7684c\u8bed\u8a00for\u5faa\u73af\u8bed\u53e5\u8f6c\u6362 \u6210CUDA\u4ee3\u7801\u3002\u5728\u6267\u884c\u4ee5\u4e0a\u8f6c\u6362\u7684\u8fc7\u7a0b\u4e2dAPTCC\u5206\u6790\u9700\u8981\u4ece\u4e3b\u673a\u7aef\u4f20\u9001\u5230\u8bbe\u5907\u7aef\u7684\u6570\u636e\uff0c\u5e76 \u81ea\u52a8\u5c06for\u5faa\u73af\u8f6c\u6362\u6210\u5728GPU\u6d41\u5904\u7406\u5668\u4e0a\u6267\u884c\u7684\u51fd\u6570\u3002APTCC\u65e0\u6cd5\u81ea\u52a8\u5224\u65adfor\u5faa\u73af\u8bed\u53e5\u662f\u5426 \u53ef\u5e76\u884c,\u56e0\u6b64\u9700\u8981\u4f7f\u7528\u8005\u6307\u660e\u76ee\u6807for\u5faa\u73af\u3002\u89e3\u653e\u519b\u4fe1\u606f\u5de5\u7a0b\u5927\u5b66\u7684\u8d75\u8363\u5f69\u56e2\u961f\u501f\u52a9open64 \u7f16\u8bd1\u5668\u63d0\u4f9b\u7684\u4e00\u4e9b\u7f16\u8bd1\u4fe1\u606f\uff0c\u81ea\u52a8\u5bf9\u67d0\u4e9b\u7b26\u5408\u5e76\u884c\u6761\u4ef6\u7684for\u5faa\u73af\u6dfb\u52a0OpenACC\u7f16\u8bd1\u6307\u5bfc\u8bed \u53e5\uff0c\u4ece\u800c\u5b8c\u6210\u4ee3\u7801\u79fb\u690d\u5de5\u4f5c \u3002\u7531\u4e8eopen64\u4ec5\u80fd\u53d1\u73b0\u5f53\u524d\u5f62\u5f0f\u4e0bfor\u5faa\u73af\u7684\u53ef\u5e76\u884c\u6027\uff0c \u5e76\u4e0d\u80fd\u5bf9\u4ee3\u7801\u505a\u66f4\u8fdb\u4e00\u6b65\u7684\u53d8\u6362\u4ee5\u53ca\u8c03\u5ea6\uff0c\u4ece\u800c\u5bfc\u81f4\u8be5\u6846\u67b6\u4e0d\u80fd\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee3\u7801\u7684\u5e76\u884c\u6027 \u4ee5\u53ca\u6570\u636e\u672c\u5730\u6027\u3002 3.4\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u6280\u672f\u5c0f\u7ed3 \u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u662f\u5b9e\u73b0\u4ee3\u7801\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6700\u76f4\u63a5\u7684\u65b9\u6cd5\uff0c\u7136\u800c\u7531\u4e8e\u6280\u672f\u7684\u9650\u5236\uff0c \u5f53\u524d\u7684\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u9762\u4e34\u4e00\u4e9b\u56f0\u96be\u3002\u9996\u5148\uff0c\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u7531\u4e8e\u4e25\u683c\u8981\u6c42\u8f93\u5165\u7a0b\u5e8f \u7684\u5faa\u73af\u8fb9\u754c\u4ee5\u53ca\u6570\u7ec4\u4e0b\u6807\u4e3a\u5404\u5faa\u73af\u53d8\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u4e14\u5bf9\u590d\u6742\u6761\u4ef6\u5224\u65ad\u8bed\u53e5\u7684\u652f\u6301\u4e0d\u8db3\uff0c \u76ee\u524d\u5e38\u88ab\u7528\u4e8eStencil\u4ee3\u7801\u4f18\u5316\uff1b\u5176\u6b21\uff0c\u4f7f\u7528\u9057\u4f20\u7f16\u7a0b\u5c06\u4e32\u884c\u4ee3\u7801\u5e76\u884c\u5316\uff0c\u672c\u8d28\u4e0a\u662f\u7528\u7c7b \u4f3c\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u4e0d\u65ad\u751f\u6210\u4ee3\u7801\u5bf9\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u62df\u5408\uff0c\u56e0\u6b64\u65e0\u6cd5\u786e\u4fdd\u6700\u7ec8\u751f\u6210\u7684\u4ee3\u7801\u5728 \u63a5\u6536\u6d4b\u8bd5\u7528\u4f8b\u4e4b\u5916\u7684\u6570\u636e\u540e\u4e5f\u80fd\u8f93\u51fa\u6b63\u786e\u7684\u7ed3\u679c\uff1b\u6700\u540e\uff0c\u5f53\u524d\u7f16\u8bd1\u5668\u80fd\u591f\u63d0\u53d6\u7684\u4ee3\u7801\u7684\u4f18 \u5316\u4fe1\u606f\u5f80\u5f80\u6709\u9650\uff0c\u56e0\u6b64\u57fa\u4e8e\u7f16\u8bd1\u4fe1\u606f\u7684\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u6280\u672f\u4e0d\u80fd\u50cf\u591a\u9762\u4f53\u7f16\u8bd1\u6280\u672f\u4e00 \u6837\u901a\u8fc7\u6539\u8fdb\u4ee3\u7801\uff0c\u8fdb\u4e00\u6b65\u53d1\u6398\u4ee3\u7801\u7684\u5e76\u884c\u6027\u4ee5\u53ca\u6570\u636e\u672c\u5730\u6027\u3002 4\u3001\u9762\u5411\u6027\u80fd\u53ef\u79fb\u690d\u7684\u5e76\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u5de5\u5177 \u5bf9\u4e8e\u5df2\u7ecf\u9488\u5bf9\u67d0\u79cd\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u5b8c\u6210\u5e76\u884c\u5316\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee3\u7801\u8f6c\u6362\u5de5\u5177\u5c06\u5176\u81ea \u52a8\u8f6c\u6362\u4e3a\u53ef\u4ee5\u5728\u5176\u5b83\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4ee3\u7801\uff0c\u4ece\u800c\u5b9e\u73b0\u4ee3\u7801\u5728\u4e0d\u540c\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907 \u4e4b\u95f4\u7684\u79fb\u690d\u3002\u7531\u4e8e\u5df2\u7ecf\u901a\u8fc7\u7279\u5b9a\u65b9\u5f0f\u6307\u660e\u4e86\u4ee3\u7801\u7684\u5e76\u884c\u7b56\u7565\u4ee5\u53ca\u8bbf\u5b58\u65b9\u5f0f\uff0c\u56e0\u6b64\u65e0\u9700\u5bf9\u4ee3 \u7801\u8fdb\u884c\u590d\u6742\u7684\u5206\u6790\u4ee5\u786e\u5b9a\u4ee3\u7801\u7684\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u53ef\u5e76\u884c\u6027\uff0c\u53ea\u9700\u8981\u7ed3\u5408\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\u7684 \u67b6\u6784\u7279\u70b9\uff0c\u91cd\u65b0\u5b9e\u73b0\u5f53\u524d\u4ee3\u7801\u4e2d\u5df2\u6709\u7684\u5e76\u884c\u7b56\u7565\u4ee5\u53ca\u5b58\u50a8\u4f18\u5316\u65b9\u5f0f\u3002 4.1\u540c\u6784\u5230\u5f02\u6784\u67b6\u6784\u7684\u81ea\u52a8\u4ee3\u7801\u8f6c\u6362\u6280\u672f \u56fe4: OpenMPC\u6e90\u7801\u8f6c\u6362\u4ee5\u53ca\u4f18\u5316\u6d41\u7a0b OpenMP\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5171\u4eab\u5185\u5b58\u8ba1\u7b97\u673a\u7684\u5e76\u884c\u7f16\u7a0b\uff0cOpenMP\u53ef\u4ee5\u9ad8\u6548\u8868\u8fbefor\u5faa\u73af\u5404\u5faa\u73af \u5b9e\u4f8b\u4e4b\u95f4\u7684\u5e76\u884c\uff0c\u5176fork-join\u6a21\u578b\u4e2d\u4e3b\u7ebf\u7a0b\u4ee5\u53ca\u5de5\u4f5c\u7ebf\u7a0b\u7684\u6982\u5ff5\u4e5f\u80fd\u4e0eCUDA\u7f16\u7a0b\u6a21\u578b\u4e2d \u8fd0\u884c\u4e8eCPU\u7684\u4e3b\u7ebf\u7a0b\u4ee5\u53ca\u8fd0\u884c\u4e8eGPU\u7684\u5de5\u4f5c\u7ebf\u7a0b\u8f83\u597d\u7684\u5bf9\u5e94\u8d77\u6765\u3002Seyong\u7b49\u4eba\u5f00\u53d1\u7684\u6e90\u7801\u7f16 \u8bd1\u5668Cetus \uff0c\u53ef\u4ee5\u81ea\u52a8\u5c06OpenMP\u4ee3\u7801\u8f6c\u53d8\u6210CUDA\u4ee3\u7801\uff0c\u4ece\u800c\u5b9e\u73b0\u7a0b\u5e8f\u5728\u4e0d\u540c\u67b6\u6784\u8ba1\u7b97 \u8bbe\u5907\u4e4b\u95f4\u7684\u79fb\u690d\u3002\u5728\u6267\u884c\u4ee3\u7801\u8f6c\u6362\u7684\u8fc7\u7a0b\u4e2d\uff0cCetus\u9996\u5148\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684OpenMP\u6307\u5bfc\u8bed\u53e5\uff0c \u5e76\u5c06\u5176\u8f6c\u6362\u6210\u5bf9\u5e94\u7684CUDA\u7f16\u7a0b\u6a21\u578b\u4e2d\u7684\u8bed\u53e5\uff1b\u7136\u540e\uff0cCetus\u4ece\u7a0b\u5e8f\u4e2d\u62bd\u53d6\u51fa\u6838\u5fc3\u4ee3\u7801\uff0c\u5e76 \u5c06\u5176\u8f6c\u6362\u4e3aCUDA\u7684\u6838\u51fd\u6570\uff1b\u6700\u540e\uff0cCetus\u89e3\u6790\u51faGPU\u8981\u5904\u7406\u7684\u6570\u636e\uff0c\u5e76\u63d2\u5165CUDA\u6570\u636e\u4f20\u8f93\u8bed \u53e5\u3002Cetus\u9488\u5bf9CPU\u4ee5\u53caGPU\u7684\u67b6\u6784\u7279\u70b9\uff0c\u91c7\u7528\u4e86\u65b0\u578b\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u7b56\u7565\u3001\u5faa\u73af\u4ea4\u6362\u3001\u5faa\u73af \u5408\u5e76\u3001\u7ebf\u7a0b\u79c1\u6709\u6570\u7ec4\u6309\u5217\u4e3b\u5e8f\u6269\u5c55\u3001\u4ee5\u53ca\u6570\u636e\u7f13\u5b58\u7b49\u4e00\u7cfb\u5217\u6280\u672f\u4f18\u5316GPU\u7ebf\u7a0b\u7684\u8bbf \u5b58\u3002Cetus\u57fa\u4e8eJava\u8bed\u8a00\u5b9e\u73b0\uff0c\u5176\u901a\u8fc7\u6784\u9020\u62bd\u8c61\u8bed\u6cd5\u6811\u5e76\u5bf9\u5176\u8fdb\u884c\u4e00\u7cfb\u5217\u64cd\u4f5c\u5b8c\u6210\u6e90\u7801\u8f6c \u6362\u3002\u7531\u4e8eCetus\u7684\u6210\u529f\uff0cSeyong\u7b49\u4eba\u63d0\u51fa\u4e86OpenMPC \uff0cOpenMPC\u5728Cetus\u7684\u57fa\u7840\u4e0a\u5b9a\u4e49\u4e86 \u51e0\u7c7b\u6807\u8bb0\uff0c\u5206\u522b\u7528\u4e8e\u8ba9\u7528\u6237\u6307\u5b9a\u4e00\u4e9b\u4f18\u5316\u7ec6\u5219\u3001\u8bbe\u7f6e\u4f18\u5316\u53c2\u6570\u3001\u4ee5\u53ca\u4e3a\u73af\u5883\u53d8\u91cf\u8d4b\u503c\uff0c\u4ee5 \u4f9b\u5176\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\u8fdb\u884c\u8c03\u4f18\u3002\u56fe4\u63cf\u8ff0\u4e86OpenMPC\u6e90\u7801\u8f6c\u6362\u4ee5\u53ca\u4f18\u5316\u6d41\u7a0b\u3002Cetus\u4ee5\u53ca OpenMPC\u7684\u6e90\u7801\u8f6c\u6362\u673a\u5236\u590d\u6742\uff0c\u540c\u65f6\u7531\u4e8e\u751f\u6210\u7684\u662fCUDA\u4ee3\u7801\uff0c\u56e0\u6b64\u751f\u6210\u7684\u4ee3\u7801\u80fd\u4f7f\u7528\u7684\u8ba1 \u7b97\u8bbe\u5907\u7c7b\u578b\u6709\u9650\u3002\u738b\u71d5\u71d5\u7b49\u4eba\u5728\u4ed6\u4eec\u7684\u5de5\u4f5c\u4e2d \uff0c\u4f7f\u7528Clang\u7f16\u8bd1\u5668\u5b8c\u6210\u4e86OpenMP\u4ee3\u7801 \u5230OpenCL\u4ee3\u7801\u7684\u81ea\u52a8\u8f6c\u6362\uff0c\u5728\u6e90\u7801\u8f6c\u6362\u7684\u8fc7\u7a0b\u4e2d\u8fd8\u91c7\u7528\u4e86\u90e8\u5206\u4e0eCetus\u76f8\u4f3c\u7684\u8bbf\u5b58\u4f18\u5316\u6280 \u672f\u3002\u8be5\u5de5\u4f5c\u4f7f\u7528\u4e86Clang\u7f16\u8bd1\u5668\u4ee5\u7b80\u5316\u6e90\u7801\u8f6c\u6362\u6d41\u7a0b\uff0c\u540c\u65f6\u7531\u4e8e\u751f\u6210\u7684\u662fOpenCL\u4ee3\u7801\uff0c\u56e0 \u6b64\u80fd\u4f7f\u7528\u5404\u7c7b\u4e0d\u540c\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe\u5907\u3002 4.2\u5f02\u6784\u5230\u540c\u6784\u67b6\u6784\u7684\u81ea\u52a8\u4ee3\u7801\u8f6c\u6362\u6280\u672f \u4e0e\u4ee5\u4e0a\u5de5\u4f5c\u5b8c\u5168\u76f8\u53cd\uff0c\u5e76\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u5de5\u5177MCUDA \u53ef\u4ee5\u5c06CUDA\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u6210 \u5728\u5171\u4eab\u5185\u5b58\u67b6\u6784\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u7684\u4ee3\u7801\u3002GPU\u4e2d\u4e0d\u540c\u7ebf\u7a0b\u5757\u4e4b\u95f4\u5b8c\u5168\u72ec\u7acb\uff0c\u56e0\u6b64MCUDA\u5c06\u540c\u4e00 \u4e2a\u7ebf\u7a0b\u5757\u4e2d\u7684\u7ebf\u7a0b\u5168\u90e8\u6620\u5c04\u5230\u540c\u4e00\u4e2a\u5904\u7406\u5668\u6838\u5fc3\uff0c\u4ee5\u907f\u514d\u8ba1\u7b97\u6838\u5fc3\u4e4b\u95f4\u7684\u540c\u6b65\u64cd\u4f5c\u3002\u5bf9\u6bcf \u4e00\u4e2aGPU\u7ebf\u7a0b\u5757\uff0cMCUDA\u4f1a\u6784\u9020\u4e00\u4e2afor\u5faa\u73af\u987a\u5e8f\u6267\u884c\u5757\u5185\u6240\u6709GPU\u7ebf\u7a0b\u7684\u4ee3\u7801\uff0c\u5982\u679cGPU\u7ebf \u7a0b\u5757\u4ee3\u7801\u4e2d\u6709\u540c\u6b65\u70b9\uff0c\u5728\u6784\u9020for\u5faa\u73af\u65f6\u4f1a\u5220\u9664\u540c\u6b65\u8bed\u53e5\uff0c\u5e76\u4f7f\u7528\u5faa\u73af\u5206\u88c2\u6280\u672f\u4ece\u540c\u6b65\u70b9 \u5904\u5c06for\u5faa\u73af\u62c6\u5206\u3002MCUDA\u5728\u6784\u9020for\u5faa\u73af\u65f6\uff0c\u4f1a\u5bf9GPU\u7ebf\u7a0b\u5757\u4e2d\u6bcf\u4e2aGPU\u7ebf\u7a0b\u7684\u79c1\u6709\u6570\u636e\u7684 \u4f5c\u7528\u8303\u56f4\u8fdb\u884c\u5206\u6790\uff0c\u5bf9\u4e8e\u4f5c\u7528\u8303\u56f4\u8d85\u51fafor\u5faa\u73af\u7684\u53d8\u91cf\uff0cMCUDA\u4f1a\u5c06\u5176\u6269\u5c55\u6210\u4e3a\u6570\u7ec4\uff0c\u6bcf\u4e00 \u4e2a\u5faa\u73af\u5b9e\u4f8b\u5bf9\u5e94\u4e00\u4e2a\u6570\u7ec4\u5143\u7d20\uff0c\u800c\u5bf9\u4e8e\u4f5c\u7528\u8303\u56f4\u4ec5\u9650\u4e8efor\u5faa\u73af\u7684\u53d8\u91cf\u5219\u4e0d\u6269\u5c55\u3002\u5728\u8fdb\u884c \u4ee3\u7801\u8f6c\u6362\u7684\u8fc7\u7a0b\u4e2d\uff0cMCUDA\u5c06\u4e00\u4e2aGPU\u7ebf\u7a0b\u5757\u8981\u8bbf\u95ee\u7684\u6570\u636e\u5b58\u50a8\u5728\u8ba1\u7b97\u6838\u5fc3\u72ec\u6709\u7684L1\u7ea7\u7f13\u5b58 \u4e2d\uff0c\u800c\u5bf9\u4e8e\u5404\u7ebf\u7a0b\u5757\u540c\u65f6\u8bbf\u95ee\u7684\u6570\u636e\u5219\u5b58\u50a8\u5728\u5404\u8ba1\u7b97\u6838\u5fc3\u5171\u4eab\u7684L2\u7ea7\u7f13\u5b58\u4e2d\u3002 Joel\u7b49\u4eba\u5f00\u53d1\u4e86Clacc \uff0c\u53ef\u4ee5\u5c06OpenACC\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u6210OpenMP\u4ee3\u7801\uff0c\u4f7f\u7528LLVM\u7f16 \u8bd1\u5668\u5bf9\u8f6c\u6362\u540e\u7684\u4ee3\u7801\u8fdb\u884c\u7f16\u8bd1,\u53ef\u4ee5\u5b9e\u73b0LLVM\u7f16\u8bd1\u5668\u5bf9OpenACC\u7684\u652f\u6301\u3002\u76f8\u6bd4\u4e8e OpenMP,OpenACC\u5bf9\u5e95\u5c42\u786c\u4ef6\u8fdb\u884c\u4e86\u66f4\u9ad8\u7a0b\u5ea6\u7684\u62bd\u8c61\uff0c\u56e0\u6b64\u8be5\u8f6c\u6362\u8fc7\u7a0b\u4e0e\u7f16\u8bd1\u5668\u7684\u81ea\u4e0a\u800c\u4e0b \u7684\u7f16\u8bd1\u6d41\u7a0b\u76f8\u4f3c\u3002\u9996\u5148\uff0cClacc\u4f7f\u7528LLVM\u7684\u524d\u7aef\u7f16\u8bd1\u5668Clang\u521b\u5efa\u5305\u542bOpenACC\u8282\u70b9\u7684\u865a\u62df \u8bed\u6cd5\u6811\uff0c\u7136\u540e\uff0c\u4f7f\u7528\u4e13\u95e8\u5f00\u53d1\u7684acc2omp\u5de5\u5177\u5c06OpenACC\u7684\u865a\u62df\u8bed\u6cd5\u6811\u8f6c\u6362\u6210OpenMP\u7684\u865a\u62df \u8bed\u6cd5\u6811\uff0c\u5e76\u5c06\u8be5\u865a\u62df\u8bed\u6cd5\u6811\u8f6c\u6362\u6210LLVM\u7684\u4e2d\u95f4\u8bed\u8a00\uff0c\u6700\u540e\uff0c\u4f7f\u7528LLVM\u540e\u7aef\u751f\u6210\u53ef\u6267\u884c\u4ee3 \u7801\u3002\u76ee\u524d\uff0cClacc\u5c06OpenACC\u4ee3\u7801\u8f6c\u6362\u4e3aOpenMP\u4ee3\u7801\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6ca1\u6709\u5f15\u5165\u8fc7\u591a\u7684\u4f18\u5316\u7b56\u7565\u3002 \u4e0eClacc\u7c7b\u4f3c\u7684\u8fd8\u6709Nawrin Sultana\u7b49\u4eba\u7684\u5de5\u4f5c \uff0c\u4ed6\u4eec\u5c06\u8bbe\u8ba1\u7684\u66ff\u6362\u7b97\u6cd5\u96c6\u6210\u5230 Eclipse C/C++ Development Tools\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5c06\u8fd0\u884c\u4e8eGPU\u7684OpenACC\u4ee3\u7801\u8f6c\u6362\u4e3aOpenMP \u4ee3\u7801\u3002 4.3\u5e76\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u5de5\u5177\u5c0f\u7ed3 \u5e76\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u53ef\u5c06\u6570\u91cf\u5e9e\u5927\u7684\u9057\u4ea7\u5e76\u884c\u4ee3\u7801\u8fc5\u901f\u8f6c\u6362\u4e3a\u53ef\u4ee5\u5728\u4e0d\u540c\u67b6\u6784\u7684\u8ba1\u7b97\u8bbe \u5907\u4e0a\u8fd0\u884c\u7684\u4ee3\u7801\uff0c\u56e0\u6b64\u5176\u6709\u7740\u91cd\u8981\u7684\u7814\u7a76\u4ef7\u503c\u3002\u7531\u4e8e\u53ea\u9700\u8981\u7ed3\u5408\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\u7684\u67b6\u6784\u7279\u70b9 \u91cd\u65b0\u5b9e\u73b0\u5e76\u884c\u4ee3\u7801\u4e2d\u5df2\u6709\u7684\u5e76\u884c\u7b56\u7565\u4ee5\u53ca\u5b58\u50a8\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u7814\u7a76\u65b9\u5411\u4e0d\u5b58\u5728\u7406\u8bba\u4e0a\u7684\u96be \u9898\u3002\u540c\u65f6\uff0c\u968f\u7740Clang\u7b49\u6e90\u7801\u5206\u6790\u5de5\u5177\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u5f00\u53d1\u5e76\u884c\u4ee3\u7801\u8f6c\u6362\u5de5\u5177\u7684\u6280\u672f\u95e8\u69db\u4e5f \u5728\u4e0d\u65ad\u964d\u4f4e\u3002\u7efc\u4e0a\uff0c\u8be5\u7814\u7a76\u65b9\u5411\u5c06\u6210\u4e3a\u6027\u80fd\u53ef\u79fb\u690d\u9886\u57df\u7684\u70ed\u70b9\u3002 5\u3001\u5176\u5b83 \u4e00\u4e2a\u5bf9\u5404\u79cd\u4e0d\u540c\u67b6\u6784\u8ba1\u7b97\u8bbe\u5907\u90fd\u63d0\u4f9b\u826f\u597d\u652f\u6301\u7684\u79d1\u5b66\u8ba1\u7b97\u5e93\u53ef\u4ee5\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u8282\u7701\u5927 \u91cf\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u8fd9\u662f\u5b9e\u73b0\u7a0b\u5e8f\u6027\u80fd\u53ef\u79fb\u690d\uff0c\u5e76\u63d0\u9ad8\u751f\u4ea7\u6548\u7387\u7684\u6709\u6548\u65b9\u5f0f\u3002\u5728\u8fc7\u53bb\u51e0\u5341\u5e74 \u4e2d\uff0c\u51fa\u73b0\u4e86\u5f88\u591a\u5e94\u7528\u5e7f\u6cdb\u4e14\u9488\u5bf9\u4e0d\u540c\u67b6\u6784\u8ba1\u7b97\u8bbe\u5907\u8fdb\u884c\u4f18\u5316\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u51fd\u6570\u5e93\uff0c\u5982\u57fa\u672c \u7ebf\u6027\u4ee3\u6570\u4f8b\u7a0b\uff08BLAS\uff09\u3001\u9ad8\u6269\u5c55\u7ebf\u6027\u4ee3\u6570\u5e93\uff08scalable LAPACK\uff09\u3001\u4ee5\u53ca\u897f\u65b9\u6700\u5feb\u5085\u91cc\u53f6 \u53d8\u6362\uff08FFTW\uff09\u7b49\u3002\u5c3d\u7ba1\u5f53\u524d\u8bb8\u591a\u7a0b\u5e8f\u5e7f\u6cdb\u4f7f\u7528\u4e86\u8fd9\u4e9b\u9488\u5bf9\u7279\u5b9a\u8ba1\u7b97\u8bbe\u5907\u9ad8\u5ea6\u4f18\u5316\u7684\u79d1\u5b66\u8ba1 \u7b97\u5e93\uff0c\u4f46\u662f\u4ecd\u7136\u9700\u8981\u5927\u91cf\u624b\u5199\u4ee3\u7801\u5bf9\u4e00\u4e2a\u5e93\u51fd\u6570\u7684\u8f93\u51fa\u8fdb\u884c\u9884\u5904\u7406\uff08\u6570\u636e\u5e03\u5c40\u66f4\u65b0\uff0c\u5b58\u50a8 \u5206\u914d\uff09\uff0c\u4ece\u800c\u5c06\u5176\u8f6c\u6362\u4e3a\u53e6\u5916\u4e00\u4e2a\u5e93\u51fd\u6570\u7684\u8f93\u5165\u3002\u80f6\u6c34\u4ee3\u7801\u4f1a\u5f71\u54cd\u7a0b\u5e8f\u7684\u53ef\u79fb\u690d\u6027\uff0c\u8fd9\u662f \u56e0\u4e3a\u80f6\u6c34\u4ee3\u7801\u5f80\u5f80\u662f\u9488\u5bf9\u7279\u5b9a\u8ba1\u7b97\u8bbe\u5907\u7f16\u5199\u7684\uff0c\u56e0\u6b64\u5728\u5c06\u5176\u79fb\u690d\u5230\u65b0\u7684\u8ba1\u7b97\u8bbe\u5907\u4e0a\u65f6\uff0c\u4e5f \u9700\u8981\u9488\u5bf9\u65b0\u7684\u8ba1\u7b97\u8bbe\u5907\u7684\u67b6\u6784\u8fdb\u884c\u4f18\u5316\u3002\u53e6\u5916\u5728\u4f7f\u7528\u534f\u52a0\u901f\u5668\u7684\u8ba1\u7b97\u8282\u70b9\u4e0a\uff0c\u80f6\u6c34\u4ee3\u7801\u8fd8 \u4f1a\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u4e3b\u673a\u5185\u5b58\u4e0e\u8bbe\u5907\u5185\u5b58\u4e4b\u95f4\u7684\u6570\u636e\u79fb\u52a8\u3002\u56fe5(a)\u4e2d\u57fa\u4e8efftwf_plan_dft_1d \u51fd\u6570\u5b9e\u73b0\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u4ee3\u7801\u4e2d\u5b58\u5728\u8f83\u591a\u80f6\u6c34\u4ee3\u7801\u4ee5\u5faa\u73af\u8c03\u7528fftwf_plan_dft_1d\u51fd\u6570 \u5e76\u8fdb\u884c\u6570\u636e\u91cd\u6392\uff0c\u800c\u56fe5(b)\u4e2d\u4f7f\u7528fftwf_plan_guru_dft\u51fd\u6570\u5bf9\u56fe5(a)\u4e2d\u7684\u4ee3\u7801\u8fdb\u884c\u91cd \u5199\uff0c\u80f6\u6c34\u4ee3\u7801\u7684\u6570\u91cf\u5927\u5927\u51cf\u5c11\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u56fe5(b)\u4e2d\u7684\u4ee3\u7801\u76f8\u5bf9\u4e8e\u56fe5(a)\u4e2d\u7684\u4ee3\u7801\u6709\u7740 \u663e\u8457\u7684\u6027\u80fd\u63d0\u5347 \u3002 \uff08a\uff09\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u539f\u59cb\u4ee3\u7801 \uff08b\uff09\u4ee3\u7801\u878d\u5408\u540e\u7684\u5085\u91cc\u53f6\u53d8\u6362\u4ee3\u7801 \u56fe5: \u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u4ee3\u7801\u4f18\u5316 \u4ee5\u4e0a\u793a\u4f8b\u663e\u793a\u4e86\u5408\u7406\u4f7f\u7528\u79d1\u5b66\u8ba1\u7b97\u5e93\u51fd\u6570\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7a0b\u5e8f\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u5e2e\u52a9\u7a0b\u5e8f \u5f00\u53d1\u4eba\u5458\u9009\u62e9\u5408\u9002\u7684\u79d1\u5b66\u8ba1\u7b97\u5e93\u51fd\u6570\u8fdb\u884c\u8ba1\u7b97\uff0c\u674e\u97e6\u7b49\u4eba\u5728\u4ed6\u4eec\u7684\u5de5\u4f5c\u4e2d\u5f00\u53d1\u4e86\u4e00\u4e2a\u6e90\u7801 \u7f16\u8bd1\u5668 \u4ee5\u4f18\u5316\u79d1\u5b66\u8ba1\u7b97\u5e93\u51fd\u6570\u7684\u4f7f\u7528\u3002\u9996\u5148\uff0c\u4ed6\u4eec\u5bf9The Spiral Language \u8fdb\u884c \u6539\u8fdb\uff0c\u8bbe\u8ba1\u4e86\u4e00\u5957\u901a\u7528\u7684\u9886\u57df\u8bed\u8a00,\u8be5\u9886\u57df\u8bed\u8a00\u652f\u6301\u57fa\u672c\u7684\u6570\u5b66\u8fd0\u7b97\u529f\u80fd\uff0c\u8fd8\u5305\u542b\u4e86\u4e00\u4e9b \u9ad8\u6027\u80fd\u8ba1\u7b97\u5e93\u51fd\u6570\u6e90\u8bed\uff0c\u8fd9\u4e9b\u6e90\u8bed\u4e0eFFTW\u4ee5\u53caBLAS\u7b49\u5e38\u7528\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e93\u4e2d\u7684\u51fd\u6570\u4e00\u4e00\u5bf9 \u5e94\uff0c\u540c\u65f6\u8be5\u9886\u57df\u8bed\u8a00\u5b9a\u4e49\u4e86\u4e00\u4e9b\u7b26\u53f7\u7528\u4e8e\u62bd\u8c61\u7a0b\u5e8f\u7684\u6570\u5b66\u8ba1\u7b97\u884c\u4e3a\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u57fa\u4e8e GAP4 \u5f00\u53d1\u4e86\u7f16\u8bd1\u5668\u524d\u7aef\uff0c\u8be5\u524d\u7aef\u80fd\u591f\u5c06\u56fe5\uff08a\uff09\u4e2d\u7684\u4ee3\u7801\u7f16\u8bd1\u6210\u56fe6(a)\u4e2d\u6240\u793a\u4f7f\u7528\u9886 \u57df\u8bed\u8a00\u63cf\u8ff0\u7684\u4ee3\u7801\uff0c\u56fe6(a)\u4e2d\u7684\u4ee3\u7801\u6309\u7167\u4f18\u5316\u89c4\u5219\u4f18\u5316\u540e\uff0c\u88ab\u8f6c\u6362\u4e3a\u56fe6(b)\u4e2d\u7684\u4ee3\u7801\u3002\u6700 \u540e\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u7f16\u8bd1\u5668\u540e\u7aef\uff0c\u8be5\u540e\u7aef\u80fd\u5c06\u56fe6(b)\u4e2d\u7684\u4ee3\u7801\u8f6c\u6362\u4e3a\u56fe5\uff08b\uff09\u4e2d\u7684\u4ee3\u7801\u3002 \uff08a\uff09\u9886\u57df\u8bed\u8a00\u63cf\u8ff0\u56fe5\uff08a\uff09\u4ee3\u7801 \uff08b\uff09\u9886\u57df\u8bed\u8a00\u63cf\u8ff0\u56fe5\uff08b\uff09\u4ee3\u7801 \u56fe6: \u5bf9\u4f7f\u7528\u9886\u57df\u8bed\u8a00\u63cf\u8ff0\u7684\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u8fdb\u884c\u4f18\u5316 6.\u603b\u7ed3\u4e0e\u5c55\u671b \u4e3a\u4f5c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u4e00\u4e2a\u91cd\u8981\u76ee\u6807\uff0c\u6027\u80fd\u53ef\u79fb\u690d\u7684\u6982\u5ff5\u5df2\u7ecf\u51fa\u73b0\u4e86\u5f88\u591a\u5e74\u3002\u4e3a\u4e86\u7a81\u7834 \u6469\u5c14\u5b9a\u5f8b\u7684\u9650\u5236\uff0c\u5f53\u524d\u8d85\u7b97\u7cfb\u7edf\u7684\u8ba1\u7b97\u8282\u70b9\u901a\u5e38\u5c06\u901a\u7528\u5904\u7406\u5668\u4e0e\u4e00\u4e2a\u6216\u591a\u4e2a\u534f\u52a0\u901f\u5668\u5728\u4e3b \u677f\u6216\u7247\u4e0a\u76f8\u8fde\uff0c\u901a\u7528\u5904\u7406\u5668\u63a7\u5236\u8c03\u5ea6\u8ba1\u7b97\u4efb\u52a1\uff0c\u800c\u534f\u52a0\u901f\u5668\u6267\u884c\u5e76\u884c\u8ba1\u7b97\u4efb\u52a1\u3002\u5f53\u524d\u8d85\u7ea7 \u8ba1\u7b97\u673a\u5927\u591a\u91c7\u7528\u901a\u7528GPU\u4ee5\u53caIntel Xeon Phi\u4f5c\u4e3a\u534f\u52a0\u901f\u5668\uff0c\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u9886\u57df \u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u4e13\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u82af\u7247\u4e5f\u5f00\u59cb\u4e0d\u65ad\u6d8c\u73b0\u3002\u4e0d\u540c\u67b6\u6784\u7684\u534f\u52a0\u901f\u5668\u7684\u51fa\u73b0\u53ca\u5728 \u8d85\u7ea7\u8ba1\u7b97\u673a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u7a0b\u5e8f\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u4e4b\u95f4\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u63d0\u51fa\u4e86\u6311\u6218\u3002 \u672c\u6587\u4ece\u7f16\u7a0b\u6a21\u578b\u3001\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u3001\u5e76\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u7b49\u65b9\u9762\u5bf9\u5f53\u524d\u7684\u6027\u80fd\u53ef\u79fb \u690d\u6280\u672f\u8fdb\u884c\u4e86\u5f52\u7eb3\u4ecb\u7ecd\uff0c\u540c\u65f6\u5728\u6587\u7ae0\u7684\u6700\u540e\u8fd8\u603b\u7ed3\u4e86\u5982\u4f55\u901a\u8fc7\u5408\u7406\u4f7f\u7528\u79d1\u5b66\u8ba1\u7b97\u5e93\u51fd\u6570\uff0c \u8fbe\u5230\u63d0\u5347\u7a0b\u5e8f\u7684\u6027\u80fd\u4ee5\u53ca\u6027\u80fd\u53ef\u79fb\u690d\u6027\u7684\u76ee\u6807\u3002\u5bf9\u4e8e\u80fd\u591f\u6ee1\u8db3\u6027\u80fd\u53ef\u79fb\u690d\u8981\u6c42\u7684\u7f16\u7a0b\u6a21 \u578b\uff0c\u62bd\u8c61\u5c42\u6b21\u4f4e\u7684\u7f16\u7a0b\u6a21\u578b\u53ef\u4ee5\u5e2e\u52a9\u6709\u7ecf\u9a8c\u7684\u5f00\u53d1\u4eba\u5458\u5b9e\u73b0\u9ad8\u6548\u7684\u4ee3\u7801\uff0c\u4f46\u662f\u4e5f\u5bfc\u81f4\u4e86\u5176 \u4f7f\u7528\u96be\u5ea6\u8f83\u9ad8\uff0c\u800c\u62bd\u8c61\u5c42\u6b21\u9ad8\u7684\u7f16\u7a0b\u6a21\u578b\u964d\u4f4e\u4e86\u4ee3\u7801\u7684\u5f00\u53d1\u96be\u5ea6\uff0c\u4f46\u5bf9\u7f16\u8bd1\u5668\u63d0\u51fa\u4e86\u8f83\u9ad8 \u7684\u8981\u6c42\u3002\u7531\u4e8e\u6280\u672f\u7684\u9650\u5236\uff0c\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u6280\u672f\u76ee\u524d\u53ea\u80fd\u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u88ab\u4f7f\u7528\u3002\u7531 \u4e8e\u5e76\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u6280\u672f\u5728\u7406\u8bba\u548c\u6280\u672f\u4e0a\u4e0d\u5b58\u5728\u5f88\u9ad8\u7684\u96be\u5ea6\uff0c\u5f53\u524d\u5df2\u7ecf\u6709\u8f83\u591a\u7814\u7a76\u7ed3\u5408\u76ee \u6807\u8ba1\u7b97\u8bbe\u5907\u7684\u67b6\u6784\u7279\u70b9\u91cd\u65b0\u5b9e\u73b0\u5e76\u884c\u4ee3\u7801\u4e2d\u5df2\u6709\u7684\u5e76\u884c\u7b56\u7565\u4ee5\u53ca\u5b58\u50a8\u4f18\u5316\u65b9\u6cd5\u3002\u7efc\u4e0a\u6240 \u8ff0\uff0c\u4e0d\u540c\u5e94\u7528\u573a\u666f\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u7a0b\u5e8f\u6027\u80fd\u53ef\u79fb\u690d\u7684\u5b9e\u73b0\u6280\u672f,\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4e3a\u81ea\u5df1\u7684\u7a0b\u5e8f \u9009\u62e9\u6027\u80fd\u53ef\u79fb\u690d\u65b9\u6848\uff0c\u5176\u5b9e\u662f\u5728\u5404\u79cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u5bf9\u7f16\u7a0b\u6548\u7387\u4ee5\u53ca\u4f18\u5316\u6548\u679c\u8fdb\u884c\u5e73\u8861\u53d6\u820d\u3002 \u6839\u636e\u4ee5\u5f80\u7814\u7a76\u6210\u679c\u548c\u76ee\u524d\u6b63\u5728\u8fdb\u884c\u7684\u7814\u7a76\uff0c\u53ef\u4ee5\u5f97\u51fa\u4ee5\u4e0b\u7ed3\u8bba\uff1a1\uff09\u7a0b\u5e8f\u6027\u80fd\u53ef\u79fb\u690d \u6280\u672f\u6db5\u76d6\u8303\u56f4\u975e\u5e38\u5e7f\u6cdb\uff0c\u6bcf\u4e00\u79cd\u6280\u672f\u90fd\u6709\u5176\u6700\u9002\u7528\u7684\u8303\u56f4\u3002\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u4e3a\u81ea\u5df1\u7684\u7a0b\u5e8f\u9009 \u62e9\u6027\u80fd\u53ef\u79fb\u690d\u65b9\u6848\uff0c\u5176\u5b9e\u662f\u5728\u5404\u79cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u5bf9\u7f16\u7a0b\u6548\u7387\u4ee5\u53ca\u4f18\u5316\u6548\u679c\u8fdb\u884c\u5e73\u8861\u53d6\u820d\uff1b 2\uff09\u5c06\u7b97\u6cd5\u63cf\u8ff0\u4ee5\u53ca\u4f18\u5316\u89e3\u8026\u548c\u5bf9\u4e8e\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u6709\u7740\u91cd\u8981\u610f\u4e49\uff0c\u5f53\u524d\u4ec5\u6709\u5c11\u6570\u9886\u57df\u7f16 \u7a0b\u8bed\u8a00\u80fd\u591f\u5c06\u7b97\u6cd5\u63cf\u8ff0\u4ee5\u53ca\u4f18\u5316\u89e3\u8026\uff0c\u5e76\u63d0\u4f9b\u8ba9\u7f16\u7a0b\u4eba\u5458\u4ecb\u5165\u4ee3\u7801\u4f18\u5316\u7684\u63a5\u53e3\u3002\u5982\u4f55\u901a\u8fc7 \u8bbe\u8ba1\u65b0\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u5728\u66f4\u5e7f\u7684\u8303\u56f4\u5185\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba1\u7b97\u7a0b\u5e8f\u7b97\u6cd5\u63cf\u8ff0\u4ee5\u53ca\u4f18\u5316\u7684\u89e3\u8026\uff0c\u8fd8\u9700 \u8981\u8fdb\u884c\u91cd\u70b9\u7814\u7a76\uff1b3\uff09\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u5e76\u884c\u5316\u662f\u5b9e\u73b0\u4ee3\u7801\u6027\u80fd\u53ef\u79fb\u690d\u6700\u76f4\u63a5\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u591a \u9762\u4f53\u7f16\u8bd1\u6280\u672f\u5df2\u7ecf\u53ef\u4ee5\u5c06\u7b26\u5408\u6761\u4ef6\u7684\u4e32\u884c\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u6210\u76ee\u6807\u8ba1\u7b97\u8bbe\u5907\u4e0a\u7684\u5e76\u884c\u4ee3\u7801\uff0c\u9057 \u4f20\u7f16\u7a0b\u5bf9\u4e32\u884c\u4ee3\u7801\u7684\u9650\u5236\u5219\u5c0f\u7684\u591a\uff0c\u4f46\u662f\u5176\u4ea7\u751f\u7684\u4ee3\u7801\u7684\u53ef\u89e3\u91ca\u6027\u4e5f\u5dee\u8bb8\u591a\u3002\u5982\u4f55\u5c06\u81ea\u52a8 \u5e76\u884c\u5316\u6280\u672f\u96c6\u6210\u5230\u7f16\u7a0b\u6a21\u578b\u4e2d\uff0c\u63d0\u9ad8\u7a0b\u5e8f\u7684\u5f00\u53d1\u6548\u7387\u8fd8\u9700\u8981\u66f4\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002"
      },
      "sections_summary": {
        "Abstract": "[Skipped - Large PDF]"
      },
      "tables": [
        {
          "page": 2,
          "table_index": 0,
          "content": [
            [
              "\u6392\n\u540d",
              "\u7cfb\u7edf/\u578b\u53f7",
              "\u8ba1\u7b97\u8282\u70b9\u7ed3\u6784",
              "\u5b89\u88c5\u5730\u70b9",
              "Linpack\n\u503c/PFLOPS",
              "\u5cf0\n\u503c/PFLO\nPS"
            ],
            [
              "1",
              "Summit",
              "IBM POWER9 22C 3.1GHz, NVIDIA\nVolta GV100",
              "ORNL",
              "148.6",
              "187.66"
            ],
            [
              "2",
              "Sierra",
              "IBM POWER9 22C 3.1GHz, NVIDIA\nVolta GV100",
              "LLNL",
              "94.6",
              "125"
            ],
            [
              "3",
              "\u795e\u5a01\u592a\u6e56\u4e4b\n\u5149",
              "Sunway SW26010 260C 1.45GHz",
              "\u56fd\u5bb6\u8d85\u7b97\u65e0\u9521\n\u4e2d\u5fc3",
              "93.015",
              "125.436"
            ],
            [
              "4",
              "Tianhe-2A",
              "Intel Xeon E5-2692v2 12C 2.2GHz,\nTH Express-2, Matrix-2000",
              "\u56fd\u5bb6\u8d85\u7b97\u5e7f\u5dde\n\u4e2d\u5fc3",
              "61.445",
              "100.067\n9"
            ],
            [
              "5",
              "Frontera",
              "Intel Xeon Platinum 8280 28C\n2.7GHz",
              "\u5fb7\u514b\u8428\u65af\u5dde\u9ad8\n\u7ea7\u8ba1\u7b97\u4e2d\u5fc3",
              "23.5",
              "38.7"
            ],
            [
              "6",
              "Piz Daint",
              "Intel Xeon E5-2690v3 12C 2.6GHz,\nNVIDIA Tesla P100",
              "\u745e\u58eb\u5362\u52a0\u8bfa\u56fd\n\u5bb6\u8d85\u7b97\u4e2d\u5fc3",
              "21.2",
              "27.154"
            ],
            [
              "7",
              "Trinity",
              "Intel Xeon E5-2698v3 16C 2.3GHz,\nIntel Xeon Phi 7250 68C 1.4GHz",
              "LANL",
              "20.2",
              "41.5"
            ],
            [
              "8",
              "ABCI",
              "Intel Xeon Gold 6148 20C 2.4GHz,\nNVIDIA Tesla V100 SXM2",
              "\u4e1c\u4eac\u5927\u5b66",
              "19.88",
              "32.56"
            ],
            [
              "9",
              "SuperMUC-NG",
              "Intel Xeon Platinum 8174 24C\n3.1GHz",
              "\u83b1\u5e03\u5c3c\u5179\u8ba1\u7b97\n\u4e2d\u5fc3",
              "19.5",
              "26.7"
            ],
            [
              "10",
              "Lassen",
              "IBM POWER9 22C 3.1GHz, NVIDIA\nTesla V100",
              "LLNL",
              "18.2",
              "23.05"
            ]
          ]
        },
        {
          "page": 4,
          "table_index": 0,
          "content": [
            [
              "\u5bf9\u4ee5\u4e0a\u4e09\u7c7b\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u8fdb\u884c\u68b3\u7406\u603b\u7ed3\u3002\n\u88682. \u5f53\u524d\u4e3b\u6d41\u5e76\u884c\u7f16\u7a0b\u6a21\u578b",
              null,
              null,
              null,
              null
            ],
            [
              "\u7c7b\u578b",
              "\u7f16\u7a0b\u6a21\u578b",
              "\u540e\u7aef\u5e76\u884c\u65b9\u5f0f\u53ca\u652f\u6301\u7684\u8ba1\u7b97\u8bbe\n\u5907",
              "\u652f\u6301\u6027\u80fd\u53ef\n\u79fb\u690d\uff08\u5426\u3001\n\u662f\uff09",
              "\u7d22\u5f15"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "OpenCL",
              "CPU\u3001Xeon Phi\u3001GPU\u7b49",
              "\u662f",
              "[6]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "SkelCL",
              "OpenCL",
              "\u662f",
              "[7]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "Boost.compute",
              "OpenCL",
              "\u662f",
              "[8]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "Bolt",
              "OpenCL",
              "\u662f",
              "[9]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "SYCL",
              "CPU\u3001Xeon Phi\u3001GPU\u7b49",
              "\u662f",
              "[10]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "DPC++",
              "CPU\u3001Xeon Phi\u3001GPU\u3001FPGA\u7b49",
              "\u662f",
              "[11]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "C++ AMP",
              "CPU\u3001GPU\u7b49",
              "\u662f",
              "[11]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "Kokkos",
              "OpenMP\u3001CUDA\u3001OpenCL",
              "\u662f",
              "[12]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "RAJA",
              "OpenMP\u3001CUDA\u3001OpenCL",
              "\u662f",
              "[13]"
            ],
            [
              "\u7f16\u7a0b\u63a5\u53e3",
              "OP2",
              "MPI\u3001OpenMP\u3001CUDA\u3001OpenCL",
              "\u662f",
              "[14,\n15]"
            ],
            [
              "\u6307\u5bfc\u8bed\u53e5",
              "OpenACC",
              "CPU\u3001Xeon Phi\u3001GPU\u7b49",
              "\u662f",
              "[16]"
            ],
            [
              "\u6307\u5bfc\u8bed\u53e5",
              "OpenMP 4.X",
              "CPU\u3001Xeon Phi\u3001GPU\u7b49",
              "\u662f",
              "[17]"
            ],
            [
              "\u6307\u5bfc\u8bed\u53e5",
              "OpenHMPP",
              "CPU\u3001Xeon Phi\u3001GPU\u7b49",
              "\u662f",
              "[18]"
            ],
            [
              "\u6307\u5bfc\u8bed\u53e5",
              "Mint",
              "GPU",
              "\u5426",
              "[19]"
            ],
            [
              "\u8bed\u8a00",
              "Chapel",
              "CPU\u3001GPU",
              "\u662f",
              "[20,\n21]"
            ],
            [
              "\u8bed\u8a00",
              "X10",
              "CPU\u3001GPU",
              "\u662f",
              "[22]"
            ],
            [
              "\u8bed\u8a00",
              "Halide",
              "CPU\u3001GPU\u7b49",
              "\u662f",
              "[44]"
            ],
            [
              "\u8bed\u8a00",
              "TVM",
              "CPU\u3001GPU\u7b49",
              "\u662f",
              "[23]"
            ],
            [
              "\u8bed\u8a00",
              "XLA",
              "CPU\u3001GPU\u7b49",
              "\u662f",
              "[24]"
            ],
            [
              "2.1 \u57fa\u4e8e\u7f16\u7a0b\u63a5\u53e3\u7684\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\n\u4e3a\u65b9\u4fbf\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\u57fa\u4e8e\u73b0\u6709\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u7a0b\u5e8f\u7684\u6027\u80fd\u53ef\u79fb\u690d\uff0c\u5f53\u524d\u6709\u8f83\u591a\u5de5\u4f5c\u5c06\u5bf9\n\u8ba1\u7b97\u8bbe\u5907\u8fdb\u884c\u7684\u64cd\u4f5c\u4ee5\u7f16\u7a0b\u63a5\u53e3\u7684\u5f62\u5f0f\u63d0\u4f9b\u7ed9\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\uff0c\u4ece\u800c\u65b9\u4fbf\u5f00\u53d1\u4eba\u5458\u57fa\u4e8e\u73b0\u6709\n\u8bed\u8a00\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u8ba1\u7b97\u8bbe\u5907\u3002",
              null,
              null,
              null,
              null
            ]
          ]
        }
      ],
      "images": [
        "processed/images/2308.13802v1_page3_img0.png",
        "processed/images/2308.13802v1_page9_img0.png",
        "processed/images/2308.13802v1_page11_img0.png",
        "processed/images/2308.13802v1_page13_img0.png",
        "processed/images/2308.13802v1_page14_img0.png",
        "processed/images/2308.13802v1_page17_img0.png",
        "processed/images/2308.13802v1_page18_img0.png",
        "processed/images/2308.13802v1_page18_img1.png",
        "processed/images/2308.13802v1_page19_img0.png"
      ],
      "status": "completed",
      "json_file": "processed/compiled/2308.13802v1_compiled.json"
    },
    {
      "metadata": {
        "title": "Comparative benchmarking of cloud computing vendors with High   Performance Linpack",
        "authors": [
          "Mohammad Mohammadi",
          "Timur Bazhirov"
        ],
        "abstract": "We present a comparative analysis of the maximum performance achieved by the Linpack benchmark on compute intensive hardware publicly available from multiple cloud providers. We study both performance within a single compute node, and speedup for distributed memory calculations with up to 32 nodes or at least 512 computing cores. We distinguish between hyper-threaded and non-hyper-threaded scenarios and estimate the performance per single computing core. We also compare results with a traditional supercomputing system for reference. Our findings provide a way to rank the cloud providers and demonstrate the viability of the cloud for high performance computing applications.",
        "published": "",
        "arxiv_id": "1702.02968v1",
        "categories": [
          "cs.PF",
          "cs.DC"
        ],
        "pdf_url": "http://arxiv.org/pdf/1702.02968v1",
        "pdf_file": "data/pdfs/1702.02968v1.pdf",
        "pdf_filename": "1702.02968v1.pdf"
      },
      "processing_info": {
        "processed_at": "2025-11-11T10:30:02.551912",
        "is_large_pdf": false,
        "sections_found": 15,
        "tables_found": 2,
        "images_found": 0
      },
      "sections_text": {
        "Abstract": "arXiv:1702.02968v1 [cs.PF] Feb Comparative benchmarking of cloud computing vendors with High Performance Linpack",
        "Mohammad Mohammadi, Timur Bazhirov": "Exabyte Inc., San Francisco, California 94103, USA Abstract \u2014We present a comparative analysis of the maximum performance achieved by the Linpack benchmark on compute intensive hardware publicly available from multiple cloud providers. We study both performance within a single compute node, and speedup for distributed memory calculations with up to nodes or at least computing cores. We distinguish between hyper-threaded and non-hyper-threaded scenarios and estimate the performance per single computing core. We also compare results with a traditional supercomputing system for reference. Our \ufb01ndings provide a way to rank the cloud providers and demonstrate the viability of the cloud for high performance computing applications. Index Terms \u2014Cloud Computing, High Performance Computing, Linpack, Benchmarking.",
        "I Ntroduction": "During the last decade cloud computing established itself as a viable alternative to on-premises hardware for missioncritical applications in multiple areas , , . For high performance computing (HPC) workloads that traditionally required large and cost-intensive hardware procurement, however, the feasibility and advantages of cloud computing are still debated. In particular, it is often questioned whether software applications that require distributed memory can be ef\ufb01ciently run on \u201dcommodity\u201d compute infrastructure publicly available from cloud computing vendors . Several studies reported on the poor applicability of cloud-based environments for scienti\ufb01c computing. Multiple research groups ran both standard benchmark suites such as Linpack and NAS , , , and network performance tests . The cost of solving a system of linear equations was found to increase exponentially with the problem size, illustrating that cloud was not mature enough for such workloads in . A study of the impact of virtualization on network performance reported signi\ufb01cant throughput instability and abnormal delay variations . An empirical",
        "Experiments": "publicly available cloud computing systems are capable of delivering comparable, if not better, performance than the top-tier traditional high performance computing systems. This fact con\ufb01rms that cloud computing is already a viable and cost-effective alternative to traditional costintensive supercomputing procurement. We believe that with further advancements in virtualization, such as lowoverhead container technology, and future improvements in cloud datacenter hardware we may experience a large-scale migration from on-premises to cloud-based usage for high performance applications, similar to what happened with less compute-intensive workloads.",
        "M Ethodology": "Benchmarking presented in this article is done through High Performance Linpack (HPL). The program solves a random system of linear equations, represented by a dense matrix, in double precision ( bits) arithmetic on distributed-memory computers. It does so through a two-dimensional blockcyclic data distribution, and right-looking variant of the LU factorization with row partial pivoting. It is a portable and freely available software package. HPL provides testing and timing means to quantify the accuracy of the obtained solution as well as the time-to-completion. The best performance achievable depends on a variety of factors, and the algorithm is scalable such that its parallel ef\ufb01ciency is kept constant with respect to per processor memory usage. information: , , , . process grid dimensions. These parameters are changed choosing the exact input parameters could be demonstrated Q slightly larger than P.",
        "R Esults": "We present the cloud server instance types and hardware speci\ufb01cation for all studied cases inside Table 1. We choose Table 1: Hardware speci\ufb01cation for the compute nodes used during benchmarking. Core count for physical computing cores and processor frequency, in GHz, are given together with Memory (RAM) size, in gigabytes, and network bandwidth in gigabit-per-second , , , . Provider Nodes Cores Freq. RAM Net AWS-* c4.8xlarage 2. Azure-AZ Standard F16s 2. Azure-IB-A 2. Azure-IB-H H16 3. SoftLayer Virtual 2. Rackspace Compute12. NERSC Edison 2. Table 2: [AWS] Results for Amazon Web Services c4.8xlarge instances with hyperthreading enabled (default scenario). Core count is given for virtual (hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). It can be seen that the ratio of absolute speedup to the number of nodes falls rapidly as the number of nodes is increased. Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 1. 1. 0. 3. 1. 1. 6. 2. 2. 13. 5. 5. 26. 9. 8. 52. 16. the highest performing servers available in an on-demand fashion. Most of the compute servers have physical cores and all have at least 2GB per or random access memory per core. The network options differ quite a bit, from to gigabit per second in bandwidth. We also provide metrics for the traditional supercomputing system used as reference . 3.",
        "Amazon Web Services": "For Amazon Web Services (AWS) we study different scenarios: the default hyper-threaded, non-hyper-threaded and non-hyper-threaded mode with placement group option enabled. The c4.8xlarge instance types are used. 3.1. Hyper-threaded regime Table shows the results for AWS instances with hyperthreading enabled (default regime). It can be seen that the ratio of absolute speedup to the number of nodes rapidly decreases as the node count increases. 3.1. Non-hyper-threaded regime Table shows the results for AWS with Hyper-Threading disabled. Thus only out of cores were used to run the benchmark, and each core was able to boost into the turbo-frequency . It can be seen that the ratio of absolute speedup to the number of nodes still rapidly degrades with increased node count. Table [AWS-NHT] Results for Amazon Web Services c4.8xlarge instances with hyper-threading disabled. Core count is given for physical (non-hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 6. 5. 5. 13. 8. 10. 26. 16.",
        "Microsoft Azure": "3.2. F-series Table shows the HPL benchmark results running on Azure Standard F16 instances. Although the overall performance degradation with increased node count is evident, it appears to be less severe than for AWS. The bare performance is worse however. 3.2. A-series Table shows the HPL benchmark results running on",
        "Azure Standard": "A9 instances using In\ufb01niband interconnect network. The low-latency network interconnect de\ufb01nitely affects the scaling, increasing the speed-up ratio from 0. to 0. for compute nodes. The bare performance \ufb01gures, however are still better for AWS due to the higher processor speed. Table 5: [AZ-F] Results for Azure F-series instances. Core count is given for physical (non-hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 0. 1. 0. 1. 1. 1. 2. 3. 3. 4. 6. 5. 9. 11. 10. 19. 22. Table 6: [AZ-A] Results for Azure A-series instances with In\ufb01niband interconnect network. Core count is given for physical (non-hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 0. 1. 0. 1. 1. 1. 2. 3. 2. 5. 7. 4. 10. 14. 8. 20. 28. 3.2. H-series Table shows the HPL benchmark results running on Azure Standard H16r instances using In\ufb01niband interconnect network. The low-latency network interconnect enables the best scaling pattern, with sustained ratio above 0. in the 1node count (1computing cores) range. The bare performance \ufb01gures are best of all cases studied, even when compared with the traditional supercomputing system of reference. 3. Rackspace Table shows the HPL benchmark results running on",
        "D Iscussion": "In Fig. we present a comparison of the speedup ratios for the scenarios described the previous part. As it can be seen, Microsoft Azure outperforms other cloud providers because of the low latency interconnect network that facilitates ef\ufb01cient scaling. SoftLayer has the least favorable speedup ratio at scale, likely because of the interconnect network again. AWS and Rackspace show a signi\ufb01cant degree of parallel performance degradation, such that at nodes the measured performance is about one-half of the peak value.",
        "Figure 1:": "Speedup ratios (the ratios of maximum speedup Rmax to peak speedup Rpeak) against the number of nodes for all benchmarked cases. Speedup ratio for 1,2,4,8, and nodes are investigated and given by points. Lines are drawn to guide the eye. The legend is as follows: AWS Amazon Web Services in the default hyper-threaded regime; AWS-NHT same, with hyperthreading disabled; AWS-NHTPG same, with placement group option enabled; AZ Microsoft Azure standard F16 instances; AZ-IB-A same provider, A9 instances; AZ-IB-H same provider, H16 instances; RS Rackspace compute1instances; SL IBM/Softlayer virtual servers; NERSC Edison computing facility of the National Energy Research Scienti\ufb01c Computing Center. Fig. shows a comparative plot of the performance per core in giga-FLOPS for the previously described scenarios. Microsoft Azure H-instances are the highest performing option in this view as well (AZ-IB-H). One interesting fact is that although Microsoft Azure A-instances (AZ-IB-A) show better overall scaling in Fig. 1, AWS c4.8xlarge instances deliver better performance per core for up to nodes. This is likely because of faster processors speed. NERSC Edison supercomputer delivers a rather low performance per core metric, likely due to the type of processors used.",
        "Figure 2:": "Performance per core in giga-FLOPS against the number of nodes for all benchmarked cases. Performance per core is obtained by dividing the maximum performance by the total number of computing cores. The legend is the same as in Fig. 1. Lines are given to guide the eye.",
        "C Onclusion": "We benchmarked the performance of the best available computing hardware from public cloud providers with high performance Linpack. We optimized the benchmark for each computing environment and evaluated the relative performance for distributed memory calculations. We found Microsoft Azure to deliver the best results, and demonstrated that the performance per single computing core on public cloud to be comparable to modern traditional supercomputing systems. Based on our \ufb01ndings we suggest that the concept of high performance computing in the cloud is ready for a widespread adoption and can provide a viable and cost-ef\ufb01cient alternative to capital-intensive onpremises hardware deployments.",
        "A Cknowledgement": "Authors would like to thank Michael G. Haverty for reading the manuscript, and acknowledge support from the National Energy Research Scienti\ufb01c Computing Center in a form of a startup allocation.",
        "R Eferences": "Dongarra, Luszczek, and Petitet, \u201cThe LINPACK benchmark: past, present and future,\u201d Concurrency and Computation: Practice and Experience , vol. 15, no. 9, pp. 803\u2013820, 2003. [Online]. Available: http://dx.doi.org/10.1002/cpe. P. Mell and T. Grance, \u201cThe nist de\ufb01nition of cloud computing,\u201d National Institute of Standards and Technology , vol. 53, no. 6, p. 50, 2009. Armbrust, Fox, Grif\ufb01th, Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, \u201cA view of cloud computing,\u201d Journal Emerging Trends Computing and Information Sciences , vol. 53, no. 4, pp. 50\u201358, 2010. [Online]. Available: J. W. Rittinghouse and J. F. Ransome, Cloud computing: implementation, management, and security . CRC press, 2016. K. Jackson, L. Ramakrishnan, K. Muriki, S. Canon, S. Cholia, Shalf, Wasserman, and Wright, \u201cPerformance analysis high performance computing applications the amazon web services cloud,\u201d Proceedings the IEEE Second International Conference on Cloud Computing Technology and Science (CloudCom 2010) , pp. 159\u2013168, 2010. [Online]. Available: R. Masud, \u201cHigh performance computing with clouds,\u201d Technical Report, University of Oregon , 2010. S. Ostermann, A. Iosup, N. Yigitbasi, R. Prodan, T. Fahringer, and D. Epema, \u201cAn early performance analysis of cloud computing services for scienti\ufb01c computing,\u201d Delft University of Technology, Tech. Rep , 2008. E. Walker, \u201cBenchmarking amazon ec2 for high-performance scienti\ufb01c computing,\u201d USENIX Login , vol. 33, no. 5, pp. 18\u201323, 2008. G. Wang and T. E. Ng, \u201cThe impact of virtualization on network performance of amazon ec2 data center,\u201d Proceedings of IEEE INFOCOM , 2010. J. Napper and P. Bientinesi, \u201cCan cloud computing reach the top500?\u201d Proceedings of the combined workshops on UnConventional high performance computing workshop plus memory access workshop. ACM , vol. 2008, pp. 17\u201320, 2009. A. Iosup, S. Ostermann, N. Yigitbasi, R. Prodan, T. Fahringer, and D. Epema, \u201cPerformance analysis of cloud computing services for many-tasks scienti\ufb01c computing,\u201d IEEE Transactions on Parallel and Distributed Systems , vol. 22, no. 6, pp. 931\u2013945, 2011. K. Yelick, \u201cThe magellan report cloud computing for science,\u201d U.S. Department Energy Of\ufb01ce Science , 2011. [Online]. Available: S. Hazelhurst, \u201cScienti\ufb01c computing using virtual highperformance computing: a case study using the amazon elastic computing cloud,\u201d Proceedings of the annual research conference of the South African Institute of Computer Scientists and Information Technologists on IT research in developing countries: riding the wave of technology. ACM , pp. 94\u2013103, 2008. E. Deelman, G. Singh, M. Livny, B. Berriman, and J. Good, \u201cThe cost of doing science on the cloud: the montage example,\u201d Proceedings of the ACM/IEEE conference on Supercomputing. IEEE Press , pp. 1\u201312, 2008. C. Evangelinos and C. Hill, \u201cCloud computing for parallel scienti\ufb01c hpc applications: Feasibility of running coupled atmosphereocean climate models on amazons ec2,\u201d ratio , vol. 2, no. 2.40, pp. 2\u201334, 2008. K. Keahey, T. Freeman, J. Lauret, and D. Olson, \u201cVirtual workspaces for scienti\ufb01c applications,\u201d Journal of Physics: Conference Series , vol. 78, 2007. K. Keahey, R. Figueiredo, J. Fortes, T. Freeman, and M. Tsugawa, \u201cScience clouds: Early experiences in cloud computing for scienti\ufb01c applications,\u201d Cloud Computing and Applications , vol. 2008, 2008. K. Keahey, \u201cCloud computing for science,\u201d Proceedings of the 21st International Conference on Scienti\ufb01c and Statistical Database Management. Springer-Verlag , vol. 2008, p. 478, 2009. J. Li, D. Agarwal, M. Humphrey, C. van Ingen, K. Jackson, and Y. Ryu, \u201cescience in the cloud: A modis satellite data reprojection and reduction pipeline in the windows azure platform,\u201d Proceedings of the 24th IEEE International Parallel and Distributed Processing Symposium , 2010. L. Ramakrishnan, K. R. Jackson, S. Canon, S. Cholia, and J. Shalf, \u201cDe\ufb01ning future platform requirements for e-science clouds,\u201d Proceedings of the ACM Symposium on Cloud Computing , 2010. J. Rehr, F. Vila, J. Gardner, L. Svec, and M. Prange, \u201cScienti\ufb01c computing in the cloud,\u201d Computing in Science and Engineering , vol. 99, 2010. S. P. Ahuja and S. Mani, \u201cThe state of high performance computing in the cloud,\u201d Journal of Emerging Trends in Computing and Information Sciences , vol. 3, no. 2, 2012. T. Bazhirov, M. Mohammadi, K. Ding, and S. Barabash, \u201cLargescale high-throughput computer-aided discovery of advanced materials using cloud computing,\u201d Proceedings of the American Physical Society March Meeting , 2017. [Online]. Available: \u201cLinpack, top500.org webpage.\u201d [Online]. Available: \u201cEdison supercomputer, top500.org ranking.\u201d [Online]. Available: [Online]. Available: http://www.netlib.org/benchmark/hpl J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart, \u201cLinpack users guide,\u201d 1979. \u201cAmazon ec2 instance types.\u201d [Online]. Available: \u201cSizes for linux virtual machines in azure.\u201d [Online]. Available: \u201cRackspace virtual cloud server \ufb02avors.\u201d [Online]. Available: \u201cSoftlayer virtual servers.\u201d [Online]. Available: \u201cTurbo boost technology.\u201d [Online]. Available: \u201cAmazon elastic compute cloud: Placement group.\u201d [Online]. Available: G. F. P\ufb01ster, \u201cAn introduction to the in\ufb01niband architecture,\u201d High Performance Mass Storage and Parallel I/O , vol. 42, pp. 617\u2013632, 2001."
      },
      "sections_summary": {
        "Abstract": "arXiv:1702.02968v1 [cs.PF] Feb Comparative benchmarking of cloud computing vendors with High Performance Linpack",
        "Mohammad Mohammadi, Timur Bazhirov": "This study compares the maximum performance achieved by the Linpack benchmark on compute-intensive hardware from multiple cloud providers. It analyzes single-node performance, distributed memory calculations with up to 16 nodes or cores, and estimates performance per core. The results rank the cloud providers and demonstrate their viability for high-performance computing applications.",
        "I Ntroduction": "Cloud computing's viability for high-performance computing (HPC) workloads is debated due to concerns over efficiency, scalability, and reliability. Several studies have shown that cloud-based environments struggle with large-scale computations, exhibiting poor applicability and significant performance issues, such as exponential cost increases and unstable network throughput.",
        "Experiments": "Publicly available cloud computing systems can deliver comparable or better performance than top-tier traditional high-performance computing systems. This confirms that cloud computing is a viable and cost-effective alternative to traditional supercomputing procurement, and a large-scale migration to cloud-based usage may occur for high-performance applications.",
        "M Ethodology": "Benchmarking is done using High Performance Linpack (HPL), which solves random linear equations on distributed-memory computers through LU factorization with row partial pivoting, in a two-dimensional blockcyclic data distribution. HPL provides accuracy and timing measures to evaluate performance. The best performance depends on various factors, including process grid dimensions, and is scalable with constant parallel efficiency per processor memory usage.",
        "R Esults": "Table 1 lists hardware specifications for compute nodes used during benchmarking, including:\n\n- Core count and processor frequency\n- Memory (RAM) size in gigabytes\n- Network bandwidth in gigabits per second\n\nTable 2 provides results for Amazon Web Services c4.8xlarge instances with hyperthreading enabled, showing:\n\n- Number of computing cores and nodes\n- Maximum achieved performance indicators (Rmax and Rpeak)\n- Peak performance (TFLOPS)\n- Absolute achieved speedup",
        "Amazon Web Services": "The study examines AWS instance types with and without hyper-threading enabled, comparing default and non-hyper-threaded modes. \n\nIn the hyper-threaded regime, the absolute speedup ratio decreases rapidly as the number of nodes increases.\n\nIn the non-hyper-threaded regime, the core count is given for physical cores only, leading to a similar rapid degradation of speedup with increased node count.\n\nPerformance indicators include maximum achieved (Rmax) and peak (Rpeak) performance, with results shown in tables.",
        "Microsoft Azure": "The F-series Table shows decreased overall performance with more nodes in Azure compared to AWS, while the A-series Table also displays lower performance compared to AWS.",
        "Azure Standard": "Azure F-series instances: \n- Increased speed-up ratio from 0. to 0.\n- Higher processor speeds still outperform A-series.\n\nAzure A-series instances:\n- Low-latency network interconnect enables best scaling pattern.\n- Sustained ratio above 1.5 in 1-node count range.\n\nH-series:\n- Best performance figures, even compared to traditional supercomputing systems.\n- Sustained ratio above 0.",
        "D Iscussion": "Microsoft Azure outperforms other cloud providers due to its low latency interconnect network, which facilitates efficient scaling. SoftLayer has the least favorable speedup ratio at scale, likely due to its interconnect network as well. AWS and Rackspace experience significant parallel performance degradation, with performance at nodes about half of the peak value.",
        "Figure 1:": "The text shows the results of benchmarking different cloud computing environments (AWS, Microsoft Azure, Rackspace, IBM/Softlayer, and NERSC) with various instance types. The plots show that:\n\n- Microsoft Azure H-instances perform the best in terms of overall performance per core.\n- AWS c4.8xlarge instances deliver better performance per core for up to 32 nodes due to faster processor speed.\n- NERSC Edison supercomputer has lower performance per core, likely due to its processors.\n- AZ Microsoft Azure standard and A9 instances show better overall scaling compared to other options.",
        "Figure 2:": "There is no text provided for me to summarize. You mentioned a figure or legend, but there's no actual text to work with. Please provide the relevant information, and I'll be happy to help!",
        "C Onclusion": "Benchmarking public cloud providers' best available computing hardware, optimized Linpack was used to evaluate relative performance. Microsoft Azure delivered the best results, with performance comparable to modern traditional supercomputing systems. This suggests widespread adoption of high-performance computing in the cloud as a viable and cost-efficient alternative to on-premises hardware deployments.",
        "A Cknowledgement": "The authors thank Michael G. Haverty for reviewing their manuscript and acknowledge support from the National Energy Research Scientific Computing Center through a startup allocation.",
        "R Eferences": "Here's a concise overall summary:\n\nResearch on using cloud computing for high-performance computing (HPC) has been ongoing since 2008. Key papers have explored various aspects, including benchmarking, performance analysis, and feasibility studies of cloud-based HPC applications. Studies have found that cloud computing offers advantages in scientific data management, materials science research, and high-performance computing, particularly with Linpack and the top500.org. Cloud service providers such as Amazon EC2, Azure, and Rackspace have also been evaluated for their virtual server configurations.\n\nThe research has led to the development of guidelines and benchmarks, including a Linpack users guide, which provide standards for scientific computing in the cloud. Overall, these findings suggest that cloud computing can be a viable option for HPC applications, offering benefits such as scalability, flexibility, and cost-effectiveness."
      },
      "tables": [
        {
          "page": 4,
          "table_index": 0,
          "content": [
            [
              "",
              "",
              null,
              "",
              "",
              "",
              "",
              "",
              "",
              null
            ],
            [
              "",
              "",
              null,
              "",
              "",
              "",
              "",
              "",
              "",
              null
            ],
            [
              "",
              "",
              null,
              "",
              "",
              "",
              "",
              "",
              "",
              null
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              null
            ],
            [
              null,
              null,
              null,
              "",
              "",
              "",
              "",
              "",
              null,
              null
            ],
            [
              "",
              "",
              null,
              "",
              "",
              "",
              "",
              "",
              "",
              null
            ],
            [
              "",
              "",
              null,
              "",
              "",
              "",
              "",
              "",
              "",
              null
            ]
          ]
        },
        {
          "page": 5,
          "table_index": 0,
          "content": [
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ]
        }
      ],
      "images": [],
      "status": "completed",
      "json_file": "processed/compiled/1702.02968v1_compiled.json"
    },
    {
      "metadata": {
        "title": "HPS: A C++11 High Performance Serialization Library",
        "authors": [
          "Junhao Li"
        ],
        "abstract": "Data serialization is a common and crucial component in high performance computing. In this paper, I present a C++11 based serialization library for performance critical systems. It provides an interface similar to Boost but up to 150% faster and beats several popular serialization libraries.",
        "published": "",
        "arxiv_id": "1811.04556v2",
        "categories": [
          "cs.PF"
        ],
        "pdf_url": "http://arxiv.org/pdf/1811.04556v2",
        "pdf_file": "data/pdfs/1811.04556v2.pdf",
        "pdf_filename": "1811.04556v2.pdf"
      },
      "processing_info": {
        "processed_at": "2025-11-11T10:30:20.264257",
        "is_large_pdf": false,
        "sections_found": 4,
        "tables_found": 10,
        "images_found": 2
      },
      "sections_text": {
        "Abstract": "HPS: A C++ High Performance",
        "Serialization Library": "Junhao Li Department of Physics, Cornell University Data serialization is a common and crucial component in high performance computing. In this paper, I present a C++ based serialization library for performance critical systems. It provides an interface similar to Boost but is upto 1. times faster and beats several popular serialization libraries. Overview Data serialization is a common and crucial component in high performance computing. It is a prerequisite for transferring data over the networks in distributed systems and for storing them on persistence storage devices. The bandwidths of the networks and the storage devices are the bottleneck of many distributed applications, such as MapReduce. Hence, a serialization framework that can produce compact serialized data efficiently can have game-changing impacts on these applications. HPS is designed to and has achieved state of the art performance and beats well-known serialization libraries. Compared to Boost Serialization, HPS is upto 1. times faster and the serialized data from HPS are upto 40% smaller than those from Boost. Compared to Protocol Buffers, HPS is upto 4. times faster for certain types and the serialized data are upto 50% smaller. The interface of HPS is similar to Boost, which means it works on standard containers and user defined structures directly without requiring users to deal with additional tightly coupled serialization classes. This increases code reusability and reduces cognitive efforts for the developers.",
        "Encoding Scheme": "The encoding scheme of HPS is the root of its high performance and compact serialization. I heavily borrow the encoding scheme of Protocol Buffers while removing all the unnecessary fields in a performance critical setting. The resulting scheme contains only data and minimal structural information for recovering the original data unambiguously. The integral types are encoded with basevarints. Signed integrals types use ZigZag encoding before the baseencoding. Hence, if we use HPS to serialize a number, such as -33, the serialized message takes only one byte. The floating point numbers are directly copied from the source memory location to the target. For iterable STL containers, HPS encodes their sizes first, followed by the elements. For custom classes, HPS looks for the serialize and parse methods in these classes. In the following I give two encoding examples: Case #1: A vector of integers #include <cassert> #include <iostream> #include \"../src/hps.h\" int main() { std::vector<unsigned> data({22, 333}); std::string serialized = hps::to_string(data); auto parsed = hps::from_string<std::vector<int>>(serialized); assert(parsed == data); std::cout << \"size (B): \" << serialized.size() << std::endl; // size (B): // Serialized as (in hexadecimal): // (number of elements) // (first number) // cd (first byte of the second number in baseencoding) // (second byte of the second number) return 0; Case #2: Custom Class A Quantum System #include <cassert> #include <iostream> #include \"../src/hps.h\" class QuantumState { public: unsigned n_elecs; std::unordered_set<unsigned> orbs_from; std::unordered_set<unsigned> orbs_to; template <class B> void serialize(B& buf) const { buf << n_elecs << orbs_from << orbs_to; template <class B> void parse(B& buf) { buf >> n_elecs >> orbs_from >> orbs_to; int main() { QuantumState qs; qs.n_elecs = 33; qs.orbs_from.insert({11, 22}); qs.orbs_to.insert({44, 66}); std::string serialized = hps::to_string(qs); std::cout << \"size (B): \" << serialized.size() << std::endl; // size (B): // HPS looks for the serialize<B> and parse<B> in QuantumState. // The first byte is n_elecs, then bytes for each unordered_set. return 0; Compared to Protocol Buffers, HPS does not store data types and field numbers. This reduces the size of the serialized message and also increases the performance. In the extreme case, if the leaf messages contain only non-repeating fields, the serialized message from Protocol Buffers will be times larger than HPS. Implementation There are two key classes in the implementation of HPS, the Serializer and the Buffer. Buffer provides the read/write char(s) interfaces to the Serializer, and provides the \"<<\" and \">>\" operators to the wrapper functions. Serializer<DataType, BufferType> provides the logical serialize and parse methods of the given DataType. It uses the read/write char(s) methods of BufferType to push data to or pull data from the buffer. To make the codebase highly maintainable without sacrificing performance, we heavily use static polymorphism and SFINAE. For example, here is the Serializer specialization for the floating point numbers: template <class T, class B> class Serializer<T, B, typename std::enable_if<std::is_floating_point<T>::value, void>::type> { Public: static void serialize(const T& num, B& ob) { const char* num_ptr = reinterpret_cast<const char*>(&num); ob.write(num_ptr, sizeof(num)); static void parse(T& num, B& ib) { char* num_ptr = reinterpret_cast<char*>(&num); ib.read(num_ptr, sizeof(num)); The wrapper functions call the Serializer to provide an easy to use interface to the users, for example: std::ofstream out_file(\"data.log\", std::ofstream::binary); hps::to_stream(data, out_file); std::ifstream in_file(\"data.log\", std::ifstream::binary); auto parsed = hps::from_stream<std::vector<int>>(in_file); By default, Serializer<DataType, BufferType> will call the serialize<B> and parse<B> methods of the corresponding class. This provides a simple and loosely coupled way of extending HPS for custom classes (See the quantum system case above for example). The complete implementation is hosted at \u200b https://github.com/jl2922/hps\u200b . Benchmark The performance of HPS compared to other well-known C++ serializers for some most common data structures in high performance computing are as follows: (less is better) The sparse matrix is stored as a list of rows, each of which contains a list of 64-bit integers for the column indices and a list of doubles for the values. The hash map is a map from strings to doubles. Both HPS and Boost can serialize std::unordered_map directly, ProtoBuf uses its own Map type and CapnProto does not support hash map or similar types. We can see that HPS is consistently and sometimes significantly faster than Boost and Protocol Buffers. It also beats Capnproto in all the cases except the sparse matrix. However, for the sparse matrix, although Capnproto is faster, the serialized message is also significantly larger, and for large distributed systems, minimizing the size of the data transferred is often more important than minimizing the local CPU time. In addition to the traditional benchmarks for computational cost, we also provide the cognitive effort in terms of source lines of code for these test cases: (less is better) SLOC double array sparse matrix hash map fixed cost protobuf capnproto boost hps Note: fixed cost includes the estimated number of lines of commands needed for an experienced user to install the library, set the environment variables, extra lines of code needed in the Makefile, and various includes, etc. Due to the header-only nature of HPS, it is the easiest one to set up, as we can see from the table above.",
        "Conclusion": "In the paper, I present a high performance serialization library based on C++11. It is easy to use and beats the state of the art performance. Data serialization is important and may be the dominating factor of performance in many applications. HPS enables developers to serialize their structured data to compact binary formats efficiently so that their applications will incur less network traffic and disk operations and thus cost less and run faster. Acknowledgements This work is supported by the U.S. National Science Foundation (NSF) grant ACIand the Air Force Office of Scientific Research (AFOSR) grant FA9550-18-1-0095. We also thank professor Cyrus Umrigar for the helpful suggestions for the paper. Reference Varda, Kenton. \"Protocol buffers: Google\u2019s data interchange format.\" Google Open Source Blog, Available at least as early as Jul (2008). Stuart, Stephen, and Rex Fernando. \"Encoding rules and MIME type for Protocol Buffers.\" (2012). Varda, Kenton. \"Cap\u2019n Proto, 2015.\" URL: https://capnproto.org (2015). Ramey, Robert. \"Boost serialization library.\" URL: Vandevoorde, David, and Nicolai M. Josuttis. C++ Templates. Addison-Wesley Longman Publishing Co., Inc., 2002."
      },
      "sections_summary": {
        "Abstract": "HPS: A C++ High Performance",
        "Serialization Library": "The paper presents a C++ based serialization library called HPS (High Performance Serialization) designed for performance-critical systems. It provides an interface similar to Boost, achieving state-of-the-art performance and beating well-known serialization libraries like Boost Serialization and Protocol Buffers.",
        "Encoding Scheme": "Here's a concise overall summary:\n\nHPS (High-Performance Serialization) is a lightweight serialization framework that optimizes data transmission by removing unnecessary fields for compact storage. It supports serializing integral types, floating-point numbers, iterable containers, and custom classes using a simple interface with functions like `to_stream` and `from_stream`. HPS outperforms popular alternatives such as Boost, Protocol Buffers, and CapnProto in terms of speed, particularly in cases where leaf messages contain only non-repeating fields.",
        "Conclusion": "A high-performance serialization library is presented based on C++11, offering easy-to-use functionality that surpasses state-of-the-art performance. The library enables efficient compact binary formats for structured data, reducing network traffic and disk operations."
      },
      "tables": [
        {
          "page": 1,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              "HPS: A C++11 High Performance\nSerialization Library\nJunhao Li\nDepartment of Physics, Cornell University\nData serialization is a common and crucial component in high performance\ncomputing. In this paper, I present a C++11 based serialization library for\nperformance critical systems. It provides an interface similar to Boost but is upto\n1.5 times faster and beats several popular serialization libraries.\nOverview\nData serialization is a common and crucial component in high performance\ncomputing. It is a prerequisite for transferring data over the networks in\ndistributed systems and for storing them on persistence storage devices. The\nbandwidths of the networks and the storage devices are the bottleneck of many\ndistributed applications, such as MapReduce. Hence, a serialization framework\nthat can produce compact serialized data efficiently can have game-changing\nimpacts on these applications.\nHPS is designed to and has achieved state of the art performance and beats\nwell-known serialization libraries. Compared to Boost Serialization, HPS is upto\n1.5 times faster and the serialized data from HPS are upto 40% smaller than\nthose from Boost. Compared to Protocol Buffers, HPS is upto 4.5 times faster\nfor certain types and the serialized data are upto 50% smaller.\nThe interface of HPS is similar to Boost, which means it works on standard\ncontainers and user defined structures directly without requiring users to deal\nwith additional tightly coupled serialization classes. This increases code\nreusability and reduces cognitive efforts for the developers."
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 2,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "Encoding Scheme\nThe encoding scheme of HPS is the root of its high performance and compact\nserialization. I heavily borrow the encoding scheme of Protocol Buffers while\nremoving all the unnecessary fields in a performance critical setting. The\nresulting scheme contains only data and minimal structural information for\nrecovering the original data unambiguously.\nThe integral types are encoded with base-128 varints. Signed integrals types\nuse ZigZag encoding before the base-128 encoding. Hence, if we use HPS to\nserialize a number, such as -33, the serialized message takes only one byte.\nThe floating point numbers are directly copied from the source memory location\nto the target. For iterable STL containers, HPS encodes their sizes first,\nfollowed by the elements. For custom classes, HPS looks for the serialize and\nparse methods in these classes.\nIn the following I give two encoding examples:\nCase #1: A vector of integers\n#include <cassert>\n#include <iostream>\n#include \"../src/hps.h\"\nint main() {\nstd::vector<unsigned> data({22, 333});\nstd::string serialized = hps::to_string(data);\nauto parsed = hps::from_string<std::vector<int>>(serialized);\nassert(parsed == data);\nstd::cout << \"size (B): \" << serialized.size() << std::endl;\n// size (B): 7\n// Serialized as (in hexadecimal):\n// 02 (number of elements)\n// 16 (first number)\n// cd (first byte of the second number in base-128 encoding)\n// 02 (second byte of the second number)\nreturn 0;\n}"
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 3,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "Case #2: Custom Class - A Quantum System\n#include <cassert>\n#include <iostream>\n#include \"../src/hps.h\"\nclass QuantumState {\npublic:\nunsigned n_elecs;\nstd::unordered_set<unsigned> orbs_from;\nstd::unordered_set<unsigned> orbs_to;\ntemplate <class B>\nvoid serialize(B& buf) const {\nbuf << n_elecs << orbs_from << orbs_to;\n}\ntemplate <class B>\nvoid parse(B& buf) {\nbuf >> n_elecs >> orbs_from >> orbs_to;\n}\n};\nint main() {\nQuantumState qs;\nqs.n_elecs = 33;\nqs.orbs_from.insert({11, 22});\nqs.orbs_to.insert({44, 66});\nstd::string serialized = hps::to_string(qs);\nstd::cout << \"size (B): \" << serialized.size() << std::endl;\n// size (B): 7\n// HPS looks for the serialize<B> and parse<B> in QuantumState.\n// The first byte is n_elecs, then 3 bytes for each unordered_set.\nreturn 0;\n}\nCompared to Protocol Buffers, HPS does not store data types and field\nnumbers. This reduces the size of the serialized message and also increases\nthe performance. In the extreme case, if the leaf messages contain only"
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 4,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "non-repeating fields, the serialized message from Protocol Buffers will be 2\ntimes larger than HPS.\nImplementation\nThere are two key classes in the implementation of HPS, the Serializer and the\nBuffer.\nBuffer provides the read/write char(s) interfaces to the Serializer, and provides\nthe \"<<\" and \">>\" operators to the wrapper functions.\nSerializer<DataType, BufferType> provides the logical serialize and parse\nmethods of the given DataType. It uses the read/write char(s) methods of\nBufferType to push data to or pull data from the buffer. To make the codebase\nhighly maintainable without sacrificing performance, we heavily use static\npolymorphism and SFINAE. For example, here is the Serializer specialization for\nthe floating point numbers:\ntemplate <class T, class B>\nclass Serializer<T, B, typename\nstd::enable_if<std::is_floating_point<T>::value, void>::type> {\nPublic:\nstatic void serialize(const T& num, B& ob) {\nconst char* num_ptr = reinterpret_cast<const char*>(&num);\nob.write(num_ptr, sizeof(num));\n}\nstatic void parse(T& num, B& ib) {\nchar* num_ptr = reinterpret_cast<char*>(&num);\nib.read(num_ptr, sizeof(num));\n}\n};\nThe wrapper functions call the Serializer to provide an easy to use interface to\nthe users, for example:\nstd::ofstream out_file(\"data.log\", std::ofstream::binary);\nhps::to_stream(data, out_file);"
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 5,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "std::ifstream in_file(\"data.log\", std::ifstream::binary);\nauto parsed = hps::from_stream<std::vector<int>>(in_file);\nBy default, Serializer<DataType, BufferType> will call the serialize<B> and\nparse<B> methods of the corresponding class. This provides a simple and\nloosely coupled way of extending HPS for custom classes (See the quantum\nsystem case above for example).\nThe complete implementation is hosted at \u200bhttps://github.com/jl2922/hps.\u200b\nBenchmark\nThe performance of HPS compared to other well-known C++ serializers for\nsome most common data structures in high performance computing are as\nfollows: (less is better)"
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 6,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "The sparse matrix is stored as a list of rows, each of which contains a list of\n64-bit integers for the column indices and a list of doubles for the values. The\nhash map is a map from strings to doubles. Both HPS and Boost can serialize\nstd::unordered_map directly, ProtoBuf uses its own Map type and CapnProto\ndoes not support hash map or similar types.\nWe can see that HPS is consistently and sometimes significantly faster than\nBoost and Protocol Buffers. It also beats Capnproto in all the cases except the\nsparse matrix. However, for the sparse matrix, although Capnproto is faster, the\nserialized message is also significantly larger, and for large distributed systems,\nminimizing the size of the data transferred is often more important than\nminimizing the local CPU time.\nIn addition to the traditional benchmarks for computational cost, we also\nprovide the cognitive effort in terms of source lines of code for these test\ncases: (less is better)\nSLOC double array sparse matrix hash map fixed cost\nprotobuf 12 23 12 17\ncapnproto 15 25 - 21"
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 6,
          "table_index": 1,
          "content": [
            [
              "SLOC",
              "double array",
              "sparse matrix",
              "hash map",
              "fixed cost"
            ],
            [
              "protobuf",
              "12",
              "23",
              "12",
              "17"
            ],
            [
              "capnproto",
              "15",
              "25",
              "-",
              "21"
            ]
          ]
        },
        {
          "page": 7,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "boost 13 20 13 13\nhps 7 16 7 2\nNote: fixed cost includes the estimated number of lines of commands needed\nfor an experienced user to install the library, set the environment variables, extra\nlines of code needed in the Makefile, and various includes, etc.\nDue to the header-only nature of HPS, it is the easiest one to set up, as we can\nsee from the table above.\nConclusion\nIn the paper, I present a high performance serialization library based on C++11.\nIt is easy to use and beats the state of the art performance. Data serialization is\nimportant and may be the dominating factor of performance in many\napplications. HPS enables developers to serialize their structured data to\ncompact binary formats efficiently so that their applications will incur less\nnetwork traffic and disk operations and thus cost less and run faster.\nAcknowledgements\nThis work is supported by the U.S. National Science Foundation (NSF) grant\nACI-1534965 and the Air Force Office of Scientific Research (AFOSR) grant\nFA9550-18-1-0095. We also thank professor Cyrus Umrigar for the helpful\nsuggestions for the paper.\nReference\nVarda, Kenton. \"Protocol buffers: Google\u2019s data interchange format.\" Google\nOpen Source Blog, Available at least as early as Jul 72 (2008)."
            ],
            [
              ""
            ]
          ]
        },
        {
          "page": 7,
          "table_index": 1,
          "content": [
            [
              "boost",
              "13",
              "20",
              "13",
              "13"
            ],
            [
              "hps",
              "7",
              "16",
              "7",
              "2"
            ]
          ]
        },
        {
          "page": 8,
          "table_index": 0,
          "content": [
            [
              ""
            ],
            [
              ""
            ],
            [
              "Stuart, Stephen, and Rex Fernando. \"Encoding rules and MIME type for Protocol\nBuffers.\" (2012).\nVarda, Kenton. \"Cap\u2019n Proto, 2015.\" URL: https://capnproto.org (2015).\nRamey, Robert. \"Boost serialization library.\" URL:\nwww.boost.org/doc/libs/release/libs/serialization (2008).\nVandevoorde, David, and Nicolai M. Josuttis. C++ Templates. Addison-Wesley\nLongman Publishing Co., Inc., 2002."
            ]
          ]
        }
      ],
      "images": [
        "processed/images/1811.04556v2_page5_img0.png",
        "processed/images/1811.04556v2_page6_img0.png"
      ],
      "status": "completed",
      "json_file": "processed/compiled/1811.04556v2_compiled.json"
    },
    {
      "metadata": {
        "title": "Architectures for High Performance Computing and Data Systems using   Byte-Addressable Persistent Memory",
        "authors": [
          "Adrian Jackson",
          "Michele Weiland",
          "Mark Parsons",
          "Bernhard Homoelle"
        ],
        "abstract": "Non-volatile, byte addressable, memory technology with performance close to main memory promises to revolutionise computing systems in the near future. Such memory technology provides the potential for extremely large memory regions (i.e. > 3TB per server), very high performance I/O, and new ways of storing and sharing data for applications and workflows. This paper outlines an architecture that has been designed to exploit such memory for High Performance Computing and High Performance Data Analytics systems, along with descriptions of how applications could benefit from such hardware.",
        "published": "",
        "arxiv_id": "1805.10041v1",
        "categories": [
          "cs.DC",
          "cs.AR"
        ],
        "pdf_url": "http://arxiv.org/pdf/1805.10041v1",
        "pdf_file": "data/pdfs/1805.10041v1.pdf",
        "pdf_filename": "1805.10041v1.pdf"
      },
      "processing_info": {
        "processed_at": "2025-11-11T10:32:18.769553",
        "is_large_pdf": false,
        "sections_found": 4,
        "tables_found": 1,
        "images_found": 8
      },
      "sections_text": {
        "Abstract": "with performance close to main memory promises to revolutionise computing systems in the near future. Such memory technology provides the potential for extremely large memory regions (i.e. > 3TB per server), very high performance I/O, and new ways of storing and sharing data for applications and work\ufb02ows. This paper outlines an architecture that has been designed to exploit such memory for High Performance Computing and High Performance Data Analytics systems, along with descriptions of how applications could bene\ufb01t from such hardware. Index Terms \u2014Non-volatile memory, persistent memory, storage class memory, system architecture, systemware, NVRAM,",
        "Byte-Addressable Persistent Memory": "Adrian Jackson \u2217 , Mark Parsons \u2020 , Mich`ele Weiland \u2021 EPCC, The University of Edinburgh Edinburgh, United Kingdom \u2217 a.jackson@epcc.ed.ac.uk, \u2020 m.parsons@epcc.ed.ac.uk, \u2021 m.weiland@epcc.ed.ac.uk Bernhard Hom\u00a8olle \u00a7 SVA System Vertrieb Alexander GmbH Paderborn, Germany \u00a7 Bernhard.homoelle@sva.de",
        "Scm, B-Apm": "I. I NTRODUCTION There are a number of new memory technologies that are impacting, or likely to impact, computing architectures in the near future. One example of such a technology is so called high bandwidth memory, already featured today on Intel\u2019s latest many-core processor, the Xeon Phi Knights Landing , and NVIDIA\u2019s latest GPU, Volta . These contain MCDRAM and HBM2 respectively, memory technologies built with traditional DRAM hardware but connected with a very wide memory bus (or series of buses) directly to the processor to provide very high memory bandwidth when compared to traditional main memory (DDR channels). This has been enabled, in part, by the hardware trend for incorporating memory controllers and memory controller hubs directly onto processors, enabling memory to be attached to the processor itself rather than through the motherboard and associated chipset. However, the underlying memory hardware is the same, or at least very similar, to the traditional volatile DRAM memory that is still used as main memory for computer architectures, and that remains attached to the motherboard rather than the processor. Non-volatile memory, i.e. memory that retains data even after power is turned off, has been exploited by consumer electronics and computer systems for many years. The \ufb02ash memory cards used in cameras and mobile phones are an example of such hardware, used for data storage. More recently, \ufb02ash memory has been used for high performance I/O in the form of Solid State Disk (SSD) drives, providing higher bandwidth and lower latency than traditional Hard Disk Drives (HDD). Whilst \ufb02ash memory can provide fast input/output (I/O) performance for computer systems, there are some draw backs. It has limited endurance when compare to HDD technology, restricted by the number of modi\ufb01cations a memory cell can undertake and thus the effective lifetime of the \ufb02ash storage . It is often also more expensive than other storage technologies. However, SSD storage, and enterprise level SSD drives, are heavily used for I/O intensive functionality in large scale computer systems because of their random read and write performance capabilities. Byte-addressable random access persistent memory (BAPM), also known as storage class memory (SCM), NVRAM or NVDIMMs, exploits a new generation of non-volatile memory hardware that is directly accessible via CPU load/store operations, has much higher durability than standard \ufb02ash memory, and much higher read and write performance. High-performance computing (HPC) and high-performance data analytics (HPDA) systems currently have different hardware and con\ufb01guration requirements. HPC systems generally require very high \ufb02oating-point performance, high memory bandwidth, and high-performance networks, with a highperformance \ufb01lesystem for production I/O and possibly a larger \ufb01lesystem for long term data storage. However, HPC applications do not generally have large memory requirements (although there are some exceptions to this) . HPDA systems on the other hand often do have high memory capacity requirements, and also require a high-performance \ufb01lesystem to enable very large amounts of data to be read, processed, and written. B-APM, with its very high performance I/O characteristics, and vastly increased capacity (compared to volatile memory), offers a potential hardware solution to enable the construction arXiv:1805.10041v1 [cs.DC] May of a compute platform that can support both types of use case, with high performance processors, very large amounts of B-APM in compute nodes, and a high-performance network, providing a scalable compute, memory, and I/O system. In this paper, we outline the systemware and hardware required to provide such a system. We start by describing persistent memory, and the functionality it provides, in more detail in section II. In section III we discuss how B-APM could be exploited for scienti\ufb01c computation or data analytics. Following this we outline our proposed hardware and systemware architectures in sections IV and V, describe how applications could bene\ufb01t from such a system in section VI. We \ufb01nish by discussing related work in section VII summarising the paper in the \ufb01nal section. II. P ERSISTENT M EMORY B-APM takes new non-volatile memory technology and packages it in the same form factor (i.e. using the same connector and dimensions) as main memory (SDRAM DIMM form factor). This allows B-APM to be installed and used alongside DRAM based main memory, accessed through the same memory controller. As B-APM is installed in a processors memory channels, applications running on the system can access B-APM directly as if it was main memory, including true random data access at byte or cache line granularity. Such an access mechanism is very different to the traditional block based approaches used for current HDD or SSD devices, which generally requires I/O to be done using blocks of data (i.e. 4KB of data written or read in one operation), and relies on expensive kernel interrupts. The B-APM technology that will be the \ufb01rst to market is Intel and Microns 3D XPoint TM memory . The performance of this, byte-addressable, B-APM, is projected to be lower than main memory (with a latency \u223c 5-10x that of DDR4 memory when connected to the same memory channels), but much faster than SSDs or HDDs. It is also projected to be of much larger capacity than DRAM, around 2-5x denser (i.e. 2-5x more capacity in the same form factor). A. Data Access This new class of memory offers very large memory capacity for servers, as well as long term very high performance persistent storage within the memory space of the servers, and the ability to undertake I/O (reading and writing storage data) in a new way. Direct access (DAX) from applications to individual bytes of data in the B-APM is very different from the block-oriented way I/O is currently implemented. B-APM has the potential to enable synchronous, byte level I/O, moving away from the asynchronous block-based \ufb01le I/O applications currently rely on. In current asynchronous I/O user applications pass data to the operating system (OS) which then use driver software to issue an I/O command, putting the I/O request into a queue on a hardware controller. The hardware controller will process that command when ready, notifying the OS that the I/O operation has \ufb01nished through an interrupt to the device driver. B-APM can be accessed simply by using a load or store instruction, as with any other memory operation from an application or program. If the application requires persistence, it must \ufb02ush the data from the volatile CPU caches and ensure that the same data has also arrived on the non-volatile medium. There are cache \ufb02ush commands and fence instructions. To keep the performance for persistence writes, new cache \ufb02ush operation have been introduced. Additionally, write buffers in the memory controller may be protected by hardware through modi\ufb01ed power supplies (such as those supporting asynchronous DRAM refresh ). With B-APM providing much lower latencies than external storage devices, the traditional I/O block access model, using interrupts, becomes inef\ufb01cient because of the overhead of context switches between user and kernel mode (which can take thousands of CPU cycles ). Furthermore, with B-APM it becomes possible to implement remote persistent access to data stored in the memory using RDMA technology over a suitable interconnect. Using high performance networks can enable access to data stored in B-APM in remote nodes faster than accessing local high performance SSDs via traditional I/O interfaces and stacks inside a node. Therefore, it is possible to use B-APM to greatly improve I/O performance within a server, increase the memory capacity of a server, or provide a remote data store with high performance access for a group of servers to share. Such storage hardware can also be scaled up by adding more BAPM memory in a server, or adding more nodes to the remote data store, allowing the I/O performance of a system to scale as required. However, if B-APM is provisioned in the servers, there must be software support for managing data within the B-APM. This includes moving data as required for the jobs running on the system, and providing the functionality to let applications run on any server and still utilise the B-APM for fast I/O and storage (i.e. applications should be able to access B-APM in remote nodes if the system is con\ufb01gured with BAPM only in a subset of all nodes). As B-APM is persistent, it also has the potential to be used for resiliency, providing backup for data from active applications, or providing long term storage for databases or data stores required by a range of applications. With support from the systemware, servers can be enabled to handle power loss without experiencing data loss, ef\ufb01ciently and transparently recovering from power failure and resuming applications from their latest running state, and maintaining data with little overhead in terms of performance. B. B-APM modes of operation Ongoing developments in memory hierarchies, such as the high bandwidth memory in Xeon Phi manycore processors or NVIDIA GPUS, have provided new memory models for programmers and system designers/implementers. A common model that has been proposed includes the ability to con\ufb01gure Fig. 1. Single-level memory (SLM) con\ufb01guration using main memory and B-APM Fig. 2. Dual-level memory (DLM) con\ufb01guration using main memory and B-APM main memory and B-APM in two different modes: Single-level and Dual-level memory . Single-level memory, or SLM, has main memory (DRAM) and B-APM as two separate memory spaces, both accessible by applications, as outlined in Figure 1. This is very similar to the Flat Mode con\ufb01guration of the high bandwidth, on-package, MCDRAM in current Intel Knights Landing processor. The DRAM is allocated and managed via standard memory API\u2019s such as malloc and represent the OS visible main memory size. The B-APM will be managed by programming APIs and present the non-volatile part of the system memory. Both will allow direct CPU load/store operations. In order to take advantage of B-APM in SLM mode, systemware or applications have to be adapted to use these two distinct address spaces. Dual-level memory, or DLM, con\ufb01gures DRAM as a cache in front of the B-APM, as shown in Figure 2. Only the memory space of the B-APM is available to applications, data being used is stored in DRAM, and moved to B-APM when no longer immediately required by the memory controller (as in standard CPU caches). This is very similar to the Cache Mode con\ufb01guration of MCDRAM on KNL processors. This mode of operation does not require applications to be altered to exploit the capacity of B-APM, and aims to give memory access performance at main memory speeds whilst providing access to the large memory space of B-APM. However, exactly how well the main memory cache performs will depend on the speci\ufb01c memory requirements and access pattern of a given application. Furthermore, persistence of the B-APM contents cannot be longer guaranteed, due to the volatile DRAM cache in front of the B-APM, so the nonvolatile characteristics of B-APM are not exploited. Fig. 3. PMDK software architecture C. Non-volatile memory software ecosystem The Storage Networking Industry Association (SNIA) have produced a software architecture for B-APM with persistent load/store access, formalised in the Linux Persistent Memory Development Kit (PMDK) library. This approach re-uses the naming scheme of \ufb01les as traditional persistent entities and map the B-APM regions into the address space of a process (similar to memory mapped \ufb01les in Linux). Once the mapping has been done, the \ufb01le descriptor is no longer needed and can be closed. Figure outlines the PMDK software architecture. III. O PPORTUNITIES FOR EXPLOITING B-APM FOR COMPUTATIONAL SIMULATIONS AND DATA ANALYTICS Reading data from and writing it to persistent storage is usually not the most time consuming part of computational simulation applications. Analysis of common applications from a range of different scienti\ufb01c areas shows that around 5-20% of runtime for applications is involved in I/O operations . It is evident that B-APM can be used to improve I/O performance for applications by replacing slower SSDs or HDDs in external \ufb01lesystems. However, such a use of B-APM would be only an incremental improvement in I/O performance, and would neglect some of the signi\ufb01cant features of B-APM that can provide performance bene\ufb01ts for applications. Firstly, deploying B-APM as an external \ufb01lesystem would require provisioning a \ufb01lesystem on top of the B-APM hardware. Standard storage devices require a \ufb01lesystem to enable data to be easily written to or read from the hardware. However, B-APM does not require such functionality, and data can be manipulated directly on B-APM hardware simply through load store instructions. Adding the \ufb01lesystem and associated interface guarantees (i.e. POSIX interface ), adds performance overheads that will reduce I/O performance on B-APM. Secondly, an external B-APM based \ufb01lesystem would require all I/O operations to be performed over a network connection, as current \ufb01lesystems are external to compute nodes (see Figure 4). This would limit the maximum performance Fig. 4. Current external storage for HPC and HPDA systems Fig. 5. Internal storage using B-APM in compute nodes for HPC and HPDA systems of I/O to that of the network between compute nodes and the nodes the B-APM is hosted in. Our vision for exploiting B-APM for HPC and HPDA systems is to incorporate the B-APM into the compute nodes, as outlined in Figure 5. This architecture allows applications to exploit the full performance of B-APM within the compute nodes they are using, by giving them the ability to access BAPM through load/store at byte-level granularity, as opposed to block based, asynchronous I/O in traditional storage devices. Incorporating B-APM into compute nodes also has the bene\ufb01t that I/O capacity and bandwidth can scale with the number of compute nodes in the system. Adding more compute nodes will increase the amount of B-APM in the system and add more aggregate bandwidth to I/O/B-APM operations. For example, current memory bandwidth of a HPC system scales with the number of nodes used. If we assume an achievable memory bandwidth per node of 100GB/s, then it follows that a system with nodes has the potential to provide 1TB/s of memory bandwidth for a distributed application, and a system with nodes can provide 1PB/s of memory bandwidth. If an application is memory bandwidth bound and can parallelise across nodes then scaling up nodes like this clearly has the potential to improve performance. For B-APM in nodes, and taking 3D XPoint TM as an example, if we assume 20GB/s of memory bandwidth per node (5x less than the volatile memory bandwidth), then scaling up to nodes provides 200GB/s of (I/O) memory bandwidth and nodes provides 200TB/s of (I/O) memory bandwidth. For comparison, the Titan system at ORNL has a Lustre \ufb01le system with 1.4TB/s of bandwidth and they are aiming for 50TB/s of burst buffer I/O by . Furthermore, there is the potential to optimise not only the performance of a single application, but rather the performance of a whole scienti\ufb01c work\ufb02ow, from data preparation through simulations, to data analysis and visualisation. Optimising full work\ufb02ows by sharing data between different stages or steps in the work\ufb02ow has the potential to completely remove, or greatly reduce, data movement/storage costs for large parts of the work\ufb02ow altogether. Leaving data in-situ on B-APM for other parts of the work\ufb02ow can signi\ufb01cantly improve the performance of analysis and visualisation steps at the same time as reducing I/O costs for the application when writing the data out. The total runtime of an application can be seen as the sum of its compute time, plus the time spent in I/O. Greatly reduced I/O costs therefore also have the bene\ufb01cial side effect of allowing applications to perform more I/O within the same total cost of the overall simulation. This enables applications to maintain I/O costs in line with current behaviour whilst being able to process signi\ufb01cantly more data. Furthermore, for those applications for which I/O does take up a large portion of the run time, including data analytics applications, B-APM has the potential to signi\ufb01cantly reduce runtime. A. Potential caveats However, utilising internal storage is not without drawbacks. Firstly, the bene\ufb01t of external storage is that there is a single namespace and location for compute nodes to use for data storage and retrieval. This means that applications running on the compute nodes can access data trivially as it is stored externally to the compute nodes. With internal storage, this guarantee is not provided. Data written to B-APM is local to speci\ufb01c compute nodes. It is therefore necessary for applications to be able to manage and move data between compute nodes, as well as to external data storage, or for some systemware components to undertake this task. Secondly, B-APM may be expensive to provision in all compute nodes. It may not be practical to add the same amount of B-APM to all compute nodes, and systems may be constructed with islands of nodes with B-APM, and islands of nodes without B-APM. Therefore, application or systemware functionality to enable access to remote B-APM and to exploit/manage asymmetric B-APM con\ufb01gurations will be required. Both these issues highlight the requirement for an integrated hardware and software (systemware) architecture to enable ef\ufb01cient and easy use of this new memory technology in large scale computational platforms. IV. H ARDWARE ARCHITECTURE As 3D Xpoint TM memory, and other B-APM when it becomes available, is designed to \ufb01t into standard memory form factors and be utilised using the same memory controllers that main memory exploit, the hardware aspect of incorporating BAPM into a compute server or system is not onerous. Standard HPC and HPDA systems comprise a number of compute nodes, connected together with a high performance network, along with login nodes and an external \ufb01lesystem. Inside a compute node there are generally or more multicore processors, connected to a shared motherboard, with associated volatile main memory provided for each processor. One or more network connections are also required in each node, generally connected to the PCIe bus on the motherboard. Fig. 6. Compute node hardware architecture To construct a compute cluster that incorporates B-APM all that is required is a processor and associated memory controller that support such memory. Customised memory controllers are required to intelligently deal with the variation in performance between B-APM and traditional main memory (i.e. DDR). For instance, as B-APM has a higher access latency than DDR memory it would impact performance if B-APM accesses were blocking, i.e. if the memory controller could not progress DDR accesses whilst an B-APM access was outstanding. However, other than modifying the memory controller to support such variable access latencies, it should be possible to support B-APM in a standard hardware platform, provided that suf\ufb01cient capacity for memory is provided. Given both DRAM and B-APM are connected through the same memory controller, and memory controllers have a number of memory channels, it is also important to consider the balance of DRAM and B-APM attached to a processor. If we assume a processor has memory channels, to get full DRAM bandwidth we require at least one DRAM DIMM per memory channel. Likewise, if we want full B-APM bandwidth we need a B-APM DIMM per memory channel. Assuming that a memory channel can support are two DIMM slots, this leads us to a con\ufb01guration with DRAM DIMMs and B-APM DIMMs per processor, and double that with two processors per node. This con\ufb01guration is also desirable to enable the DLM con\ufb01guration, as DLM requires DRAM available to act as a cache for B-APM, meaning at least a DRAM DIMM is required per memory controller. Pairing DRAM and B-APM DIMMs on memory channels is not required for all systems, and it should be possible to have some memory channels with no B-APM installed, or some memory channels with no DRAM DIMMs installed. However, if DLM mode is required on a system, it is sensible to expect that at least one DRAM DIMM must be installed per memory controller in addition to B-APM. Future system design may consider providing more than two DIMM slots per memory channel to facilitate systems with different memory con\ufb01gurations (i.e. more B-APM than DRAM DIMMs or memory controllers enabling full B-APM population of memory channels). The proposed memory con\ufb01guration allows us to investigate possible system con\ufb01gurations using B-APM memory. Table I outlines different systems, assuming 3TB of B-APM per node, with a node capable of 2TFlop/s compute performance. These projections are achievable with existing processor technology, and demonstrate that very signi\ufb01cant I/O bandwidth can be provided to match the compute performance achieved when scaling to very large numbers of nodes. TABLE I B-APM ENABLED SYSTEM CONFIGURATIONS Number of Compute B-APM Capacity B-APM Storage I/O nodes (PFlop/s) (PB) Bandwidth (TB/s) 0. 0. 0. 1. 2. Integrating new memory technology in existing memory channels does mean that providing suf\ufb01cient locations for both main memory and B-APM to be added is important. Depending on the size of B-APM and main memory technology available, suf\ufb01cient memory slots must be provided per processor to allow a reasonable amount of both memory types to be added to a node. Therefore, we are designing our system around a standard compute node architecture with suf\ufb01cient memory slot provision to support large amounts of main memory and B-APM as shown in Figure 6. Another aspect, which we are not focusing on in the hardware architecture, is data security. As B-APM enables data to be retained inside compute nodes, ensuring the security of that data, and ensuring that it cannot be accessed by users or applications that are not authorised to access the data is important. The reason that we are not focusing on this in the hardware architecture is because this requirement can be addressed in software, but it may also be sensible to integrate encryption directly in the memory hardware, memory controller, or processor managing the B-APM. V. S YSTEMWARE ARCHITECTURE Systemware implements the software functionality necessary to enable users to easily and ef\ufb01ciently utilise the system. We have designed a systemware architecture that provides a number of different types of functionality, related to different methods for exploiting B-APM for large scale computational simulation or data analytics. From the hardware features B-APM provides, our analysis of current HPC and HPDA applications and functionality they utilise, and our investigation of future functionality that may bene\ufb01t such applications, we have identi\ufb01ed a number of different kinds of functionality that the systemware architecture should support: 1) Enable users to be able to request systemware components to load/store data in B-APM prior to a job starting, or after a job has completed. This can be thought of as similar to current burst buffer technology. This will allow users to be able to exploit B-APM without changing their applications. 2) Enable users to directly exploit B-APM by modifying their applications to implement direct memory access and management. This offers users the ability to access the best performance B-APM can provide, but requires application developers to undertake the task of programming for B-APM themselves, and ensure they are using it in an ef\ufb01cient manner. 3) Provide a \ufb01lesystem built on the B-APM in compute nodes. This allows users to exploit B-APM for I/O operations without having to fundamentally change how I/O is implemented in their applications. However, it does not enable the bene\ufb01t of moving away from \ufb01le based I/O that B-APM can provide. 4) Provide an object, or key value, store that exploits the B-APM to enable users to explore different mechanisms for storing and accessing data from their applications. 5) Enable the sharing of data between applications through B-APM. For example, this may be sharing data between different components of the same computational work\ufb02ow, or be the sharing of a common dataset between a group of users. 6) Ensure data access is restricted to those authorised to access that data and enable deletion or encryption of data to make sure those access restrictions are maintained 7) Provision of pro\ufb01ling and debugging tools to allow application developers to understand the performance characteristics of B-APM, investigate how their applications are utilising it, and identify any bugs that occur during development. 8) Ef\ufb01cient check-pointing for applications, if requested by users. 9) Provide different memory modes if they are supported by the B-APM hardware. 10) Enable or disable systemware components as required for a given user application to reduce the performance impact of the systemware to a running application, if that application is not using those systemware components. The systemware architecture we have de\ufb01ned is outlined in Figure 7. Whilst this architecture may appear to have a large number of components and signi\ufb01cant complexity, the number of systemware components that are speci\ufb01c to a system that contains B-APM is relatively small. The new or modi\ufb01ed components we have identi\ufb01ed are required to support BAPM in a large scale, multi-user, multi-application, compute platforms are as follows: \u2022 Job Scheduler \u2022 Data Scheduler \u2022 Object Store \u2022 Filesystem \u2022 Programming Environment We will describe these in more detail in the following subsections. A. Job scheduler As the innovation in our proposed system is the inclusion of B-APM within nodes, one of the key components that must support the new hardware resource is the job scheduler. Job schedulers, or batch systems, are used to manage, schedule, and run user jobs on the shared resource that are the compute nodes. Standard job schedulers are con\ufb01gured with the number of nodes in a system, the number of cores per node, and possibly the amount of memory or whether there are accelerators (like GPUs) in compute nodes in a system. They then use this information, along with a scheduling algorithm and scheduling policies to allocate user job request to a set of compute nodes. Users submit job requests specifying the compute resources required (i.e. number of node or number of compute cores a job will require) along with a maximum runtime for the job. This information is used by the job scheduler to accurately, ef\ufb01ciently, and fairly assign applications to resources. Adding B-APM to compute nodes provides another layer of hardware resource that needs to be monitored and scheduled against by the job scheduler. As data can persist in B-APM, and one of our target use cases is the sharing of data between applications using B-APM, the job scheduler needs to be extended to both be aware of this new hardware resource, and to allow data to be retained in B-APM after an individual job has \ufb01nished. This functionality is achieved through adding work\ufb02ow awareness to the job scheduler, providing functionality to allow data to be retained and shared through jobs participating in the work\ufb02ow, although not inde\ufb01nitely . The job scheduler also needs to be able to clean up the B-APM after a job has \ufb01nished, ensuring no data is left behind or B-APM resources consumed, unless speci\ufb01cally as part of a work\ufb02ow. Furthermore, as the memory system is likely to have different modes of operation, the job scheduler will need to be able to query the current con\ufb01guration of the memory hardware, and be able to change con\ufb01guration modes if required by the next job that will be using a particular set of compute nodes. We are also investigating new scheduling algorithms, specifically data aware and energy aware scheduling algorithms, to optimise system ef\ufb01ciency or throughput using B-APM functionality. These will utilise the job schedulers awareness of B-APM functionality and compute job data requirements. B. Data scheduler The data scheduler is an entirely new component, designed to run on each compute node and provide data movement and shepherding functionality. Much of the new functionality we are implementing to exploit B-APM for users involves moving data to and from B-APM asynchronously (i.e. pre-loading data before a job starts, or moving data from B-APM after a job \ufb01nishes). Furthermore, we also require the ability to move data between different nodes (i.e. in the case that a job runs on a node without B-APM and requires B-APM functionality, or a job runs and needs to access data left on B-APM in a different node by another job). To provide such support without requiring users to modify their applications we implement such functionality in the data scheduler component. This component has interfaces for applications to interact with, and is also interfaced with the job scheduler component on each compute node. Through these Fig. 7. Systemware architecture to exploit B-APM hardware in Compute nodes interfaces the data scheduler can be instructed to move data as required by a given application or work\ufb02ow. C. Object store We recognise the performance and functionality bene\ufb01ts that exploiting new storage technologies can bring to applications. We are therefore investigating the use of object stores, such as DAOS and dataClay and are porting them to the hardware architecture we are proposing, i.e. systems with distributed B-APM as the main storage hardware. D. Filesystems As previously discussed, there is a very large pre-existing code base currently exploiting HPC and HPDA systems. The majority of these will undertake I/O using \ufb01les through an external \ufb01lesystem. Therefore, the easiest mechanism for supporting such applications, in the \ufb01rst instance, is to provide \ufb01lesystems hosted on the B-APM hardware. Our architecture provides the functionality for two different types of \ufb01lesystems using B-APM: \u2022 Local, on-node \u2022 Distributed, cross-node The local \ufb01lesystem will provide applications with a space for reading or writing data from/to a \ufb01lesystem that is separate for each compute node, i.e. a scratch or /tmp \ufb01lesystem on each node. This will enable very high performance \ufb01le I/O, but require applications (or the data scheduler) to manage these \ufb01les. It will also provide a storage space for \ufb01les to be loaded prior to a job starting (i.e. similar to burst buffer functionality), or for \ufb01les that should be move to an external \ufb01lesystem when a job has \ufb01nished. The distributed \ufb01lesystem will provide functionality similar to current parallel \ufb01lesystems (e.g Lustre), except it will be hosted directly on the B-APM hardware and not require external I/O servers or nodes. E. Programming environment Finally, the programming environment, i.e. libraries, compilers, programming languages, communication libraries, used by application needs to be modi\ufb01ed to support B-APM. An obvious example of such requirements is ensuring that common I/O libraries support using B-APM for storage. For instance, many computational simulation applications use MPI-I/O, HDF5, or NetCDF to undertake I/O operations. Ensuring these libraries can utilise B-APM in some way to undertake high performance I/O will ensure a wide range of existing applications can exploit the system effectively. Further modi\ufb01cations, or new functionality, may also be bene\ufb01cial. For instance, we will deploy a task based programming environment, PyCOMPs , which can interact directly with object storage. This will enable us to evaluate whether new parallel programming approaches will enable exploitation of B-APM and the functionality it provides more easily than adapting existing applications to functionality such as object stores or byte-based data accesses to B-APM. VI. U SING B-APM To allow a fuller understanding of how a system developed from the architectures we have designed could be used, we discuss some of the possible usage scenarios in the following text. We outline the systemware and hardware components used by a given use case, and the lifecycle of the data in those components. A. Filesystems on B-APM For this use case we assume an application undertaking standard \ufb01le based I/O operations, using either parallel or serial I/O functionality. We assume the distributed, cross-node, B-APM \ufb01lesystem is used for I/O whilst the application is running, and the external high performance \ufb01lesystem is used for data storage after an application has \ufb01nished. The job scheduling request includes a request for \ufb01les to be pre-loaded on to the distributed B-APM \ufb01lesystem prior to the job starting. In this use case we expect the following systemware interaction to occur (also outlined in Figure 8): 1) User job scheduling requests submitted. a. At this point the application to be run is either stored on the external high performance \ufb01lesystem or on the local \ufb01lesystem on the login nodes, and the data for the application is stored on the external high performance \ufb01lesystem. 2) Job scheduler allocates resources for an application. a. The job scheduler ensures that nodes are in SLM mode and that the multi-node B-APM \ufb01lesystem component is operational on the nodes being used by this application. 3) Once the nodes that the job has been allocated to are available the data scheduler (triggered by the job scheduler) copies input data from the external high performance \ufb01lesystem to the multi-node B-APM \ufb01lesystem. a. This step is optional; an application could read input data directly from the external high performance \ufb01lesystem, but using the multi-node B-APM \ufb01lesystem will deliver better performance. 4) Job Launcher starts the user application on the allocated compute nodes. 5) Application reads data from the multi-node B-APM \ufb01lesystem. 6) Application writes data to the multi-node B-APM \ufb01lesystem. 7) Application \ufb01nishes. 8) Data Scheduler is triggered and moves data from multinode B-APM \ufb01lesystem to the external high performance \ufb01lesystem. VII. R ELATED WORK There are existing technological solutions that are offering similar functionality to B-APM and that can also be exploited for high performance I/O. One example is NVMe devices: SSDs that are attached to the PCIe bus and support the NVM Express interface. Indeed, Intel already has a line of an NVMe device on the market that use 3D XPoint TM memory technology, called Intel Optane. Other vendors have a large range of NVMe devices on the market, most of them based on different variations of Flash technology. NVMe devices have the potential to provide byte-level storage access, using the PMDK libraries. A \ufb01le can be opened and presented as a memory space for an application, and then can be used directly as memory by that application, removing the overhead of \ufb01le access (i.e. data access through \ufb01le reads and writes) when performing I/O and enabling the development of applications that exploit B-APM functionality. However, given that NVMe devices are connected via the PCIe bus, and have a disk controller on the device through Fig. 8. Sequence diagram for systemware component use by application requesting data be loaded into distributed B-APM-based \ufb01lesystem prior to starting which access is managed, NVMe devices do not provide the same level of performance that B-APM offers. Indeed, as these devices still use block-based data access, \ufb01ne grained memory operations can require whole blocks of data to be loaded or stored to the device, rather than individual bytes. There are a wide range of parallel and high performance \ufb01lesystems designed to enable high performance I/O from large scale compute clusters . However, these provide POSIX compliant block based I/O interfaces, which do not offer byte level data access, requiring conversion of data from program data structures to a \ufb02at \ufb01le format. Furthermore, whilst it is advantageous that such \ufb01lesystems are external resources, and therefore can be accessed from any compute node in a cluster, this means that \ufb01lesystem performance does not necessarily scale with compute nodes. Such \ufb01lesystems are speci\ufb01ed and provisioned separately from the compute resource in a HPC or HPDA system. Work has been done to optimise I/O performance of such high performance \ufb01lesystems , but they do not address B-APM or new mechanisms for storing or accessing data without the overhead of a POSIX-compliant (or weaklycompliant) \ufb01lesystem. Another technology that is being widely investigated for improving performance and changing I/O functionality for applications is some form of object, key value, store . These provide alternatives to \ufb01le-based data storage, enabling data to be stored in similar formats or structures as those used in the application itself. Object stores can start to approach byte level access granularity, however, they require applications to be signi\ufb01cantly re-engineered to exploit such functionality. We are proposing hardware and systemware architectures in this work that will integrate B-APM into large scale compute clusters, providing signi\ufb01cant I/O performance bene\ufb01ts and introducing new I/O and data storage/manipulation features to applications. Our key goal is to create systems that can both exploit the performance of the hardware and support applications whilst they port to these new I/O or data storage paradigms. Indeed, we recognise that there is a very large body of existing applications and data analysis work\ufb02ows that cannot immediately be ported to new storage hardware (for time and resource constraint reasons). Therefore, our aims in this work are to provide a system that enables applications to obtain best performance if porting work is undertaken to exploit BAPM hardware features, but still allow applications to exploit B-APM and signi\ufb01cantly improve performance without major software changes. VIII. S UMMARY This paper outlines a hardware and systemware architecture designed to enable the exploitation of B-APM hardware directly by applications, or indirectly by applications using systemware functionality that can exploit B-APM for applications. This dual nature of the system provides support for existing application to exploit this emerging memory new hardware whilst enabling developers to modify applications to best exploit the hardware over time. The system outlined provides a range of different functionality. Not all functionality will be utilised by all applications, but providing a wide range of functionality, from \ufb01lesystems to object stores to data schedulers will enable the widest possible use of such systems. We are aiming for hardware and systemware that enables HPC and HPDA applications to co-exist on the same platform. Whilst the hardware is novel and interesting in its own right, we predict that the biggest bene\ufb01t in such technology will be realised through changes in application structure and data storage approaches facilitated by the byte-addressable persistent memory that will become routinely available in computing systems. In time it could possible to completely remove the external \ufb01lesystem from HPC and HPDA systems, removing hardware complexity and the energy/cost associated with such functionality. There is also the potential for volatile memory to disappear from the memory stack everywhere except on the processor itself, removing further energy costs from compute nodes. However, further work is required to evaluate the impact of the costs of the active systemware environment we have outlined in this paper, and the memory usage patterns of applications. Moving data asynchronous to support applications can potentially bring big performance bene\ufb01ts but the impact such functionality has on applications running on those compute node needs to be investigated. This is especially important as with distributed \ufb01lesystems or object stores hosted on node distributed B-APM such in-node asynchronous data movements will be ubiquitous, even with intelligent scheduling algorithms. A CKNOWLEDGEMENTS The NEXTGenIO project and the work presented in this paper were funded by the European Unions Horizon Research and Innovation programme under Grant Agreement no.",
        "671951. All the NEXTGenIO Consortium members (EPCC,": "Allinea, Arm, ECMWF, Barcelona Supercomputing Centre, Fujitsu Technology Solutions, Intel Deutschland, Arctur and Technische Universit\u00a8at Dresden) contributed to the design of the architectures. R EFERENCES Avinash Sodani. Knights Landing (KNL): 2nd Generation Intel Xeon Phi Processor In Hot Chips Symposium (HCS), IEEE. IEEE, 124. NVIDIA Volta. volta-gpu-architecture Hongshin Jun, Jinhee Cho, Kangseol Lee, Ho-Young Son, Kwiwook Kim, Hanho Jin, Keith Kim, HBM (High Bandwidth Memory) DRAM Technology and Architecture , Memory Workshop (IMW) IEEE International, pp. 1-4, Andy Turner, Simon McIntosh-Smith, A survey of application memory usage on a national supercomputer: an analysis of memory requirements on ARCHER http://www.archer.ac.uk/documentation/white-papers/ memory-use/ARCHER mem use.pdf T. Hady, Frank & Foong, A & Veal, Bryan & Williams, Dan. (2017). Platform Storage Performance With 3D XPoint Technology. Proceedings of the IEEE. PP. 1-12. 10.1109/JPROC.2017.2731776. NVDIMM Messaging and FAQ, SNIA website , accessed November 20and%20FAQ%20Jan%2020143.pdf Report MCDRAM technology from Colfax Research : colfaxresearch.com/knl-mcdram/ Intel Patent on multi-level memory con\ufb01guration for nonvolatile memory technology ; https://www.google.com/patents/US20150178204 pmem.io: http://pmem.io/ Layton J. (2010) IO Pattern Characterization of HPC Applications. In: Mewhort D.J.K., Cann N.M., Slater G.W., Naughton T.J. (eds) High Performance Computing Systems and Applications. Lecture Notes in Computer Science, vol 5976. Springer, Berlin, Heidelberg Huong Luu, Marianne Winslett, William Gropp, Robert Ross, Philip Carns, Kevin Harms, Mr Prabhat, Suren Byna, and Yushu Yao. 2015. A Multiplatform Study of I/O Behavior on Petascale Supercomputers . In Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing (HPDC \u201915). ACM, New York, NY, USA, 33-44. DOI=http://dx.doi.org/10.1145/2749246. IEEE Std 1003.1(Revision of IEEE Std 1003.1-2004) IEEE Standard for Information Technology Portable Operating System Interface (POSIX(R)) Schwan, Philip. Lustre: Building a \ufb01le system for 1000-node clusters. In Proceedings of the Linux Symposium, vol. 2003. Frank Schmuck and Roger Haskin. 2002. GPFS: A Shared-Disk File System for Large Computing Clusters . In Proceedings of the 1st USENIX Conference on File and Storage Technologies (FAST \u201902). USENIX Association, Berkeley, CA, USA, Article 19. Introduction BeeGFS : Introduction to BeeGFS by ThinkParQ.pdf S. Jian, L. Zhan-huai and Z. Xiao, The performance optimization of Lustre \ufb01le system 7th International Conference on Computer Science & Education (ICCSE), Melbourne, VIC, 2012, pp. 214-217. doi: 10.1109/ICCSE.2012. Wonil Choi ; Myoungsoo Jung ; Mahmut Kandemir ; Chita Das, A ScaleOut Enterprise Storage Architecture , IEEE International Conference on Computer Design (ICCD), 2017, 10.1109/ICCD.2017. www.nextgenio.eu Kuan-Wu Lin, Surendra Byna, Jerry Chou, and Kesheng Wu. 2013. Optimizing fastquery performance on lustre \ufb01le system . In Proceedings of the 25th International Conference on Scienti\ufb01c and Statistical Database Management (SSDBM), Alex Szalay, Tamas Budavari, Magdalena Balazinska, Alexandra Meliou, and Ahmet Sacan (Eds.). ACM, New York, NY, USA, , Article , pages. DOI=http://dx.doi.org/10.1145/2484838. Philip Carns, Kevin Harms, William Allcock, Charles Bacon, Samuel Lang, Robert Latham, and Robert Ross. 2011. Understanding and improving computational science storage access through continuous characterization. Proceedings the IEEE 27th Symposium Mass Storage Systems and Technologies (MSST \u201911). IEEE Computer Society, Washington, DC, USA, 1-14. DOI=http://dx.doi.org/10.1109/MSST.2011. Jungwon Kim, Seyong Lee, Jeffrey S. Vetter: PapyrusKV: a highperformance parallel key-value store for distributed NVM architectures . SC 2017: 57:1-57: J. Lofstead, I. Jimenez, C. Maltzahn, Q. Koziol, J. Bent and E. Barton, DAOS and Friends: A Proposal for an Exascale Storage System , SC16: International Conference for High Performance Computing, Networking, Storage and Analysis, Salt Lake City, UT, 2016, pp. 585-596. doi: 10.1109/SC.2016. Jonathan Mart, Anna Queralt, Daniel Gasull, Alex Barcel, Juan Jos Costa, Toni Cortes, Dataclay: distributed data store for effective inter-player data sharing , Journal Systems and Software, Volume 131, 2017, Pages 129-145, ISSN 0164-1212, Enric Tejedor, Yolanda Becerra, Guillem Alomar, Anna Queralt, Rosa M Badia, Jordi Torres, Toni Cortes, Jess Labarta, PyCOMPSs: Parallel computational work\ufb02ows in Python , The International Journal of High Performance Computing Applications, Vol 31, Issue 1, pp. First Published August 19, 201, https://doi.org/10.1177/ E. Farsarakis, I. Panourgias, A. Jackson, J.F.R. Herrera, M. Weiland, M. Parsons; Resource Requirement Speci\ufb01cation for Novel Data-aware and Work\ufb02ow-enabled HPC Job Schedulers , PDSW-DISCS17 http://www. pdsw.org/pdsw-discs17/wips/farsarakis-wip-pdsw-discs17.pdf, Weiland, M, Jackson, A, Johnson, N & Parsons, M 2018, Exploiting the Performance Bene\ufb01ts of Storage Class Memory for HPC and HPDA Work\ufb02ows Supercomputing Frontiers and Innovations, vol 5, no. 1, pp. 79-94. DOI: 10.14529/js\ufb01180105 ORNL Titan speci\ufb01cation http://phys.org/pdf285408062.pdf Valentine Anantharaj, Fernanda Foertter, Wayne Joubert ,Jack Wells Approaching Exascale: Application Requirements for OLCF Leadership Computing https://www.olcf.ornl.gov/wp-content/uploads/2013/01/ OLCF Requirements TM Final1.pdf July Daley, C., Ghoshal, D., Lockwood, G., Dosanjh, S., Ramakrishnan, L., Wright, N.: Performance Characterization of Scienti\ufb01c Work\ufb02ows for the Optimal Use of Burst Buffers. Future Generation Computer Systems (2017), 10.1016/j.future.2017.12. N. R. Mielke, R. E. Frickey, I. Kalastirsky, M. Quan, D. Ustinov and V. J. Vasudevan, Reliability of Solid-State Drives Based on NAND Flash Memory in Proceedings of the IEEE, vol. 105, no. 9, pp. 1725-1750, Sept. 2017. doi: 10.1109/JPROC.2017. Chuanpeng Li, Chen Ding, and Kai Shen. 2007. Quantifying the cost of context switch . In Proceedings of the workshop on Experimental computer science (ExpCS \u201907). ACM, New York, NY, USA, Article ."
      },
      "sections_summary": {
        "Abstract": "New computing technology promises to revolutionize systems by providing large amounts of memory (over 3TB per server), high performance I/O, and new data storage options. A paper outlines an architecture designed to utilize this technology for High Performance Computing and Data Analytics applications.",
        "Byte-Addressable Persistent Memory": "The authors are:\n\n- Adrian Jackson\n- Mark Parsons\n- Mich\u00e8le Weiland\n- Bernhard Homolle",
        "Scm, B-Apm": "Here's a concise summary of the proposed B-APM (Bit-Herz Aggregate Persistent Memory) technology:\n\nB-APM is a new non-volatile memory technology that enables true random data access at byte granularity, offering high performance and large capacity compared to traditional storage devices. It has the potential to revolutionize computing architectures by providing fast persistent storage with lower latencies than external devices.\n\nThe proposed system architecture includes:\n\n* High-performance processors\n* Large amounts of B-APM in compute nodes\n* A high-performance network\n\nB-APM allows for:\n\n* Fast persistent storage with lower latencies than external devices\n* Efficient I/O operations within a server or across multiple nodes\n* Scalability, with projected performance lower than main memory but faster than SSDs or HDDs\n* Resiliency, providing backup and long-term storage for applications\n\nThe system also includes various software components:\n\n* Job scheduler to manage and schedule user jobs on shared resources\n* Data scheduler to handle data movement and shepherding functionality\n* Object store to handle B-APM data storage and retrieval\n* Filesystem adapted to work with B-APM\n* Programming environment modified to support B-APM functionality\n\nThe proposed system aims to:\n\n* Simplify application structure and data storage approaches by leveraging byte-addressable persistent memory\n* Remove external filesystems and volatile memory in compute nodes\n* Enable coexistence of HPC and HPDA applications on the same platform\n\nHowever, further evaluation is needed to assess the impact on system costs and applications' performance.\n\nKey benefits include:\n\n* Improved I/O performance for computational simulations and data analytics\n* Reduced I/O costs by allowing applications to process more data within the same total cost\n* Scalability and resiliency\n\nOverall, B-APM has the potential to transform computing architectures by providing fast persistent storage with lower latencies than external devices, while simplifying application structure and data storage approaches.",
        "671951. All the NEXTGenIO Consortium members (EPCC,": "Here is a concise overall summary:\n\nA collaboration between multiple contributors designed innovative high-performance computing architectures, leveraging cutting-edge technologies such as HBM DRAM, 3D XPoint, NVDIMM, and Storage Class Memory. Researchers studied I/O behavior, memory usage, and performance optimization to develop scalable storage systems and workflow management solutions. Key papers focus on designing exascale storage systems, improving parallel computational workflows, and understanding the benefits of emerging technologies like Storage Class Memory, with a goal of optimizing data sharing and job scheduling in high-performance computing environments."
      },
      "tables": [
        {
          "page": 5,
          "table_index": 0,
          "content": [
            [
              "Numberof\nnodes",
              "Compute\n(PFlop/s)",
              "B-APMCapacity\n(PB)",
              "B-APMStorageI/O\nBandwidth(TB/s)"
            ],
            [
              "1",
              "0.002",
              "0.003",
              "0.02"
            ],
            [
              "768",
              "1.5",
              "2.3",
              "15"
            ],
            [
              "3072",
              "6",
              "9",
              "61"
            ],
            [
              "24576",
              "49",
              "73",
              "491"
            ],
            [
              "196608",
              "393",
              "589",
              "3932"
            ]
          ]
        }
      ],
      "images": [
        "processed/images/1805.10041v1_page3_img0.png",
        "processed/images/1805.10041v1_page3_img1.png",
        "processed/images/1805.10041v1_page3_img2.png",
        "processed/images/1805.10041v1_page4_img0.png",
        "processed/images/1805.10041v1_page4_img1.png",
        "processed/images/1805.10041v1_page5_img0.png",
        "processed/images/1805.10041v1_page7_img0.png",
        "processed/images/1805.10041v1_page8_img0.png"
      ],
      "status": "completed",
      "json_file": "processed/compiled/1805.10041v1_compiled.json"
    },
    {
      "metadata": {
        "title": "Performance Analysis of Embarassingly Parallel Application on Cluster   Computer Environment: A Case Study of Virtual Screening with Autodock Vina   1.1 on Hastinapura Cluster",
        "authors": [
          "Muhammad Hilman",
          "Heru Suhartanto",
          "Arry Yanuar"
        ],
        "abstract": "IT based scientific research requires high computational resources. The limitation on funding and infrastructure led the high performance computing era from supercomputer to cluster and grid computing technology. Parallel application running well on cluster computer as well as supercomputer, one of the type is embarrassingly parallel application. Many scientist loves EP because it doesn't need any sophisticated technique but gives amazing performance. This paper discuss the bioinformatics research that used embarrassingly application and show its performance on cluster computer.",
        "published": "",
        "arxiv_id": "1305.3123v1",
        "categories": [
          "cs.DC"
        ],
        "pdf_url": "http://arxiv.org/pdf/1305.3123v1",
        "pdf_file": "data/pdfs/1305.3123v1.pdf",
        "pdf_filename": "1305.3123v1.pdf"
      },
      "processing_info": {
        "processed_at": "2025-11-11T10:33:07.552894",
        "is_large_pdf": false,
        "sections_found": 9,
        "tables_found": 6,
        "images_found": 2
      },
      "sections_text": {
        "Abstract": "high computational resources. The limitaton on funding and infrastructure led the high performance computing era from supercomputer to cluster and grid computing technology. Parallel application running well on cluster computer as well as supercomputer, one of the type is embarassingly parallel application. Many scientist loves EP because it doesn\u2019t need any sophisticated",
        "Methodology": "paper discusses the bionformatics research that used embarassingly parallel application and show its performance on cluster computer. I. I NTRODUCTION HE great invention on information and technology has changed the research paradigm on many field. The researchers used virtual experiment on IT to get more accurate result and to reduce the cost of experiment. However, many problems comes when many scientist try to use the computer modeling to build virtual laboratory. Sophisticated of mathematics model, limitation funding and research infrastructure, and the most annoying problem is limitation of computational resources to get the information result as soon as posible. A. Need of High Performance Computing Science and engineering problems need many computational resources get the relevant Manuscript received October 10, 2010. This work was part of the project on drug design research collaboration between Faculty of Computer Science and Department of Pharmacy, Faculty of Mathematics and Natural Sciences, Universitas Indonesia Muhammad H. Hilman is a master student on computer science at Faculty of Computer Science, Universitas Indonesia, Depok,",
        "16424 Indonesia": "Heru Suhartanto is a professor on parallel computing at Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia. (e-mail: heru@cs.ui.ac.id). Arry Yanuar is a researcher on biomolecular and drug design at Department of Pharmacy, Faculty of Mathematics and Natural Sciences, Universitas Indonesia, Depok, Indonesia. (e-mail: arry.yanuar@gmail.com). information on research. For example the polluted concentration substance which modeled with the Differential Ordinarry Equation System \ud835\udf15\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc61 = \u2212 \ud835\udf15\ud835\udc62\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc65 + \ud835\udf15\ud835\udc63\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc66 + \ud835\udf15\ud835\udc64\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc67 + \ud835\udf15(\ud835\udc3e \ud835\udc65 \ud835\udf15\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc65 \ud835\udf15(\ud835\udc3e \ud835\udc66 \ud835\udf15\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc66 + \ud835\udf15(\ud835\udc3e \ud835\udc67 \ud835\udf15\ud835\udc50 \ud835\udc60 \ud835\udf15\ud835\udc67 + \ud835\udc38 \ud835\udc60 \ud835\udf03, \ud835\udc61 \u2212 \ud835\udc58 1\ud835\udc60 + \ud835\udc58 2\ud835\udc60 \ud835\udc50 \ud835\udc60 \ud835\udf03,\ud835\udc61 + \ud835\udc45 \ud835\udc60 (\ud835\udc50 ,\u2026 .. , \ud835\udc50 \ud835\udc5e ) (1) where s = 1, ....., q; c s (\u03b8,t) is concentration of pollutant s at the space \u03b8; then u(\u03b8,t), v(\u03b8,t), and x(\u03b8,t) is the wind velocity along the x, y, and z axis; E s (\u03b8,t) states the emission on space point \u03b8 and time t for pollutant s; k 1s and k 2s are coefficient of deposition dry and wet; K(\u03b8,t) states difusion coefficient along three coordinate axis; and R s states the chemical reaction related to component s [1,2]. This mathematics model has been researched by Denmark Pollution Laboratory using q = 29. If axis grid used is x = 32, y = 32, and z = 9, it results 267, equations that have to be solved in each integration step along time scale to know the variation each month to get actual information of pollution. This sophisticated equation can only be solved by computers that have very large computational resources . B. Cost and Infrastructure Limitation High computational resource that can solve the sophisticated problems like the differential ordiner equations above can only received supercomputer infrastructure (ex: Cray X-MP, CDC, Illiac-IV) some decades ago. Limitation of integrated circuit (IC) and processor development; and growing research on networks and protocols has been stimulating the new technology of high performance computer. The new one is cluster and grid computing technology . Actually, the concepts aren\u201ft literally new. When IT researcher try to develop high performance computer in the first place, they already thought the possibility of cluster and grid based computer but at that time the protocols and networks security isn\u201ft advance as now Application on Cluster Computer Environment : A Case Study of Virtual Screening with Autodock Vina 1. on Hastinapura Cluster Muhammad H. Hilman , Heru Suhartanto and Arry Yanuar Faculty of Computer Science, Universitas Indonesia",
        "2 Department of Pharmacy, Faculty of Mathematics and Natural Sciences, Universitas Indonesia": "Email: muhammad.hilman@ui.ac.id, heru@cs.ui.ac.id, arry.yanuar@gmail.com on. So they began their research on supercomputer technology that specifically building special computer with special compilers that able to run parallel application to solve any problems that need high performance coputational roseources . The development cost actually got significant aspect developing those technologies. Supercomputer infrastructures are quite expensive to build but cluster and grid technology isn\u201ft that high in comparison. Building cluster and grid computers relatively cheap. It has motivated many researchers not only in the third world country but also in the whole world trying to implement cluster and grid computer to help their scientific computation as part of their research activity. C. Paper Structure In this paper we will discuss the performance on embarrassingly parallel application as one of the parallelization method that run on cluster computer infrastructure called \u201ehastinapura\u201f in Universitas Indonesia. The paper will describe the introduction as well as the background behind research in the section",
        "1. Section 2 will describe the hardware and software": "architecture of our cluster \u201ehastinapura\u201f. The overview of parallel computation will be described in the section as well as the term embarrassingly parallel application. Section will show the result of bioinformatics case that embarrassingly parallel on cluster \u201ehastinapura\u201f. The last section is discussing conclusion our works and many future opportunities to do research on this field. II. H ASTINAPURA C LUSTER C OMPUTER A RCHITECTURE A. Hardware Infrastructure Hastinapura cluster consist of one head node, one grid portal node, one storage node and worker nodes . Head node is the main key of the cluster technology. All of the cluster management is done by middleware that put on this server. The head node specifications are listed below, Table 1. Head node machine spesification Machine Type Sun Fire X2100 Processor AMD Opteron 2. GHz Memory",
        "2 GB": "Hard Disk x GB Operating System GNU/Linux Debian 4. B. Software Infrastructure Hastinapura cluster uses Globus Toolkit as middleware that handle all the software management above the machine. Globus Toolkit can manage different cluster with different machine to one grid system. Above Globus Toolkit, hastinapura uses Sun Grid Engine as a job submission and cluster resources management. MPICH is provided in hastinapura to support the MPI-based parallel application that run on the cluster . Many applications has been deployed hastinapura cluster to support research activities in Universitas Indonesia. Some of the applications are mpiBlast, sequence allignment in bioinformatics that based on MPI; GROMACS, molecular dynamic simulation software; and MPI-POV-Ray, optical modelling for object-ray interaction . III. E MBARASSINGLY P ARALLEL P ARADIGM There are many paradigms to do parallel programming. One of the most favourite paradigm based on its easyness and less sophisticated control is message passing paradigm. Message passing requires the programmer to handle parallel aspect of the codes when doing the programming. The message passing paradigm generally divided into three kind of programs . First, embarassingly parallel paradigm. It has virtually no communication required; easily load balanced; and it has perfect speedup. Second, regular and synchronous. It easily statically load balanced; expect good speedup for nonlocal comunication; and expect reasonable speedupfor non-local communication. Third, irregular and/or asynchronous. It is difficult to load balance; communication overhead usually high; and usually can\u201ft be done efficiently using data parallel programming. A. EP Problems Embarassingly parallel problems has unique criteria that can easily being distinguishad. Many real world problem can be define into EP problems. Here are the EP problems criteria Machine Type Sun Fire X2100 Processor AMD Opteron 2. GHz Memory Hard Disk Machine Type Intel PC Processor Dual Intel Xeon 2. GHz (HT) Memory independently of the others. the final result. any kind of distribution since communication is not a factor. form approach.",
        "5. Expect perfect speedup": "Here are some illustration on how the very simple EP problems modelled. Fig 2. Disconnected computational graph . More advanced EP problems based on master-slave concept of job tasking. Fig 3. EP problems with dynamic process creation and the masterslave approach . B. EP Performance Measurement Performance measurement on EP application is quite simple since no communication overhead involved. The formula for measuring the speedup is \ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc62\ud835\udc5d= \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 \ud835\udc5c\ud835\udc5b \ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc50\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc5c\ud835\udc5f \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 \ud835\udc5c\ud835\udc5b \ud835\udc41 \ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc50\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc60 and the formula for measuring the efficiency of the EP application is \ud835\udc52\ud835\udc53\ud835\udc53\ud835\udc56\ud835\udc50\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66= \ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc62\ud835\udc5d To measure the EP application doesn\u201ft need the function that calculate some overhead involved in the process such as used in Amdahl\u201fs Law . C. EP Cases There are many sample cases that already defined. Here are some sample on EP cases Farms.",
        "5. Random Number Generation": "and many cases on bioinformatics that just easily perform as submitting many jobs with different parameter and different data type. In this paper we will use one of the bioinformatics case for showing the EP. The problems is molecular docking and virtual screening. IV. B IOINFORMATICS C ASE A. Molecular Docking and Virtual Screening Molecular docking is a computational procedure that attempts to predict noncovalent binding of macromolecules. The goal is to predict the bound conformations and the binding affinity . The prediction process is based on information that embedded inside the chemical bond of substance. One of the method to calculate energy that related to binding affinity aspect is AMBER (Assisted Model Building with Energy Refinement) force fields \ud835\udc49 \ud835\udc5f \ud835\udc41 = \ud835\udc58 \ud835\udc4f (\ud835\udc59\u2212\ud835\udc59 ) \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc60 + \ud835\udc58 \ud835\udc4e (\ud835\udf03\u2212\ud835\udf03 ) \ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52\ud835\udc60 \ud835\udc49 \ud835\udc5b [ + \ud835\udc50\ud835\udc5c\ud835\udc60(\ud835\udf02\ud835\udf14\u2212\ud835\udefe)] \ud835\udc61\ud835\udc5c\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 + {\ud835\udf16 \ud835\udc56,\ud835\udc57 \ud835\udf0e \ud835\udc56,\ud835\udc57 \ud835\udc5f \ud835\udc56,\ud835\udc57 \u2212 \ud835\udf0e \ud835\udc56,\ud835\udc57 \ud835\udc5f \ud835\udc56,\ud835\udc57 \ud835\udc56=\ud835\udc57+ \ud835\udc41\u2212 \ud835\udc57= \ud835\udc5e \ud835\udc56 \ud835\udc5e \ud835\udc57 4\ud835\udf0b\ud835\udf16 \ud835\udc5f \ud835\udc56,\ud835\udc57 Eq 2. AMBER force fields formula where \u03a3bonds states the distance aspect that represented by l and l ; \u03a3angles states the angles aspect between chemical bond represented by \u03b8 and \u03b8 ; \u03a3torsions states the active torsion aspect; and \u03a3ij shows the non-covalent chemical bond in the substance . Virtual Screening is molecular docking process that involved large database of chemical compound. We can say that virtual screening is a lot of molecular docking process to get the best molecule to become drug candidate. It\u201fs important to say that this simulation is not the only one method to get the drug candidates, practical laboratory experiment is the one to do that. This virtual experiment used to process large number of substance to produce small number of best drug candidates which cannot be done manually due large number of data. The key to parallelization process in molecular docking lies on this aspect, \u201ebig number of molecular docking jobs\u201f. B. Autodock Vina 1. Autodock Vina is molecular docking software that developed by The Scripps Research Institute, nonprofit biomedical research from San Diego, California, USA. Autodock Vina is the next generation of molecular docking engine after The Scripps Research Institute released Autodock in the first place. Autodock Vina gives transparent user point of view in using molecular docking parameters. User doesn\u201ft have to do many script programming for adjusting the parameters, Autodock Vina has provided simple library that users can only put the desired parameter on docking process. The Scripps Research Institute claimed that Autodock Vina has better performance than Autodock. This claim proved by their experimental performance that published together with the Autodock Vina software news and update in the journal of computational chemistry . The most important from Autodock Vina is the application can run on multicore machine. Autodock Vina has implemented hyper threading concept that run perfectly on a machine that has more than one processor. But the hard point in this paper is not the capability of Autodock Vina running on multicore machine; we will discuss the input problem of virtual screening in Autodock Vina that has very likely type with EP problems. C. Experimental Result Here we will show the result of virtual screening experiment that uses Autodock Vina on hastinapura. The ligand database uses in this experiment was taken from ZINC (ZINC Is Not Commercial) Database in the mol2 format and the receptor was taken from PDB (Protein Data Bank) Database in the pdb format. We process both the input to the pdbqt format and begin the virtual screening process after setting some parameters on both ligands and receptor. We do the experiment on two different system to get comparison how is the performance. First, we experiment on sequential paradigm by running it locally on PC desktop with the specification as listed below Table 5. PC desktop machine spesification PC desktop Machine Spesification Machine Type Intel PC Processor Pentium IV, 2. GHz Memory",
        "80 GB": "Operating System GNU/Linux Ubuntu 8. Second, we run the parallel experiment in the hastinapura cluster using qsub job submission script that used in Sun Grid Engine. The cluster machine specification is discussed in the section before. We use different numbers of data in these two different machines to get the information about speedup and efficiency. The experimental show the result as follows Table 6. Experimental result Data Execution Time (minutes) Serial Parallel 2277. 77. 4629. 159. 8117. 292. 12370. 406. 15294. 509. Fig 4. Experimental result (general) From the table and diagram above we can see that running time is accelerated 29. times. We also can conclude that speed up on this experiment is 29. times because serial experiment is done on machine with one cpu. Beside the running time and speed up, the efficiency can be calculated from information we got. Efficiency of the experiment is 91%, almost linier. We just can see it clearly from two diagrams below about the linierity of the experiment. These three diagrams and one table show clearly that performance of embarassingly parallel application is generally linier and give good result on speed up and efficiency. 2277. 4629. 8117. 12370. 15294. 77. 159. 292. 406. 509. Serial Paralel Fig 5. Experimental result (parallel) Fig 6. Experimental result (serial) Fig 5., and fig 6. Show the linierity of virtual screening experiment that run on the hastinapura cluster computer. V. C ONCLUSION AND F UTURE W ORKS A. Conclusion Scientific research with the spesific problems that can be divide into many jobs which does not have high dependencies in each proces can be define into embarassingly parallel problems. EP problems runs well on cluster computer environment that can be build the substitution supercomputer infrasturcture. EP has became favorite between researchers that need large computational resources. EP gives very nice result on cluster computer environment. It show good speed up and efficiency regarding general performance of application. B. Future Works The open research problem on this area is concerned on the hardware of cluster infrastructure. Simple explanation on EP is the application submitting many jobs concurently, but actually the job submission engine and the machine itself exploited too much on running this application. So, it will become big problem if there is some interruption on job submission or the machine itself. The new software and hardware configuration for cluster computer environment should be designed well to handle that problems. A CKNOWLEDGMENT We would like to thank directorate of research and social service of Universitas Indonesia that already gave financial support on the whole project of biomolecular and drug design computational research using hastinapura cluster computer in Universitas Indonesia. R EFERENCES H. Suhartanto, \u201cParallel Iterated Techniques based on Multistep Runge-Kutta Methods of Radau Types\u201d (Ph.D thesis), University of Queensland, Australia, 1998. Z. Zlatev, R. Berkowicz, Numerical Treatment of Large-scale Air Pollutant Models (Book style) . Comput. Math. Applic., pp. 93\u2013109. 1998. B. Nazief,\u201dRI-Grid: Usulan Pengembangan Infrastruktur komputasi Grid nasional\u201d (Published Conference Proceedings Style), in Proc. E-Indonesia Initiatives , Bandung. May 2006. J. Santoso, G.D. Van Albada, B. Nazief, P.M.A. Sloot, \u201cHierarchical Job Scheduling for Cluster of Workstation\u201d, (Published Conference Proceedings Style) in Proc. The th Annual Conference on the Advanced School for Computing and Imaging , pp. 99-105, ASCI, Delft, Netherland, June 2000. Grid Research Group, \u201cHastinapura\u201d (Online), Faculty of Computer Science, Universitas Indonesia. Available : Grid Research Group, \u201cManual Penggunaan Aplikasi dalam inGrid\u201d (Online), Faculty of Computer Science, Universitas Indonesia. Available aplikasi.pdf P.Coddington, \u201cMessage Passing Programming and MPI\u201d (Lecture Notes), Distributed & High Performance Computing Group, Department of Computer Science, Uniersity of Adelaide. June 2002. B. Wilkinson, M. Allen, Parallel Programming: Techniques and Applications using Networked Workstations and Parallel Computers (Book Style). Prentice Hall. 1999. P.Coddington, \u201cPrograming and Performance Analysis of Parallel Computers\u201d (Lecture Notes), Distributed & High Performance Computing Group, Department of Computer Science, Uniersity of Adelaide. June 2002. O. Trott, A.J. Olson, \u201cAutodock Vina : Improving The Speed and Accuracy of Docking with a New Scoring Function, Efficient, Optimization, and Mutithreading\u201d (Journal Online Sources Style), Journal of Computational Chemistry pp 455. 2010. Cornell WD, Cieplak P, Bayly CI, Gould IR, Merz KM Jr, Ferguson DM, Spellmeyer DC, Fox T, Caldwell JW, Kollman PA. \u201cA Second Generation of Force Fields for The Simulation of Proteins, Nucleic Acids, and Organic Molecules\u201d. J. Am. Chem. Soc . 117: \u2013 5197. May, 2001. 77. 159. 292. 406. 509. Paralel 2277. 4629. 8117. 12370. 15294. Serial"
      },
      "sections_summary": {
        "Abstract": "The transition from supercomputers to cluster and grid computing was driven by limited funding and infrastructure. Cluster computers can run parallel applications that were also suitable for supercomputers, including \"embarrassingly parallel\" ones.",
        "Methodology": "Researchers use virtual experiments with IT to get more accurate results and reduce costs. However, many problems arise when multiple scientists try to use computer modeling for building virtual laboratories due to limitations in mathematical models, funding, research infrastructure, and computational resources. High-performance computing is necessary for scientific engineering problems that require large amounts of computation time.",
        "16424 Indonesia": "Heru Suhartanto is a professor at Universitas Indonesia, researching parallel computing, while Arry Yanuar is a researcher in biomolecular and drug design.\n\nA mathematical model for pollution concentration uses the Differential Ordinary Equation System to simulate pollutant concentrations in space and time. The model has 29 equations that require large computational resources, making it unsolvable by conventional computers.\n\nHigh-performance computing was previously limited by supercomputer infrastructure, but advancements in integrated circuits and processor development have led to cluster and grid computing technology.",
        "2 Department of Pharmacy, Faculty of Mathematics and Natural Sciences, Universitas Indonesia": "Researchers developed supercomputer technology using special compilers to run parallel applications, but building a supercomputer is expensive. In contrast, building cluster and grid computers is relatively cheap. This has motivated researchers worldwide to implement cluster and grid computing for scientific computation, with many implementing it in both third-world countries and the whole world.",
        "1. Section 2 will describe the hardware and software": "Hastinapura cluster consists of one head node, grid portal node, storage node, and worker nodes. The head node is the main hub for cluster technology, with middleware managing all cluster operations. It has specifications listed in Table 1: Sun Fire X2100 processor, AMD Opteron 2. GHz, and memory.",
        "2 GB": "The Hastinapura cluster uses Globus Toolkit, Sun Grid Engine, and MPICH to manage software, job submission, and parallel computing resources for various research applications, including bioinformatics and molecular dynamics simulations. The cluster also employs the message passing paradigm in parallel programming, which is divided into three categories: embarrassingly parallel, regular/synchronous, and irregular/asynchronous paradigms. Embarassingly parallel problems have unique criteria that can be easily distinguished, making them ideal for certain types of real-world problems.",
        "5. Expect perfect speedup": "EP problems can be modeled using a disconnected computational graph and/or the master-slave concept of job tasking with dynamic process creation. Performance measurement is simple as it doesn't involve communication overhead, calculated by speedup (measure of time spent) or efficiency (speedup measure). Examples of EP cases include farms with predefined cases.",
        "5. Random Number Generation": "Here is a concise overall summary:\n\nMolecular docking and virtual screening are computational methods used to predict noncovalent binding of macromolecules. A software tool called Autodock Vina has improved performance compared to its predecessor, offering transparent user control over parameters and support for mult-core machines with hyper-threading technology. This enables efficient parallel processing of large numbers of molecular docking jobs. In a recent virtual screening experiment using Autodock Vina, two systems were compared: one running sequentially on a PC desktop with specific specifications, while the other utilized the software's capabilities to optimize performance.",
        "80 GB": "Here is a concise overall summary:\n\nThis study demonstrates the effectiveness of cluster computing in accelerating execution time by 29 times, achieving 91% efficiency, with performance scaling linearly across varying data sizes and job numbers. The findings support the suitability of embarrassingly parallel applications for cluster environments. A range of sources from 1999 to 2010 were consulted, including research papers and lecture notes on parallel computing techniques, message passing programming, and high-performance computing methodologies, highlighting the growing importance of cluster computing in optimizing computational performance."
      },
      "tables": [
        {
          "page": 2,
          "table_index": 0,
          "content": [
            [
              "",
              "Grid Portal Node Machine Spesification",
              null,
              ""
            ],
            [
              "Machine Type",
              null,
              "Sun Fire X2100",
              null
            ],
            [
              "Processor",
              null,
              "AMD Opteron 2.2 GHz",
              null
            ],
            [
              "Memory",
              null,
              "2 GB",
              null
            ],
            [
              "Operating System",
              null,
              "GNU/Linux Debian 3.1",
              null
            ]
          ]
        },
        {
          "page": 2,
          "table_index": 1,
          "content": [
            [
              "",
              "Grid Portal Node Machine Spesification",
              null,
              ""
            ],
            [
              "Machine Type",
              null,
              "Sun Fire X2100",
              null
            ],
            [
              "Processor",
              null,
              "AMD Opteron 2.2 GHz",
              null
            ],
            [
              "Memory",
              null,
              "1 GB",
              null
            ],
            [
              "Operating System",
              null,
              "GNU/Linux Debian 3.1",
              null
            ]
          ]
        },
        {
          "page": 2,
          "table_index": 2,
          "content": [
            [
              "",
              "Storage Node Machine Spesification",
              null,
              ""
            ],
            [
              "Machine Type",
              null,
              "Intel PC",
              null
            ],
            [
              "Processor",
              null,
              "Dual Intel Xeon 2.8 GHz (HT)",
              null
            ],
            [
              "Memory",
              null,
              "2 GB",
              null
            ],
            [
              "Hard Disk",
              null,
              "3 x 320 GB",
              null
            ],
            [
              "Operating System",
              null,
              "GNU/Linux Debian 4.2",
              null
            ]
          ]
        },
        {
          "page": 2,
          "table_index": 3,
          "content": [
            [
              "",
              "Head Node Machine Spesification",
              null,
              ""
            ],
            [
              "Machine Type",
              null,
              "Sun Fire X2100",
              null
            ],
            [
              "Processor",
              null,
              "AMD Opteron 2.2 GHz",
              null
            ],
            [
              "Memory",
              null,
              "2 GB",
              null
            ],
            [
              "Operating System",
              null,
              "GNU/Linux Debian 3.1",
              null
            ]
          ]
        },
        {
          "page": 4,
          "table_index": 0,
          "content": [
            [
              "",
              "PC desktop Machine Spesification",
              null,
              ""
            ],
            [
              "Machine Type",
              null,
              "Intel PC",
              null
            ],
            [
              "Processor",
              null,
              "Pentium IV, 2.00 GHz",
              null
            ],
            [
              "Memory",
              null,
              "1 GB",
              null
            ],
            [
              "Hard Disk",
              null,
              "80 GB",
              null
            ],
            [
              "Operating System",
              null,
              "GNU/Linux Ubuntu 8.0",
              null
            ]
          ]
        },
        {
          "page": 4,
          "table_index": 1,
          "content": [
            [
              "",
              "Data",
              "",
              "",
              "Execution Time (minutes)",
              null,
              null,
              ""
            ],
            [
              "",
              null,
              null,
              "",
              "Serial",
              "",
              "Parallel",
              ""
            ],
            [
              "1000",
              null,
              null,
              "2277.42",
              null,
              null,
              "77.43",
              null
            ],
            [
              "2000",
              null,
              null,
              "4629.72",
              null,
              null,
              "159.5",
              null
            ],
            [
              "3000",
              null,
              null,
              "8117.6",
              null,
              null,
              "292.27",
              null
            ],
            [
              "4000",
              null,
              null,
              "12370.5",
              null,
              null,
              "406.8",
              null
            ],
            [
              "5000",
              null,
              null,
              "15294.2",
              null,
              null,
              "509.8",
              null
            ]
          ]
        }
      ],
      "images": [
        "processed/images/1305.3123v1_page3_img0.png",
        "processed/images/1305.3123v1_page3_img1.png"
      ],
      "status": "completed",
      "json_file": "processed/compiled/1305.3123v1_compiled.json"
    }
  ],
  "summary": {
    "total_papers": 5,
    "topic": "high performance computing",
    "processing_time": "0:04:04"
  }
}