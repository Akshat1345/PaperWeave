{
  "metadata": {
    "title": "Limitations of performance of Exascale Applications and supercomputers   they are running on",
    "authors": [
      "János Végh"
    ],
    "abstract": "The paper highlights that the cooperation of the components of the computing systems receives even more focus in the coming age of exascale computing. It discovers that inherent performance limitations exist and identifies the major critical contributions of the performance on many-many processor systems. The extended and reinterpreted simple Amdahl model describes the behavior of the existing supercomputers surprisingly well, and explains some mystical happenings around high-performance computing. It is pointed out that using the present technology and paradigm only marginal development of performance is possible, and that the major obstacle towards higher performance applications is the 70-years old computing paradigm itself. A way to step forward is also suggested",
    "published": "",
    "arxiv_id": "1808.05338v1",
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1808.05338v1",
    "pdf_file": "data/pdfs/1808.05338v1.pdf",
    "pdf_filename": "1808.05338v1.pdf"
  },
  "processing_info": {
    "processed_at": "2025-10-28T10:43:08.126095",
    "is_large_pdf": true,
    "sections_found": 27,
    "tables_found": 1,
    "images_found": 0
  },
  "sections_text": {
    "Abstract": "arXiv:1808.05338v1 [cs.DC] Aug Limitations of performance of Exascale Applications and supercomputers they are running on J ´ ANOS V ´ EGH Abstract The paper highlights that the cooperation of the components of the computing systems receives even more focus in the coming age of exascale computing. It discovers that inherent performance limitations exist and identiﬁes the major critical contributions of the performance on many-many processor systems. The extended and reinterpreted simple Amdahl model describes the behavior of the existing supercomputers surprisingly well, and explains some mystical happenings around high-performance computing. It is pointed out that using the present technology and paradigm only marginal development of performance is possible, and that the major obstacle towards higher performance applications is the 70-years old computing paradigm itself. A way to step forward is also suggested. Keywords: processor single-threaded performance, Amdahl’s law, supercomputer application eﬃciency, supercomputer performance limitation, cooperative computing, explicitly many-processor approach, extending the computing paradigm",
    "1. Introduction": "In the age of exascale applications new challenges for the application writers and users appear. The supercomputers –and especially the exascale ones– are strongly custom-made, rather than general purpose computers. The cooperation of the components will be as crucial as it was never before, so the users and the programmers of the exascale applications –as well as the architecture designers– must exactly understand how the strongly parallelized and distributed computing system works. Otherwise minor imprecisions, a poorly organized cooperation or a small amount of poorly organized code can result in catastrophics performance losses. With the expected and wanted advent of exascale computing a lot of application developments has been started practically in all ﬁelds of science, military, industry, services, etc. The increased interest is partly motivated by real economical needs (as proved by the growing number of industry-hosted supercomputers), by the need of elaborating ”big data”, by the increased importance of computer modeling or providing platform for applications utilizing artiﬁcial Preprint submitted to Journal of Parallel and Distributed Computing August 17, intelligence, etc. In contrast with those ”commodity supercomputers”, the ”racing supercomputers” are mainly motivated by the prestige value of the better position on the list of high-performance supercomputers, ”green” supercomputers or total supercomputer capacity. Of course the acquired experiences and developed technologies can be eﬀectively utilized later in the ”commodity supercomputers” as well. The ongoing race between institutions, nations, companies, processors, interconnection and memory access methods, etc. resulted in a kind of ”gold rush”. The computing performance quietly approached its technological bounds, but the ”computing stack” has not been revised, in the past years. It is well known, that computing has its numerous limitations and even those limitations are limited , as well as that the computing growth (like Moore’s observation) is only initially exponential . It is also a common experience on all ﬁelds that when approaching some extremities (big masses, small sizes, big sizes, high speed), the behavior of the studied subject drastically changes. Followers of the high-performance computing ﬁeld faced strange experiences, like aborting projects in the very last phase of their development, utilizing only a fragment of the available nominal performance when nominating to the TOP500 , withdrawn from the competition in a half year, diﬀerent ranking on diﬀerent benchmarks, tragically low eﬃciency of some application on some architecture, strategic allience of manufacturers with conﬂicting interests for producing new processor/accelerator/supercomputer, or calling for project ideas for a supercomputer with unknown architecture and features. All these happenings seem to support the presumption that parallel processing has approached some mystical performance bound. Amdahl called ﬁrst the attention to that the parallel systems built from single processors have serious performance limitations . His warning was used successfully on quite diﬀerent ﬁelds , sometimes misunderstood, abused and even refutated (for a review see ). Despite of that ” Amdahl’s Law is one of the few, fundamental laws of computing ” , for today it is quite forgotten because it is commonly assumed to be valid for software (SW) only, although Amdahl was speaking about complex computing systems. The paper attempts to review some issues important for the coming exascale computing age and is structured as follows. Amdahl’s law is reinterpreted for modern computing systems in section in its original spirit, rather than as formulated by the successors of Amdahl and commonly accepted today. Section explains why in consequence of Amdahl’s Law the present computing technologies have an inherent (although depending on many factors) upper bound for the achievable performance gain. In the light of this, section considers the documented history of supercomputers, and demonstrates that the eﬃciency of parallelization (in other words the achievable parallelization performance gain) governed the supercomputer development. Based on the idea of Amdahl that the diﬀerent non-parallelizable contributions, independently of their origin , act as a single non-parallelizable fraction, section introduces a by intention strongly simpliﬁed model, that assigns some separated fractions to some major contributors, enabling to prepare a semi-technical model. The behavior of the introduced contributions is analyzed in section 6. Section explains what attribute of supercomputers is measured by the diﬀerent benchmarks and why selecting a proper benchmark is important for ranking the supercomputers. As section presents, the model enables to understand what factors aﬀect the eﬃciency of an application on supercomputers having diﬀerent conﬁgurations. The behavior of applications depend heavily on the supercomputer hardware (HW). Based on the behavior of those diﬀerent contributions, section provides some shortterm predictions on the development of supercomputer performance. As it can be concluded from the previous sections, and was guessed by many researchers, one of the major obstacles on the road towards even higher performance is the computing paradigm itself. This issue is discussed in section 10, where also a possible solution is suggested. The discussion is concluded in section with repeating the prophecy of Amdahl: the age of single-processor approach development is over, other than marginal advances will only be possible if using cooperating processors .",
    "2.1. Amdahl’s idea": "A general misconception (introduced by the successors of Amdahl) is to assume that Amdahl’s law is valid for software only and that the non-parallelizable fraction contains something like the ratio of the numbers of the corresponding instructions. Actually, Amdahl’s law is valid for any partly parallelizable activity (including computer unrelated ones) and the non-parallelizable fragment shall be given as the ratio of the time spent with non-parallelizable activity to the total time . Amdahl in his famous paper speaks about ” the fraction of the computational load ” and explicitly mentions, in the same sentence and same rank, algorithmic reasons like ” computations required may be dependent on the states of the variables at each point ”; architectural aspects like ” may be strongly dependent on sweeping through the array along diﬀerent axes on succeeding passes ” as well as ” physical problems ” like ” propagation rates of diﬀerent physical eﬀects may be quite diﬀerent ”. His point of view is valid also today: one has to consider the workload of the complex HW/SW system, rather than some segregated component , and his idea describes parallelization imperfectness of any kind. When applied to a particular case, especially in the case of exascale systems, it shall be scrutinized which contributions can be neglected. Notice that the eligibility of neglecting some component changes with time, technology and conditions.",
    "2.2. Deriving the eﬀective parallelization": "Successors of Amdahl expressed Amdahl’s law with the formula S − = ( − α ) + α/k (1) where k is the number of parallelized code fragments, α is the ratio of the parallelizable part within the total code, S is the measurable speedup. The assumption can be visualized that (assuming many processors) in α fraction of the running time the processors are executing parallelized code, in (1α ) fraction they are waiting (all but one), or making non-payload activity. That is α describes how much, in average, processors are utilized, or how eﬀective (at the level of the computing system) the parallelization is. For a system under test, where α is not a priory known, one can derive from the measurable speedup S an eﬀective parallelization factor () as α eff = k − S − (2) Obviously, this is not more than α expressed in terms of S and k from Equ. (1). For the classical case, α = α eff ; which simply means that in the ideal case the actually measurable eﬀective parallelization achieves the theoretically possible one. In other words, α describes a system the architecture of which is completely known, while α eff characterizes the performance , which describes both the complex architecture and the actual conditions. It was also demonstrated that α eff can be successfully utilized to describe parallelized behavior from SW load balancing through measuring eﬃciacy of the on-chip HW communication to characterize performance of clouds. The value α eff can also be used to refer back to Amdahl’s classical assumption even in the realistic case when the parallelized chunks have diﬀerent lengths and the overhead to organize parallelization is not negligible. The speedup S can be measured and α eff can be utilized to characterize the measurement setup and conditions, how much from the theoretically possible maximum parallelization is realized. Numerically ( − α eff ) equals with the f value, established theoretically . The distinguished constituent in Amdahl’s classic analysis is the parallelizable fraction α , all the rest (including wait time, non-payload activity, etc.) goes into the ”sequential-only” fraction. When using several processors, one of them makes the sequential calculation, the others are waiting (use the same amount of time). So, when calculating the speedup, one calculates S = ( − α ) + α ( − α ) + α/k = k ( − α ) + α (3) hence the eﬃciency is E = S k = k ( − α ) + α (4) This explains the behavior of diagram S k in function of k experienced in practice: the more processors, the lower eﬃciency. At this point one can notice that is a linear function of number of processors, and its slope equals to ( − α ). Equ. (4) also underlines the importance of the single-processor performance: the lower is the number of the processors used in the parallel system having the expected performance, the higher can be the eﬃcacy of the system. Notice also, that through using Equ. (4), the eﬃciency S k can be equally good for describing the eﬃciency of parallelization of a setup, provided that the number of processors is also known. From Equ. (4) α E,k = Ek − E ( k − 1) (5) If the parallelization is well-organized (load balanced, small overhead, right number of processors), α eff is close to unity, so tendencies can be better displayed through using ( − α eff ) in the diagrams below. The importance of this practical term α eff is underlined by that the achievable speedup (performance gain) can easily be derived from Equ. (1) as G = ( − α eff ) (6)",
    "2.3. The original assumptions": "The classic interpretation implies three essential restrictions, but those restrictions are rarely mentioned in the textbooks on parallelization: • the parallelized parts are of equal length in terms of execution time • the housekeeping (controling parallelization, passing parameters, waiting for termination, exchanging messages, etc.) has no cost in terms of execution time • the number of parallelizable chunks coincides with the number of available computing resources Essentially, this is why Amdahl’s law represents a theoretical upper limit for parallelization gain . It is important to notice, however, that a ’universal’ speedup exists only if the parallelization eﬃciency α is independent from the number of the processors . As will be discussed in section 6, this assumption is only valid if the number of processors is low, so the usual linear extrapolation of the actual performance on the nominal performance will not be valid any more in the case of the exascale computing systems.",
    "2.4. The additional factors considered here": "In the spirit of the Single Processor Approach (SPA) the programmer (the person or the compiler) has to organize the job: at some point the initiating processor splits the execution, transmits the necessary parameters to some other processing units, starts their processing, then waits for the termination of started processings; see section 3. Real-life programs show sequential-parallel behavior, with variable degree of parallelization and even apparently massively parallel algorithms change their behavior during processing . All these make Amdahl’s original model non-applicable, and call for extension. As discussed in",
    "1 An additional essential point which was missed by both [9] and [2], that the same com-": "puting model was used in all computers considered . • many parallel computations today are limited by several forms of communication and synchronization • the parallel and sequential runtime components are only slightly aﬀected by cache operations • wires get increasingly slower relative to gates In the followings • the the main focus will be on synchronization and communication; they are kept at their strict absolute minimum; and their eﬀect is scrutinized • the eﬀect of cache will be neglected, and runtime components not discussed separately • the role of the wires is considered in an extended sense: both the importance of physical distance and using special connection methods will be discussed",
    "3. The inherent limit of parallelization": "As it was mentioned in the previous section, initially and ﬁnally only one thread exists, i.e. the minimal absolutely necessary non-parallelizable activity is to fork the other threads and join them again. With the present technology, no such actions can be shorter than one clock period. That is, the non-parallelizable fraction will be given as the ratio of the time of the two clock periods to the total execution time. The latter time is a free parameter in describing the eﬃciency, i.e. the value of the eﬀective parallelization α eff also depends on the total benchmarking time (and so does the achievable parallelization gain, too). This dependence is of course well known for supercomputer scientists: for measuring eﬃciency with better accuracy (and also for producing better α eff values) hours of execution times are used in practice. For example in the case of benchmarking the supercomputer T aihulight 13, seconds benchmark runtime was used; on the 1. GHz processors it means ∗ clock periods. This means that (at such benchmarking time) the inherent limit of ( − α eff ) is − (or equivalently the achievable performance gain is ). In the followings for simplicity 1. GHz processors (i.e. ns clock cycle time) will be assumed. The supercomputers, however, are distributed systems. In a stadium-sized supercomputer the distance between processors (cable length) about m can be assumed. The net signal round trip time is cca. − seconds, or clock periods. The presently available network interfaces have 100.. . ns latency times, and sending a message between processors takes time in the same order of magnitude. This also means that making better interconnection is not a bottleneck in enhancing performance . This statement is underpinned also by statistical considerations . Taking the (maybe optimistic) value ∗ clock periods for the signal propagation time, the value of the eﬀective parallelization ( − α eff ) will be at least in the range of − , only because of the physical size of the supercomputer. This also means that the expectations against the absolute performance of supercomputers are excessive: assuming a Gﬂop/s processor, the achievable absolute nominal performance is * , i.e. EFlops. Because of this, in the name of the company PEZY the last two letters are surely obsolete. It looks like that in the feasibility studies an analyzis for whether this inherent performance bound exists is done neither in USA nor EU. Another major issue arises from the computing principle Single Processor Approach (SPA): only one computer at a time can be addressed by the ﬁrst one. As a consequence, minimum as many clock cycles are to be used for organizing the parallel work as many addressing steps required. Basically, this number equals to the number of cores in the supercomputer, i.e. the addressing in the TOP10 positions typically needs clock cycles in the order of ∗ . . . ; degrading the value of ( − α eff ) into the range − . . . ∗ − . Two tricks may be used to reduce the number of the addressing steps: either the cores are organized into cluster s as many supercomputer builders do, or the processor itself can take over the responsibility of addressing its cores . Depending on the actual construction, the reducing factor can be in the range . . . ∗ , i.e the resulting value of α eff is expected to be in the range of − . . . ∗ − . Notice that utilizing ”cooperative computing” enhances further the value of ( − α eff ), but it means already utilizing a (slightly) diﬀerent computing principle. An operating system must also be used, for protection and convenience. If one considers the context change with its consumed ∗ cycles , the absolute limit is cca. ∗ − , on a zero-sized supercomputer. This value is also very close to the ”danger zone” derived above. This is why T aihulight runs the actual computations in kernel mode . It is crucial to understand that the decreasing eﬃciency (see Equ. (4)) is coming from the computing principle itself rather than from some kind of engineering imperfectness. This inherent limitation is of principial nature and cannot be mitigated without changing the computing principle. For validating these limitations, see also the measured performance data in Fig. 5. Although not explicitly dealt with here, notice that the data exchange between the ﬁrst thread and the other ones also contribute to the non-parallelizable fraction and tipically uses system calls, for details see [10, 18] and section 7.",
    "4. The history of supercomputer development": "As the supercomputer performance is traditionally characterized by the data R Max and R P eak , their eﬃciency can be derived and using Equ. (5) the value of α eff can easily be calculated. Performing that calculation for the data in TOP500 for the ﬁrst supercomputers in the ﬁrst years, the ”parallelization hillside” for the top supercomputers can be derived, see Fig. 1. From https://en.wikipedia.org/wiki/PEZY Computing: The name PEZY is an acronym derived from the greek derived Metric preﬁxs Peta, Eta, Zetta, Yotta − − − − Year Ranking ( − α eff ) Supercomputer hillside Figure 1: The Top500 supercomputer parallelization eﬃciency. The ( − α ) parameter for the past years and the (by R max ) ﬁrst computers. Data derived using the HPL benchmark. the ﬁgure one can conclude that the value of ( − α eff ) decreases with the number of years (as technology develops) and increases with the ranking (the engineering ingenuity). Fig. displays the same data from another point of view. On the vertical axis the performance gain (the single-processor performance is not included, see Equ.(6)) is shown. This diagram is for enhancing parallelization what Moore observation is for enhancing electronic density, i.e. the development of the parallelization of supercomputers is governed by the Amdahl’s Law . Notice that the data are derived from the eﬃciency , so neither single processor performance nor clock frequency are included. It was theoretically concluded in section that the performance gain data are expected to saturate around the value , as displayed in the ﬁgure. The data in the big circle show up a kind of stalling, not experienced in the former years. In other words, the saturation eﬀect is displayed. As was discussed in section 3, the processor of T aihulight deploys cooperative computing (i.e. a slightly diﬀerent computing principle) that keeps its achievable performance gain at a value slightly higher than the predicted maximum. The newly shined up supercomputer Summit , deploying the conventional computYear Performance gain bound st by R Max nd by R Max rd by R Max Best by α eff Figure 2: The trend of the development of computing performance gain in the past years, based on the ﬁrst three (by R Max ) and the ﬁrst (by ( − α )) in the year in question. Data derived using the HPL benchmark. ing principle, shows up the traditionally achievable parallelization gain; it could conquer slot # only thanks to its better single processor performance. The advantage of its processor for supercomputing was also underpinned by statistical considerations .",
    "5.1. The performance losses": "When speaking about computer performance, a modern computer system is assumed, which comprises many sophisticated components (in most cases embedding complete computers), and their complex interplay results in the ﬁnal performance of the system. In the course of eﬀorts to enhance processor performance through using some computing resources in parallel, many ideas have been suggested and implemented, both in SW and HW . All these approaches have diﬀerent usage scenarios, performance and limitations. Because of the complexity of the task and the limited access to the components, empirical methods and strictly controlled special measurement conditions are used to Proc T ime ( not proportional ) Model of parallel execution P P P P P Access Initiation Software P re OS P re T PD Process PD T PD Process PD T PD Process PD T PD Process PD T PD Process PD Just waiting Just waiting OS P ost Software P ost Access T ermination Payload T otal Extended Figure 3: The extended Amdahl’s model (strongly simpliﬁed) quantitize performance . Whether a metric is appropriate for describing parallelism, depends on many factors [9, 21, 22]. As mentioned in section 2, Amdahl listed diﬀerent reasons why losses in the ”computational load” can occur. To understand operation of computing systems working in parallel, one needs to extend Amdahl’s original (rather than that of the successors’) model in such a way, that non-parallelizable (i.e. apparently sequential) part comprises contributions from HW, operating system (OS), SW and Propagation Delay (PD) , and also some access time is needed for reaching the parallelized system. The technical implementations of diﬀerent parallelization methods show up inﬁnite variety, so here a (by intention) strongly simpliﬁed model is presented. Amdahl’s idea enables to put everything that cannot be parallelized into the sequential-only fraction. The model is general enough to discuss qualitatively some examples of parallely working systems, neglecting diﬀerent contributions as possible in the diﬀerent cases. The model can easily be converted to a technical (quantitative) one and the eﬀect of inter-core communcation can also easily be considered.",
    "5.2. The principle of the measurements": "When measuring performance, one faces serious diﬃculties, see for example , chapter 1, both with making measurements and interpreting them. When making a measurement (i.e. running a benchmark) either on a single processor or a system of parallelized processors, an instruction mix is executed many times. The large number of executions averages the rather diﬀerent execution times , with an acceptable standard deviation. In the case when the executed instruction mix is the same, the conditions (like cache and/or memory size, the network bandwidth, Input/Output (I/O) operations, etc) are diﬀerent and they form the subject of the comparison. In the case when comparing diﬀerent algorithms (like results of diﬀerent benchmarks), the instruction mix itself is also diﬀerent. Notice that the so called ”algorithmic eﬀects” – like dealing with sparse data structures (which aﬀect cache behavior) or communication between the parallelly running threads, like returning results repeatedly to the main thread in an iteration (which greatly increases the non-parallelizable fraction in the main thread) – manifest through the HW/SW architecture, and they can hardly be separated. Also notice that there are ﬁxed-size contributions, like utilizing time measurement facilities or calling system services. Since α eff is a relative merit, the absolute measurement time shall be long. When utilizing eﬃciency data from measurements which were dedicated to some other goal, a proper caution must be exercised with the interpretation and accuracy of the data. or shared among them, and also some apparently sequential activities may happen partly parallel with each other.",
    "5.3. The formal introduction of the model": "The extended Amdahl’s model is shown in Fig. 3. The contributions of the model component XXX to α eff will be denoted by α XXX eff in the followings. Notice the diﬀerent nature of those contributions. They have only one common feature: they all consume time . The vertical scale displays the actual activity for processing units shown on the horizontal scale. Notice that our model assumes no interaction between the processes running on the parallelized systems in addition to the absolutely necessary minimum: starting and terminating the otherwise independent processes, which take parameters at the beginning and return results at the end. It can, however, be trivially extended to the more general case when processes must share some resource (like a database, which shall provide diﬀerent records for the diﬀerent processes), either implicitly or explicitly. Concurrent objects have inherent sequentiality , and synchronization and communication among those objects considerably increase the non-parallelizable fraction (i.e. contribution ( − α SW eff )), so in the case of extremely large number of processors special attention must be devoted to their role on the eﬃciency of the application on the parallelized system. Let us notice that all contributions have a role during measurement: contributions due to SW, HW, OS and PD cannot be separated, though dedicated measurements can reveal their role, at least approximately. The relative weights of the diﬀerent contributions are very diﬀerent for the diﬀerent parallelized systems, and even within those cases depend on many speciﬁc factors, so in every single parallelization case a careful analyzis is required.",
    "5.4. Access time": "Initiating and terminating the parallel processing is usually made from within the same computer, except when one can only access the parallelized computer system from another computer (like in the case of clouds). This latter access time is independent from the parallelized system, and one must properly correct for the access time when derives timing data for the parallelized system . Amdahl’s law is valid only for properly selected computing system. This is a one-time, and usually ﬁxed size time contribution.",
    "5.5. Execution time": "The execution time Total covers all processings on the parallelized system. All applications, running on a parallelized system, must make some nonparallelizable activity at least before beginning and after terminating parallelizable activity. This SW activity represents what was assumed by Amdahl as the total sequential fraction . As shown in Fig. 3, the apparent execution time includes the real payload activity, as well as waiting and OS and SW activity.",
    "4 Although some OS activity was surely included, Amdahl assumed some 20 % SW fraction,": "so the other contributions could be neglected compared to SW contribution. Recall that the execution times may be diﬀerent [24, 23, 26] in the individual cases, even if the same processor executes the same instruction, but executing an instruction mix many times results in practically identical execution times, at least at model level. Note that the standard deviation of the execution times appears as a contribution to the non-parallelizable fraction, and in this way increases the ”imperfectness” of the architecture. This feature of processor s deserves serious consideration when utilizing a large number of processors. Over-optimizing a processor for single-thread regime hits back when using it in a parallelized many-processor environment, see also the statistical underpinning in .",
    "6.1. The contribution of the operating system": "All applications must use OS services and some HW facilities to initialize themself as well as to access other processors. Because operating system works in a diﬀerent (supervisor) mode, a considerable amount of time is required for the context switching . The OS initiates only the accessing of the processors, after that HW works partly in parallel with the next action of the OS and with the other actions initiating accessing other processors. This period is denoted in Fig. by T x . After the corresponding signals are generated, they must reach the target processor, that is they need some propagation time. PDs are denoted by PD x and PD x , corresponding to actions delivering input data and result, respectively. This propagation time (which of course occurs in parallel with actions on other processors, but which is a sequential contribution within the thread) depends strongly on how the processors are interconnected: this contribution can be considerable if the distance to travel is large or message transfer takes a long time (like lengthy messages, signal latency, handshaking, store and forward operations in networks, etc.).",
    "6.2. The contribution of the physical size": "Although the signals travel in a computing system with nearly the speed of the light, with increasing the physical size of the computer system a considerable time passes between issuing and receiving a signal, causing the other party to wait, without making any payload job. At the today’s frequencies and chip sizes a signal cannnot even travel in one clock period from one side of the chip to the other, in the case of a stadium-sized supercomputer this delay can be in the order of several hundreds clock cycles. Since the time of Amdahl, the ratio of the computing to the propagation time drastically changed, so –as calls the attention to it– it cannot be neglected any more, although presently it is not (yet) a dominating term.",
    "5 This is usually not a crucial contribution, but under the extremal conditions represented": "by supercomputers, specialized operation systems must be used and every single core must run a lightweight OS − − − − − − − − − R P eak ( Eflop/s ) ( − α HP L eff ) − − − R HP L Max ( Eflop/s ) α SW α OS α eff R Max ( Eflop/s ) − − − − − − − − − R P eak ( Eflop/s ) ( − α HP CG eff − − − R HP CG Max ( Eflop/s ) α SW α OS α eff R Max ( Eflop/s ) Figure 4: Contributions ( − α X eff ) to ( − α total eff ) and max payload performance R Max of a ﬁctive supercomputer ( P = Gflop/s @ GHz ), imitating behavior of benchmarks HPL and HPCG. The ( − α eff ) values refer to the left scale, the R Max values to the right scale",
    "6.3. The contributions critical for large processor numbers": "Notice that the non-payload processing activity comprises two contributions of variable length, so handling them properly is crucial for high number of processors. The ﬁrst one is the OS contribution loop iteration overhead: processors must be handled one-by-one: they receive arguments and return results. This contribution simply increases linearly with the loop count. The second one is the propagation delay overhead PD that increases with the physical size of the supercomputer. The computer components can be in proximity in the range of mm , as well as in the range of m . Similarly, they can be addressed in the ﬁrst iteration or in the last one. These two contributions may be very diﬀerent for diﬀerent processing units, so combining the short ones from the ﬁrst overhead class with long ones from the second overhead class reduces the overall overhead time, see Fig. 3. From the ﬁgure one can ﬁnd out the meaning of the introduced metric α eff : it is simply the ratio of the Payload time (the processors are utilized to do actual work) to the Total execution time on the parallel system, exactly as in the classic Amdahl model. Notice that from the point of view of the processing units there is no diﬀerence why they cannot do Payload work: all (in)activities are considered as contributions to the non-parallelizable fraction. Notice also that using the Extended time in place of the Total time falsiﬁes characteristics of the parameters of the parallelized system: it adds some foreign contribution (time consumed by systems other than the parallelized system) to the correct time.",
    "6.4. How the dominance of contributions changes with the performance": "Fig. may help to understand the role of both the diﬀerent kinds of contributions α X eff and the benchmarks. The ﬁgure displays diagrams for a hypotethic supercomputer , with two parameter sets imitating the behavior of the benchmarks HPL and High Performance Conjugate Gradients (HPCG). For clarity, only the dominant contributions from OS and SW are shown. The supercomputer is the same in both cases, the only essential diﬀerence is in the value of α SW eff (i.e. another benchmark runs on it, and utilizes the HW facilities in a diﬀerent way, see also section 7). As seen, the higher SW contribution from the benchmark causes drastic changes in the dependence of the payload performance on the nominal performance. At low values of processor numbers (low payload performance) the deviation is hardly noticable. The higher SW contribution decreases the achievable maximum R Max , and as the looping delay contribution due to the increasing number of processors exceeds the SW contribution, the R Max diagrams decline. In other words, as the nominal performance approaches the expected dream limit, the resulting α eff starts to raise and turns back the diagram of the actual computing performance R Max . This is a new phenomenon, noticable only at high number of processors. At the time of Amdahl, α was considered as constant (and was dominated by the SW contribution), but in the exascale systems it tends to dominate. The performance gain of supercomputers, see Fig. 2, shows up a behavior very similar to that of the Moore’s law. It looks like that it increases year-byyear by a factor of cca. . (the performance gain , rather than the payload performance). Some reasons were presented, however, why also this behavior is not without limitations. Let us recall, that inside the circle for three years there was no change in the parameters. Even, with the appearance of the new world recorder in T aihulight –which utilizes cooperative computing, a diﬀerent principle– the stall period seems to be prolonged for two more years. Using some reasonable assumptions about the diﬀerent contributions to the non-parallelizable fraction mentioned above, their order of magnitude were estimated, and also some saturation values were predicted in section 3. Noticable that the performance gain in Fig. even shows up a breakdown, while Fig. does not. One can guess, however, that the stalling was caused by this breakdown: adding more processors decreases the actual performance, so no measurements data were published about the measurements with more processor and less performance; see also section 9.",
    "7. Benchmarking supercomputer performance": "As experienced in running the benchmarks HPL and HPCG and explained in connection with Fig. 4, the diﬀerent benchmarks produce diﬀerent payload performance and computational eﬃciency on the same supercomputer. The model presented in Fig. enables to explain the diﬀerence. The benchmarks, utilized to derive numerical parameters for supercomputers, are specialized program s, which run in the HW/OS environment provided",
    "6 The technical model is not accurate enough because the needed parameters are not pro-": "vided, so only a guess for their order of magnitude can be given. by the supercomputer under test. One can use benchmarks for diﬀerent goals. Two typical ﬁelds of utilization: to describe the environment supercomputer application runs in, and to guess how quickly an application will run on a given supercomputer. The (apparently) sequential fraction ( − α eff ), as it is obvious from our model, cannot distinguish between the (at least apparently) sequential processing time contributions of diﬀerent origin, even the SW (including OS) and HW sequential contributions cannot be separated. Similarly, it cannot be taken for sure that those contributions sum up linearly. Diﬀerent benchmarks provide diﬀerent SW contributions to the non-parallelizable fraction of the execution time (resulting in diﬀerent eﬃciencies and ranking ), so comparing results (and especially establishing ranking!) derived using diﬀerent benchmarks shall be done with maximum care. Since the eﬃciency depends heavily on the number of cores, diﬀerent conﬁgurations shall be compared using the same benchmark and the same number of processors (or same R P eak ). If the goal is to characterize the supercomputer’s HW+OS system itself, a benchmark program should distort HW+OS contribution as little as possible, i.e. the SW contribution must be much lower than the HW+OS contribution. In the case of supercomputers, the benchmark HPL is used for this goal since the beginning of the supercomputer age. The mathematical behavior of HPL enables to minimize SW contribution, i.e. HPL delivers the possible best estimation for α HW + OS eff If the goal is to estimate the expectable behavior of an application, the benchmark program should imitate the structure and behavior of the application. In the case of supercomputers, a couple of years ago the benchmark HPCG has been introduced for this goal, since ” HPCG is designed to exercise computational and data access patterns that more closely match a diﬀerent and broad set of important applications, and to give incentive to computer system designers to invest in capabilities that will have impact on the collective performance of these applications ” . However, its utilization can be misleading: the ranking is only valid for the HPCG application, and only utilizing that number of processors. HPCG seems really to give better hints for designing supercomputer applications , than HPL does. According to our model, in the case of using the HPCG benchmark, the SW contribution dominates , i.e. HPCG delivers the best estimation for α SW eff for this class of supercomputer applications. Supercomputer community has extensively tested the eﬃciency of TOP500 supercomputers when benchmarked with HPL and HPCG . It was found that the eﬃciency (and R Max ) is typically orders of magnitude lower when benchmarked with HPCG rather than HPL, even at relatively low number of processors.",
    "8 Returning calculated gradients requires much more sequential communication (unin-": "tended blocking). − − − − − − − − − − − − R P eak (exaFLOPS) R Max ( exaFLOPS ) ∗ − HPL ∗ − ∗ − ∗ − HPCG ∗ − ∗ − Figure 5: The R Max payload performance in function of the peak performance R P eak , at diﬀerent ( − α eff ) values. The ﬁgures display the measured values derived using HPL and HPCG benchmarks, for the TOP15 supercomputers.",
    "8. The eﬃciency of the applications": "As discussed above, the value of ( − α eff ) diﬀers for the two famous benchmarks by more than two orders of magnitude. For the users of exascale applications, it is primarily interesting how their application will run on an exascale supercomputer. Fig. attempts to orient them in this question. The shading shows how nonlinearly the actual performance behaves when the nominal performance approaches the dream limit Eflop/s . The ﬁgure displays how the payload performance depends on the nominal performance, using ( − α eff ) as parameter. The diagram assumes that α eff does not depend on the nominal performance, and the nominal performance is set by changing virtually the number of the cores. For orientation, the best benchmark results (as of July) are shown for the supercomputers in the TOP10 (either by HPL or by HPCG). The empty marks refer to the HPL case, the ﬁlled ones to HPCG. The diamonds denote GPU accelerated supercomputers, the triangles unaccelerated ones. As predicted in section 3, the HPL payload performance data all ﬁt in the ( − α eff ) band − . . . − (near to the ∗ − value) and the HPCG payload peformance data all ﬁt in the α eff band − . . . − (near to the ∗ − value). The corresponding measured values in Fig. should be around the corresponding diagram lines marked by HPL and HPCG, repectively. In the light of the model above, these experiences should be easy to comprehend. The HPL and HPCG benchmarks measure the α HW eff and α SW eff , respectively. The HPCG case is easier to explain. The HPCG measures the (dominant) contribution of the same software, so the measured performance value points are expected to scatter around the same value, provided that the single processor performance is not very diﬀerent. The full marks clearly show the saturation eﬀect, and the points scatter around the corresponding diagram line. The exceptions are being the new champion and its small brother. The reason is the exceptional single-processor performance of their processors. If one corrects for the performance factor (with relative to the single-processor performance of the T aihulight ), the agreement is perfect, see the smaller ﬁlled quares at the corresponding nominal performance values. In the HPL case the marks seem to follow the predicted HPL diagram line. The top supercomputers deploy the same (or quite similar) trick: they reduce the looping delay by organizing the cores into groups. Supercomputers Summit , Sierra and T ianhe − organize single processors into clusters, T aihulight organizes ”clusters” inside the processor. As HPL measures value of α HW + OS eff , this benchmark is sensitive to decreasing the looping delay, the dominant contribution. This is why these supercomputers are on the top of the list. In addition, the higher single-processor performance also raises their value of R Max . The trick they use helps only, however, when the value of α HW + OS eff represents a relatively large contribution to the value of α eff . In the HPL case decreasing α OS eff by two-three orders of magnitude reduces considerably the resulting eﬀective parallelization, so maybe up to an order of magnitude better value of α eff can be measured. In the case of HPCG, however, the contribution of ( − α SW eff ) is about two orders of magnitude larger than the contribution ( − α HW + OS eff so decreasing this latter by orders of magnitude has only marginal eﬀect on ( − α eff ). The computers deploying this ”clusterization” trick show up good results when benchmarked with HPL, but not so good when measured with HPCG. The exceptionally good values are made by that the performance is achieved through using much less processors (due to utilizing accelerator). See also Fig. 4. So it looks like that the payload performance of supercomputers is limited as predicted in section 3. Even for the HPL-class applications, only a few tenths of Eflop/s can be achieved, for the more real-life (HPCG-class) applications achieving a few Pflop/s can be a realistic target. The major diﬀerence between those two classes is the α SW eff contribution, mainly that the iterative nature of HPCG requires intensive data exchange with the anchestor thread, repeating thread forking and joining many times, as well as repeating calculation and communication of the new values for the next iteration many times. All those actions increase the non-parallelizable fraction, i.e. decrease α eff and R Max . Programmers of exascale applications − − − − R P eak (exaFLOPS) R Max ( exaFLOPS ) R Max of Top10 Supercomputers for benchmark HPL Taihulight TianhePiz Daint Gyoukou Titan Sequoia Trinity Cori Oakforest K computer Figure 6: R Max performance of selected TOP10 (as of November) supercomputers in function of their peak performance R P eak , for the HPL benchmark. The actual R Max values are denoted by a bubble. shall reorganize their programs to mitigate this eﬀect as much as possible: a kind of ”SW clusterization” should be invented. Making local data handling and calculations in other than the achestor thread, as well as avoiding not strictly necessary communication should be advantageous.",
    "9. The perspectives of supercomputing": "On the long term the ”gold rush” will of course continue and (partly due to the unusual strategic alliences) new processors, connections, materials, principles will appear and maybe change the game. Some short time predition, however, can be drawn from the tendencies and the presently available data base, again changing virtually the number of processors in the TOP10 supercomputers, hoping that α eff will not increase. From this point of view, the predicted performance values are optimistic: when no drastic changes happen, the shown future performance values will be surely not exceeded. Fig. shows how the performance of the TOP10 supercomputers (as of November 2017) would change with the conditions given above. One can conclude that even the ”benchmark payload performance” will not achieve the dream limit in the coming few years. See also Fig 5. One has to consider Fig. seriously: simply increasing the number of the cores or utilizing accelerators is a short dead-end street. It is worth to reread Amdahl’s classic paper : ” the eﬀort expended on achieving high parallel processing rates is wasted unless it is accompanied by achievements in sequential processing rates of very nearly the same magnitude ”, see also Equ. (4). No single processor was added to the Chineese top supercomputers for years; Gyoukou failed (and withdrawn) because it could only use 12% of its processors; Aurora has been cancelled (and redesigned) because of similar reasons; Summit utilizes only 2.3M cores out of the available 2.8M. The newest (and probably worst) example is AI Bridging Cloud Infrastructure at slot #5. They have outstanding single-processor performance but –because of using the cloud infrastructure– a very poor parallelization ( α eff = . ∗ − ), so they can only use 2.8% of their cores, although they have ”only” 392K cores total.",
    "10. The role of the computing paradigm": "As it could be concluded from the discussion above, one of the major issues in increasing the payload performance of supercomputers is that the looping delay should be decreased. This obstacle originates in the SPA. Since in the paradigm only one processor exists, all components are designed in the spirit of SPA and all supercomputers are build from commodity components. As a consequence, on the bus only one processor can be addressed at a time and the cores cannot directly exchange data between each other. The OS takes over the responsibility of knowing about other processors, but it does it in a rather time-expensive way . The data exchange can take place only through some kind of ”far” memory, causing slowdown ad sharing problems. Reducing the looping delay can be relatively easily solved by ”clustering”: one can organize the cores into ”nodes” and then the main thread shall iterate only though the nodes rather than the cores. In practice, it can be solved through organizing the single (many-core) processors into clusters, or delegating the cluster organization to the many-core processor . Both solutions enabled a supercomputer to conquere the # slot for a while. There are attempts to make direct data transfer through registers of diﬀerent cores, like [31, 32], but the idea of the explicit cooperation (in form of data transfer) in processor chips has been implemented for the ﬁrst time in (just notice that some years later after that Amdahl suggested the idea ). The power of the idea is clearly shown by that the supercomputer T aihulight stayed on the top for two years, when ranked by R HP L Max , and continues to stay when ranked be α HP L eff or G HP L , thanks only to its processor. Among others, the supercomputer experiences also underpin the need for renewing computing . Working out a computing paradigm which is drastically diﬀerent from the traditional one and at the same time it is upward compatible with it, is not simple but possible . Its simulator proves the feasibility and viability of the concept. Utilizing that (or some similar) concept the limitations stemming out from the computing paradigm can be circumvented, similarly to some other limitations of computing . In that way in the farther future the ”dream limit” can be exceeded and exascale applications with reasonable eﬃciency can be prepared.",
    "11. Summary": "The paper discussed the exascale applications and exascale supercomputers as a complex HW/SW system. It has pointed out that such systems have inherent performance limitations, and through understanding the reasons of those limitations, they can be mitigated. The introduced naive model (based on extending Amdahl’s principle) describes surprisingly well the performance values measured on the recently announced supercomputers and explains some misteries experienced around supercomputers approaching the exaFLOPS dream limit. The model also explains the role of benchmarking and provides some practical hints for writing eﬃcient applications for the future exascale computers. Based on the rigorously veriﬁed database of supercomputer performance data, is is shown that with the present technologies (and computing principle) the dream limit cannot be achieved. The need for ”rebooting computing” is underlined by the analyzis and also a possible way out of the present stalling through extending the computing paradigm is shown. Acknowledgements Project no. has been implemented with the support provided from the National Research, Development and Innovation Fund of Hungary, ﬁnanced under the K funding scheme. References References I. Markov, Limits on fundamental limits to computation, Nature 512(7513) (2014) 147–154. P. J. Denning, T. Lewis, Exponential Laws of Computing Growth, Communications of the ACM (2017) 54– doi:DOI:10.1145/ . TOP500, November list supercomputers, G. M. Amdahl, Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities, in: AFIPS Conference Proceedings, Vol. 30, 1967, pp. 483–485. doi:10.1145/1465482. . S. Krishnaprasad, Uses and Abuses of Amdahl’s Law, J. Comput. Sci. Coll. (2) (2001) 288–293. URL http://dl.acm.org/citation.cfm?id=775339. F. D´evai, The Refutation of Amdahl’s Law and Its Variants, in: O. Gervasi, B. Murgante, S. Misra, G. Borruso, C. M. Torre, A. M. A. Rocha, D. Taniar, B. O. Apduhan, E. Stankova, A. Cuzzocrea (Eds.), Computational Science and Its Applications – ICCSA 2017, Springer International Publishing, Cham, 2017, pp. 480–493. J. M. Paul, B. H. Meyer, Amdahl’s Law Revisited for Single Chip Systems, International Journal of Parallel Programming (2) (2007) 101–123. URL https://doi.org/10.1007/s10766-006-0028J. V´egh, P. Moln´ar, How to measure perfectness of parallelization in hardware/software systems, in: 18th Internat. Carpathian Control Conf. ICCC, 2017, pp. 394–399. A. H. Karp, H. P. Flatt, Measuring Parallel Processor Performance, Commun. ACM (5) (1990) 539–543. doi:10.1145/78607. . URL http://doi.acm.org/10.1145/78607. L. Yavits, A. Morad, R. Ginosar, The eﬀect of communication and synchronization on Amdahl’s law in multicore systems, Parallel Computing (1) (2014) 1–16. K. Pingali, D. Nguyen, M. Kulkarni, M. Burtscher, M. A. Hassaan, R. Kaleem, T.-H. Lee, A. Lenharth, R. Manevich, M. M´endez-Lojo, D. Prountzos, X. Sui, The Tao of Parallelism in Algorithms, SIGPLAN Not. (6) (2011) 12–25. doi:10.1145/1993316. . URL http://doi.acm.org/10.1145/1993316. J. Dongarra, Report on the Sunway TaihuLight System, Tech. Rep. Tech Report UT-EECS-16-742, University of Tennessee Department of Electrical Engineering and Computer Science (June 2016). J. V´egh, Statistical considerations on limitations of supercomputers, CoRR abs/1710.08951. arXiv:1710. . URL http://arxiv.org/abs/1710. US DOE, The Opportunities and Challenges of Exascale Computing, (2010). European Commission, Implementation the Action Plan for the European High-Performance Computing strategy, id= (2016). F. Zheng, H.-L. Li, Lv, Guo, X.-H. Xu, X.-H. Xie, Cooperative computing techniques for a deeply fused and heterogeneous many-core processor architectur Journal of Computer Science and Technology (1) (2015) 145–162. URL https://doi.org/10.1007/s11390-015-1510D. Tsafrir, The context-switch overhead inﬂicted by hardware interrupts (and the enigma of do-nothing lo in: Proceedings of the Workshop on Experimental Computer Science, ExpCS ’07, ACM, New York, NY, USA, 2007, pp. 3–3. URL http://doi.acm.org/10.1145/1281700. S. Eyerman, L. Eeckhout, Modeling Critical Sections in Amdahl’s Law and Its Implications for Multicore SIGARCH Comput. Archit. News (3) (2010) 362–370. URL http://doi.acm.org/10.1145/1816038. K. Hwang, N. Jotwani, Advanced Computer Architecture: Parallelism, Scalability, Programmability, 3rd Edition, Mc Graw Hill, 2016. D. J. Lilja, Measuring Computer Performance: A practitioner’s guide, Cambridge University Press, 2004. X.-H. Sun, J. L. Gustafson, Paper: Toward a better parallel performance metric, Parallel Comput. (10-11) (1991) 1093–1109. URL http://dx.doi.org/10.1016/S0167-8191(05)80028S. Orii, Metrics for evaluation of parallel eﬃciency toward highly parallel processing, Parallel Computing (1) (2010) 25. URL http://www.sciencedirect.com/science/article/pii/S0167819109001227 D. Patterson, J. Hennessy (Eds.), Computer Organization and design. RISC-V Edition, Morgan Kaufmann, 2017. P. Moln´ar, J. V´egh, Measuring Performance of Processor Instructions and Operating System Services in Soft Processor Based Systems, in: 18th Internat. Carpathian Control Conf. ICCC, 2017, pp. 381–387. F. Ellen, Hendler, Shavit, the Inherent Sequentiality Concurrent Objects, SIAM Comput. (3) (2012) 519536. R. E. Bryant, D. R. O’Hallaron, Computer Systems: A Programmer’s Perspective, Pearson, 2014. IEEE Spectrum, Two Diﬀerent Top500 Supercomputing Benchmarks Show Two Diﬀerent Top Supercomputers, (2017). HPCG Benchmark, Hpcg benchmark, http://www.hpcg-benchmark.org/ (2016). T. Dettmers, The Brain Deep Learning Part Computational Complexity Why the Singularity Nowhere Near, (2015). J. Dongarra, The Global Race for Exascale High Performance Computing, J. Congy, et al, Accelerating Sequential Applications on CMPs Using Core Spilling, Parallel and Distributed Systems (2007) 1094–1107. ARM, big.LITTLE technology (2011). URL https://developer.arm.com/technologies/big-little J. V´egh, Renewing computing paradigms for more eﬃcient parallelization of single-threads, Vol. of Advances in Parallel Computing, IOS Press, 2018, Ch. 13, pp. 305–330. J. V´egh, Introducing the explicitly many-processor approach, Parallel Computing (2018) 40. J. V´egh, EMPAthY86: A cycle accurate simulator for Explicitly Many-Processor Approach (EMPA) comp URL https://github.com/jvegh/EMPAthY86"
  },
  "sections_summary": {
    "Abstract": "[Skipped - Large PDF]",
    "1. Introduction": "[Skipped - Large PDF]",
    "2.1. Amdahl’s idea": "[Skipped - Large PDF]",
    "2.2. Deriving the eﬀective parallelization": "[Skipped - Large PDF]",
    "2.3. The original assumptions": "[Skipped - Large PDF]",
    "2.4. The additional factors considered here": "[Skipped - Large PDF]",
    "1 An additional essential point which was missed by both [9] and [2], that the same com-": "[Skipped - Large PDF]",
    "3. The inherent limit of parallelization": "[Skipped - Large PDF]",
    "4. The history of supercomputer development": "[Skipped - Large PDF]",
    "5.1. The performance losses": "[Skipped - Large PDF]",
    "5.2. The principle of the measurements": "[Skipped - Large PDF]",
    "5.3. The formal introduction of the model": "[Skipped - Large PDF]",
    "5.4. Access time": "[Skipped - Large PDF]",
    "5.5. Execution time": "[Skipped - Large PDF]",
    "4 Although some OS activity was surely included, Amdahl assumed some 20 % SW fraction,": "[Skipped - Large PDF]",
    "6.1. The contribution of the operating system": "[Skipped - Large PDF]",
    "6.2. The contribution of the physical size": "[Skipped - Large PDF]",
    "5 This is usually not a crucial contribution, but under the extremal conditions represented": "[Skipped - Large PDF]",
    "6.3. The contributions critical for large processor numbers": "[Skipped - Large PDF]",
    "6.4. How the dominance of contributions changes with the performance": "[Skipped - Large PDF]",
    "7. Benchmarking supercomputer performance": "[Skipped - Large PDF]",
    "6 The technical model is not accurate enough because the needed parameters are not pro-": "[Skipped - Large PDF]",
    "8 Returning calculated gradients requires much more sequential communication (unin-": "[Skipped - Large PDF]",
    "8. The eﬃciency of the applications": "[Skipped - Large PDF]",
    "9. The perspectives of supercomputing": "[Skipped - Large PDF]",
    "10. The role of the computing paradigm": "[Skipped - Large PDF]",
    "11. Summary": "[Skipped - Large PDF]"
  },
  "tables": [
    {
      "page": 14,
      "table_index": 0,
      "content": [
        [
          ""
        ],
        [
          "αSW\nαOS"
        ]
      ]
    }
  ],
  "images": [],
  "status": "completed"
}