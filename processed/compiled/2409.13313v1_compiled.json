{
  "metadata": {
    "title": "Performance Enhancement of the Ozaki Scheme on Integer Matrix   Multiplication Unit",
    "authors": [
      "Yuki Uchino",
      "Katsuhisa Ozaki",
      "Toshiyuki Imamura"
    ],
    "abstract": "This study was aimed at simultaneously achieving sufficient accuracy and high performance for general matrix multiplications. Recent architectures, such as NVIDIA GPUs, feature high-performance units designed for low-precision matrix multiplications in machine learning models, and next-generation architectures are expected to follow the same design principle. The key to achieving superior performance is to fully leverage such architectures. The Ozaki scheme, a highly accurate matrix multiplication algorithm using error-free transformations, enables higher-precision matrix multiplication to be performed through multiple lower-precision matrix multiplications and higher-precision matrix additions. Ootomo et al. implemented the Ozaki scheme on high-performance matrix multiplication units with the aim of achieving both sufficient accuracy and high performance. This paper proposes alternative approaches to improving performance by reducing the numbers of lower-precision matrix multiplications and higher-precision matrix additions. Numerical experiments demonstrate the accuracy of the results and conduct performance benchmarks of the proposed approaches. These approaches are expected to yield more efficient results in next-generation architectures.",
    "published": "",
    "arxiv_id": "2409.13313v1",
    "categories": [
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/2409.13313v1",
    "pdf_file": "data/pdfs/2409.13313v1.pdf",
    "pdf_filename": "2409.13313v1.pdf"
  },
  "processing_info": {
    "processed_at": "2025-10-28T10:40:08.263789",
    "is_large_pdf": false,
    "sections_found": 13,
    "tables_found": 0,
    "images_found": 16
  },
  "sections_text": {
    "Performance Enhancement": "of the Ozaki Scheme on Integer Matrix Multiplication Unit Journal Title XX(X): – ©The Author(s) Reprints and permission: sagepub.co.uk/journalsPermissions.nav SAGE Yuki Uchino , Katsuhisa Ozaki , and Toshiyuki Imamura",
    "Abstract": "This study was aimed at simultaneously achieving sufficient accuracy and high performance for general matrix multiplications. Recent architectures, such as NVIDIA GPUs, feature high-performance units designed for low-precision matrix multiplications in machine learning models, and next-generation architectures are expected to follow the same design principle. The key to achieving superior performance is to fully leverage such architectures. The Ozaki scheme, a highly accurate matrix multiplication algorithm using error-free transformations, enables higher-precision matrix multiplication to be performed through multiple lower-precision matrix multiplications and higher-precision matrix additions. Ootomo et al. implemented the Ozaki scheme on high-performance matrix multiplication units with the aim of achieving both sufficient accuracy and high performance. This paper proposes alternative approaches to improving performance by reducing the numbers of lower-precision matrix multiplications and higher-precision matrix additions. Numerical experiments demonstrate the accuracy of the results and conduct performance benchmarks of the proposed approaches. These approaches are expected to yield more efficient results in next-generation architectures. Keywords matrix multiplication, fixed-point arithmetic, floating-point arithmetic, Tensor Cores, error-free transformation",
    "Introduction": "The field of machine learning, including AI, is evolving daily, and the scale and complexity of machine learning models are continually increasing. Recent architectures are designed to process these models rapidly and with high energy efficiency. For machine learning models, matrix multiplications using low-precision floating-point systems and integers are essential. Therefore, recent architectures are equipped with high-performance low-precision floatingpoint and integer matrix multiplication units, and highperformance mixed-precision matrix multiplication units that leverage these capabilities. A prime example of this is the NVIDIA Tensor Cores (see NVIDIA Corporation ( )). Table shows the specifications of the NVIDIA GPUs equipped with Tensor Core technology. Note that the specification of FP16 TC on RTX ( TFLOPS) is for FP16 input and FP32 output, and the specifications for the H100 and H200 are the same as those for the GH200. In the future, numerical computation algorithms that leverage the performance of cutting-edge architectures will be essential. This study focused on researching high-performance matrix multiplication algorithms that maximize the potential of the latest architectures. A highly accurate matrix multiplication scheme via the error-free transformation of matrix products was proposed in Ozaki et al. ( ). The scheme is called the Ozaki scheme and it converts a matrix product into a sum of multiple matrix products. It is also possible to convert a matrix product into a sum of lower-precision matrix products to take full advantage of the immense computational power of recent architectures. In order to compute D ← AB for higherprecision matrices A ∈ R m × n and B ∈ R n × p , the Ozaki Table 1. Specifications in TFLOPS/TOPS of NVIDIA GPUs for dense data NVIDIA Corporation ( ) RTX A100 GH200 B200 GB200 FP64 1. 9. FP64 TC 19. FP32 82. 19. TF32 TC 82. BF16 TC FP16 TC INT8 TC FP8 FP6 INT4 TC FP4 scheme using lower/mixed-precision, provided in Mukunoki et al. ( ), is constructed as the following four steps: (i) Extract the lower-precision matrices A , A , . . . , A k from the higher-precision matrix A , where k is specified by the user, shifting A i to prevent overflow and underflow at lower-precision arithmetic.",
    "2 Department of Mathematical Sciences, Shibaura Institute of Technol-": "ogy, Japan Corresponding author: Yuki Uchino, 7-1Minatojima-minami-machi, Chuo-ku, Kobe, Hyogo 650-0047, Japan Email: yuki.uchino.fe@riken.jp Prepared using sagej.cls [Version: 2017/01/ v1.20] arXiv:2409.13313v1 [cs.DC] Sep Journal Title XX(X) (ii) Extract k lower-precision matrices B i from B in the similar way as A i . (iii) Compute A i B j for i + j < = k + using lower/mixed-precision arithmetic. (iv) Reverse (shift) A i B j and accumulate the results into D using higher-precision arithmetic. When emulating the GEMM routine, the following fifth step is also performed: (v) Compute C ← αD + βC for scalars α, β and a matrix C . Ootomo et al. implemented the Ozaki scheme using the INT8 Tensor Cores and evaluated the performance for emulating DGEMM, an FP64 matrix multiplication routine, as reported in Ootomo et al. ( ). The Ozaki scheme implemented by Ootomo is named the “Ozaki Scheme on Integer Matrix Multiplication Unit” (ozIMMU), and the code is available in Ootomo ( ). Figure shows the accuracy of ozIMMU. Herein, ϕ specifies the tendency of the difficulty in terms of the accuracy of matrix multiplications. At least or slices are required to obtain sufficiently accurate results, even for well-conditioned matrix multiplications. As ϕ increases, more slices are required to obtain sufficient accuracy. Ootomo’s implementation for splitting matrices offers bit masking. Thus, the extracted matrices A i , B i may not be optimal and the splitting method can be improved (see Section 3. for details). Improvement of the splitting method should contribute to improving the accuracy of results. Figure 1. Accuracy of ozIMMU. Matrix A has entries a ij := ( U ij − . 5) · exp( ϕ · N ij ) , where U ij ∈ ( , 1) are uniformly distributed and N ij are drawn from standard normal distribution for ≤ i, j ≤ m and m = n = p . Matrix B is composed similarly. Figures and show the time breakdown of ozIMMU for double-precision matrices A, B , defined in IEEE Computer Society ( ), on GeForce RTX and GH200 Grace Hopper Superchip, respectively. Note that “split A”, “split B”, “Gemm in INT8-TC”, “accumulation in FP64”, and “copy” correspond to the steps (i) , (ii) , (iii) , (iv) , and (v) in the Ozaki scheme for emulating the GEMM routine, respectively. It can be seen that the computing time for the accumulation of matrix products in FP64 is not negligible even though the computation cost is O ( mp ) operations. This is because the performance of the INT8 Tensor Core is nearly times and times higher than that of FP64 on the RTX and GH200, respectively. Because this ratio increases significantly on the B200 and future architectures, accelerating the accumulation process is critical for the Ozaki scheme. Figure 2. Time breakdown of ozIMMU on NVIDIA GeForce RTX Figure 3. Time breakdown of ozIMMU on NVIDIA GH200 Grace Hopper Superchip In this paper, we propose new implementation methods for accelerating the accumulation of the Ozaki scheme using the INT8 Tensor Core. In addition, an alternative splitting method is also applied to improve the accuracy of results. For the number of slices, the splitting method contributes to obtaining more accurate results than those of Ootomo’s ozIMMU. The remainder of this paper is organized as follows: Section overviews previous studies about the Ozaki scheme using an integer matrix multiplication unit; Section presents the proposed method for accelerating the accumulation and optimizing the splitting method; Section shows numerical results to illustrate the efficiency of the proposed methods; Section provides a rounding error analysis of the Ozaki scheme with the proposed methods; and Section presents final remarks. Previous study In this section, we briefly summarize previous studies. Let u be the relative error unit, e.g., u = − for double-precision floating-point numbers defined in IEEE 754. Define F as a set of binary floating-point numbers with u . Let O m,n be an m × n zero matrix. Suppose that A ∈ F m × n and B ∈ F n × p . Ozaki et al. proposed an error-free transformation of a matrix product AB in Ozaki et al. ( ). For a user-specified constant k ∈ N , the Ozaki scheme transforms each of A and B into k + matrices A =: A + A + · · · + A k + V k , B =: B + B + · · · + B k + W k , where A i ∈ F m × n , B i ∈ F n × p , V i := A − j = A j , and W i := B − j = B j , (1) such that V i ∈ F m × n and W i ∈ F n × p for i ≤ k , and | ( A s ) ij | ≥| ( A s + ) ij | holds ( A s ) ij ̸ = for s = , . . . , k − , and B s satisfies the corresponding relation. If the technique provided in Minamihata et al. Prepared using sagej.cls Uchino, Ozaki, and Imamura ( ) is used, then | ( A s ) ij | > | ( A s + ) ij | holds when ( A s ) ij ̸ = . Let F N be a set of N -bit binary floating-point numbers. For the Ozaki scheme using the FP16 Tensor Core with accumulation in FP32 and outputs in FP32, the mixedprecision splitting method with shifting to prevent overflow and underflow was utilized in Mukunoki et al. ( ). For A ∈ F m × n and B ∈ F n × p , the splitting method produces A =: diag( µ ′ (1) ) A ′ + · · · + diag( µ ′ ( k ) ) A ′ k + V k , (2) B =: B ′ diag( ν ′ (1) ) + · · · + B ′ k diag( ν ′ ( k ) ) + W k (3) with double-precision shift values µ ′ ( i ) ∈ F m , ν ′ ( i ) ∈ F p and half-precision matrices A ′ i ∈ F m × n , B ′ i ∈ F n × p for i = , . . . , k . Algorithm represents the splitting method for obtaining ( ) using Minamihata’s technique. Equation ( ) can be obtained by transposing the results of Algorithm executed for B T . Algorithm can be described as a loop that is executed until s = k for simplicity; however, the loop terminates when A = O m,n in practice. Note that the binary logarithm is used at the rd line in Algorithm ; however, the calculation using the binary logarithm occasionally returns erroneous results. Therefore, it is better to use a calculation method without the binary logarithm, such as a bitwise operation or a technique leveraging rounding error in floating-point arithmetic as follows: µ ′ ( s ) ← u − α i + ( − u − ) α i , where α i := max j | a ij | . This technique was developed by Rump and provided in Ozaki et al. ( ). Algorithm Mixed-precision splitting method from Mukunoki et al. ( ) for Ozaki scheme between FP64 and FP16 using floating-point arithmetic in roundto-nearest-even mode with Minamihata’s technique from Minamihata et al. ( ) Input: A ∈ F m × n , k ∈ N",
    "Output: A ′": "s ∈ I m × n , µ ′ ( s ) ∈ F m , s = , . . . , k 1: β ← min( , ⌊ ( − log n ) / ⌋ ) 2: for s = , . . . , k do µ ′ ( s ) ← ⌈ log max j | a ij |⌉ · − β for i = , . . . , m σ i = . · · µ ′ ( s ) for i = , . . . , m ( A s ) ij = ( a ij + σ i ) − σ i for i = , . . . , m { Extract in round-to-nearest-even mode } ( A ′ s ) ij ← INT8(( µ ′ ( s ) ) − · ( A s ) ij ) ∀ i, j { Convert to INT8 } a ij ← a ij − ( A s ) ij ∀ i, j 8: end for 3. Group-wise error-free accumulation Next, we propose a method for accelerating the accumulation in FP64 for ozIMMU. Algorithm requires the computation of a sum of k ( k + 1) / FP64 matrices at the th line. The accumulation accounts for a large ratio of the total computation time of ozIMMU as shown in Figures and . For this challenge, we propose a method for accelerating ozIMMU by reducing the number of additions in FP64. Define G g ⊂ R for g = , . . . , k + as G g := { ( i, j ) | i + j = g } . Recall that Algorithm performs C ← k + t = g − s = diag( µ ′′ ) − βs + − β ( g − s )+ A ′′ s B ′′ g − s diag( ν ′′ ) . (10) Let ( i , j ) , . . . , ( i g − , j g − ) ∈ G g such that i s ̸ = i t and j s ̸ = j t implies s ̸ = t for ≤ s, t ≤ g − , where g ∈ { , . . . , k + } . Then, the inner sum of ( ) can be expressed as follows: g − s = diag( µ ′′ ) − βi s + − βj s + A ′′ i s B ′′ j s diag( ν ′′ ) = − βg + diag( µ ′′ ) g − s = A ′′ i s B ′′ j s diag( ν ′′ ) (11) which uses that i s + j s = g for all s = , . . . , g − . If no overflow occurs in A ′′ i s B ′′ j s + A ′′ i t B ′′ j t ( ≤ s, t ≤ g − ) in INT32, the summation can be performed on the accumulator of GEMM on the INT8 Tensor Core. Therefore, applying the above transformation reduces the numbers of slow conversions from INT32 to FP64, scalings, and slow summations in FP64. Let r ∈ N be r := max( , − β −⌈ log n ⌉ ) . (12) Then, the summation min( r,g − 1) s = A ′′ i s B ′′ j s can be computed without error using GEMM on the INT8 Tensor Core. The summation of r instances of A ′′ i s B ′′ j s can be computed without error; however, the summation of all | G g | instances of A ′′ i s B ′′ j s cannot always be computed without error. The proof validating error-free summation is provided in Section . Finally, we present Algorithm , which has a reduced number of summands in FP64 accumulation. Assuming r ≥ k for simplify, all A ′′ i B ′′ j for ( i, j ) ∈ G g can be accumulated without overflow for g = , . . . , k + . Algorithm shows the Ozaki scheme with groupwise errorfree accumulation for r ≥ k . 3. Combination of proposals Next, we accelerate ozIMMU by reducing the number of summands in FP64 accumulation and improving the splitting method. For this purpose, we combine the methods proposed in Sections 3. and 3. . In order to use group-wise errorfree accumulation as in Algorithm , A and B need to be expressed as in ( ) and ( ). Thus, we determine µ ′′ i ← ⌈ log max j | a ij |⌉ · − β , σ i ← . · − β ( s − 1) · µ ′′ as the rd and th lines in Algorithm . Then, A s are extracted using rounding to nearest floating-point arithmetic ( A s ) ij ← ( a ij + β ( s − 1) σ i ) − β ( s − 1) σ i for the constant β ( s − 1) σ i with a common ratio of β . Finally, we obtain Algorithm . The shift values for A s Prepared using sagej.cls Journal Title XX(X)",
    "Methodology": "splitting for Ozaki scheme between FP64 and INT8 using floating-point arithmetic in round-to-nearest-even mode with Minamihata’s technique from Minamihata et al. ( ). The results can be expressed as in ( ) for error-free group-wise accumulation. Input: A ∈ F m × n , k ∈ N",
    "Input: A ′": "s ∈ F m × n , µ ′ ( s ) ∈ F m , B ′ s ∈ F n × p , ν ′ ( s ) ∈ F n , s = , . . . , k Output: C ∈ F m × p 1: C ← O m,p { C ′ ∈ F m × p 2: for g = , . . . , k + do for s = , . . . , g − do C ′ ← A ′ s B ′ g − s { Compute using GEMM on FP16 Tensor Core } C ← C + diag( µ ′ ( s ) )FP64( C ′ )diag( ν ′ ( t ) ) { Compute in FP64 } end for 7: end for Let I N be a set of N -bit signed integers. It holds that − N − ≤ i ≤ N − − for all i ∈ I N . Remember that no error occurs in integer arithmetic, barring overflow. For the Ozaki scheme using the INT8 Tensor Core with accumulation in INT32, the mixed-precision splitting method via bit masking with shifting is offered in Ootomo et al. ( ) and provided in Ootomo ( ). Algorithm shows the splitting method. Let β := min , \u0016 − log n (4) Assume that β ≥ , i.e., n ≤ . That splitting method produces A =: diag( µ ′′ )( − β + A ′′ + · · · + − kβ + A ′′ k ) + V k , (5) B =: ( − β + B ′′ + · · · + − kβ + B ′′ k )diag( ν ′′ ) + W k (6) Prepared using sagej.cls Journal Title XX(X)",
    "Output: A ′′": "s ∈ I m × n , µ ′′ ∈ F m , s = , . . . , k 1: β ← min( , ⌊ ( − log n ) / ⌋ ) 2: µ ′′ i ← ⌈ log max j | a ij |⌉ · − β for i = , . . . , m 3: for s = , . . . , k do σ i ← . · − β ( s − 1) · µ ′′ i for j = , . . . , m ( A s ) ij ← ( a ij + σ i ) − σ i ∀ i, j { Extract in round-to-nearest-even mode } ( A ′′ s ) ij ← INT8(( µ ′′ ( s ) ) − · − β ( s − 1) · ( A s ) ij ) ∀ i, j { Convert to INT8 } a ij ← a ij − ( A s ) ij ∀ i, j 8: end for GeForce RTX with the GNU C++ Compiler 11.4. and NVIDIA CUDA Toolkit 12.5.82. The tested methods will be denoted as follows: • ozIMMUk : Ootomo’s implementation with k slices • ozIMMU RNk : Proposed method in Section 3. with k slices • ozIMMU EFk : Proposed method in Section 3. with k slices • ozIMMU Hk : Proposed method in Section 3. with k slices 4. Accuracy Figure shows the accuracies ozIMMUk , ozIMMU RNk , ozIMMU EFk , and ozIMMU Hk for k = , . . . , . set A ∈ F n × n a ij := ( U ij − . 5) · exp( ϕ · N ij ) , where U ij ∈ ( , 1) are uniformly distributed random numbers and N ij are drawn from the standard normal distribution, for ≤ i, j ≤ n . The constant ϕ specifies the tendency of the difficulty in terms of matrix multiplication accuracy. A matrix B ∈ F n × n is set similarly to A . Figure 5. Comparison of accuracy between ozIMMU and proposed methods. Matrix A has entries a ij := ( U ij − . 5) · exp( ϕ · N ij ) , where U ij ∈ ( , 1) are uniformly distributed random numbers and N ij are drawn from the standard normal distribution, for ≤ i, j ≤ m and m = n = p . Matrix B has a similar composition. Prepared using sagej.cls Uchino, Ozaki, and Imamura The accuracy deteriorated for n > because the maximum number of significant bits β = min( , ⌊ ( − log n ) / ⌋ ) of the elements of the split matrices is less than . The accuracies of ozIMMU and ozIMMU EF are comparable, as are those of, ozIMMU RN and ozIMMU H . ozIMMU RN and ozIMMU H , which use a splitting method via rounding to nearest floating-point arithmetic, are more accurate than ozIMMU and ozIMMU EF , which use a splitting method via bit masking. Occasionally, to obtain results comparable with that of FP64, ozIMMU RNk , ozIMMU Hk , ozIMMU( k + 1) , and ozIMMU EF( k + 1) are required; i.e., ozIMMU RN and ozIMMU H require fewer splits than ozIMMU and ozIMMU EF . In particular, for ϕ = , ozIMMU RNand ozIMMU Hproduce comparable results to FP64; however, the accuracies of the results of ozIMMUand ozIMMU EFare worse than that of FP64. In such cases, k = is required for ozIMMU and ozIMMU EF to attain more accurate results than FP64. 4. Time breakdown Figures – and Figures – show time breakdowns of the proposed methods on RTX and GH200, respectively. Note that “split A”, “split B”, “Gemm in INT8-TC”, “accumulation in FP64”, and “copy” correspond to the steps (i) , (ii) , (iii) , (iv) , and (v) in the Ozaki scheme for emulating the GEMM routine as shown in Section . Figure 6. Time breakdown of ozIMMU RN on NVIDIA GeForce RTX Figure 7. Time breakdown of ozIMMU EF on NVIDIA GeForce RTX Figure 8. Time breakdown of ozIMMU H on NVIDIA GeForce RTX The execution time of FP64 accumulation in ozIMMU RN is not negligible, because the number of additions in FP64 is Figure 9. Time breakdown of ozIMMU RN on NVIDIA GH200 Grace Hopper Superchip Figure 10. Time breakdown of ozIMMU EF on NVIDIA GH200 Grace Hopper Superchip Figure 11. Time breakdown of ozIMMU H on NVIDIA GH200 Grace Hopper Superchip not reduced. The execution times of FP64 accumulation in ozIMMU EF and ozIMMU H are less than those in ozIMMU and ozIMMU RN . From Figures – , the computation time of splitting via rounding to nearest floating-point arithmetic is not so fast because FP64 is much slower than the lowerprecision arithmetic on RTX 4090. From Figures and , the ratios of the computation time of splitting in ozIMMU EF and ozIMMU H are comparable on GH200. 4. Performance Figures and show throughput in TFLOPS and ratio to ozIMMU . A smaller number of slices corresponds to better performance because the total computation cost is less. Figure 12. Throughput comparison on NVIDIA GeForce RTX Prepared using sagej.cls Journal Title XX(X) Figure 13. Throughput comparison on NVIDIA GH200 Grace Hopper Superchip On RTX 4090, all methods are faster than or comparable to matrix multiplication in FP64 for n ≥ because FP64 is much slower than the lower-precision arithmetic. ozIMMU EF and ozIMMU H are faster than ozIMMU almost everywhere. In particular, ozIMMU EFand ozIMMU Hare respectively 1. and 1. times faster than ozIMMU for n = . ozIMMU RN is slower than ozIMMU . For k = , ozIMMU , ozIMMU RN , ozIMMU EF , and ozIMMU H are . , . , . , and . times faster than matrix multiplication in FP64 for n = , respectively. For k = , ozIMMU , ozIMMU RN , ozIMMU EF , and ozIMMU H are . , . , . , and . times faster than matrix multiplication in FP64 for n = , respectively. In addition, ozIMMU , ozIMMU RN , ozIMMU EF , and ozIMMU H are . , . , . , and . times faster than FP64 for k = and n = , and . , . , . , and . times faster than FP64 for k = and n = . On GH200, the methods with small k are faster than matrix multiplication in FP64 for n ≥ ; however, the methods are much slower than FP64 otherwise. ozIMMU EF and ozIMMU H are faster than ozIMMU . In particular, ozIMMU EFand ozIMMU Hare respectively 1. and 1. times faster than ozIMMU for n = . ozIMMU RN is not as slow on GH200 as it is on RTX 4090. For k = , ozIMMU , ozIMMU RN , ozIMMU EF , and ozIMMU H are . , . , . , and . times faster than matrix multiplication in FP64 for n = , respectively. ozIMMU , ozIMMU RN , ozIMMU EF, and ozIMMU Hhave comparable computation times to matrix multiplication in FP64 for n = ; however, ozIMMU k , ozIMMU RN k , ozIMMU EF( k + 1) , and ozIMMU H( k + 1) are slower than FP64 for n < or k > . 4. Performance vs. Accuracy Figure shows throughput in a TFLOPS vs. Accuracy plot, illustrating relationships between performance and accuracy for the Ozaki scheme and for matrix multiplication in FP64 for n = and ϕ = . The numbers inside the symbols are the numbers of splices for the Ozaki scheme. On RTX 4090, all methods are better than matrix multiplication in FP64 with respect to both performance and accuracy for k ≥ . ozIMMU H and ozIMMU EF produce results with comparable performances that are more accurate than those of ozIMMU . In particular, (a) NVIDIA GeForce RTX (b) NVIDIA GH200 Grace Hopper Superchip Figure 14. Relationship between performance and accuracy for n = and ϕ = ozIMMU Hand ozIMMU EFproduce results with comparable performances that are more accurate than those of ozIMMU. On GH200 as well, ozIMMU H and ozIMMU EF produce results with comparable performances that are more accurate than those of ozIMMU . In particular, ozIMMU Hand ozIMMU EFproduce results with comparable performances that are more accurate than those of ozIMMU. These results also indicate that ozIMMU H and ozIMMU EF can be computed with less memory consumption than ozIMMU .",
    "Input: A ′′": "s ∈ I m × n , µ ′′ ( s ) ∈ F m , B ′′ s ∈ I n × p , ν ′′ ( s ) ∈ F p , s = , . . . , k Output: C ∈ F m × p 1: β ← min( , ⌊ ( − log n ) / ⌋ ) = 2: C ← O m,p 3: for g = , . . . , k + do C ′′ ← O m,p { C ′′ ∈ I m × p for s = , . . . , g − do C ′′ ← C ′′ + A ′′ s B ′′ g − s { Compute using GEMM on INT8 Tensor Core } end for C ← C + diag( µ ′′ ) − βg + FP64( C ′′ )diag( ν ′′ ) { Compute in FP64 } 9: end for are µ ′′ ( s ) · β ( s − 1) ; thus, µ ′′ ( s ) are determined before the “for” statement and the shift values for A s for s ≥ are calculated by shifting µ ′′ ( s ) by a constant ratio β . Hence, the algorithm finds the maximum absolute values max j | a ij | for i = , . . . , m only once before the “for” statement. On the other hand, the shift values µ ′ ( s ) are determined inside the “for” statement in Algorithm . Therefore, the algorithm finds the maximum absolute values max j | a ij | for i = , . . . , m at most k times. Numerical examples We have implemented the proposed methods in Uchino ( ), replacing specific code in Ootomo’s ozIMMU library with our code. All numerical experiments were conducted on NVIDIA GH200 Grace Hopper Superchip and NVIDIA",
    "Experiments": "In this section, we give a rounding analysis for a fixed number of slices. We have described Algorithms and (ozIMMU), and proposed Algorithms and , Algorithms and , and Algorithms and . Even if we used the rounding to nearest strategy as in Algorithms and , their error bounds would be the same as that of the bitmask strategy used in Algorithm . Therefore, we focus on Algorithms and (ozIMMU) and Algorithms and . Below, absolute value notation applied to a matrix means the matrix from applying absolute value element-wise. We assume that neither overflow nor underflow occurs. For A ∈ F m × n and B ∈ F n × p , the following deterministic error bound is given by Jeannerod and Rump ( ): | AB − fl ( AB ) | ≤ nu | A || B | . The following alternative probabilistic error bound comes from Higham and Mary ( ) (which has the details on the assumptions and probabilities): | AB − fl ( AB ) | ≲ √ nu | A || B | . In this section, our aim is to derive an error bound on the computed result. Let T ∈ F m × p be a computed result using k slices, such as A ≈ A + A + · · · + A k , B ≈ B + B + · · · + B k . Note that for i = , , . . . , k , A i := diag( µ ′ ( i ) ) A ′ i , B i := B ′ i diag( ν ′ ( i ) ) for Algorithms , , and , and A i := diag( µ ′′ ) − iβ + A ′′ i , B i := − iβ + B ′′ i diag( ν ′′ ) Prepared using sagej.cls Uchino, Ozaki, and Imamura for Algorithms , , , , and . Let T k be an approximation of AB with k slices. Our aim is to obtain the upper bound on | AB − T | as follows: | AB − T k | AB − i = k − i + j = A i B j i = k − i + j = A i B j − T k (13) Note that we can obtain the exact product A i B j in the above by using GEMM in the INT8 Tensor Core and scaling by powers of two. The first term of the bound in ( ) indicates the truncation error, and the second term shows an error arising in the accumulation process. As matrix scaling does not affect rounding errors, a discussion on it is omitted. In the following subsections, we use an upper bound provided in Jeannerod and Rump ( ) for a sum: i = p − fl i = ! ≤ ( n − 1) u i = | p i | , p ∈ F n . (14) Let ufp ( c ) for c ∈ R be defined as ufp ( c ) := if c = , ⌊ log | c |⌋ otherwise . The following condition is used to check whether a real number is a floating-point number. For c ∈ R , if | c | is smaller than the maximum floating-point number and c is an integral multiple of the minimum positive floating-point number, then it holds that c ∈ u · ufp ( c ) Z c ∈ F . (15) 5. Error bound for Algorithms and (ozIMMU) Let matrices V k ∈ F m × n and W k ∈ F n × p be defined as in ( ) where these show the truncation errors of A and B for the case of k slices, respectively. Notation e indicates the vector e = ( , , . . . , 1) T ∈ F n . Define two vectors g ∈ F m and h ∈ F p related to row-wise maximums in A and columnwise maximums in B in the sense of the unit in the first place g i := ufp max | a ij | f j := ufp max | b ij | Then, from Figure and ( ), we have | V i | ≤ − βi + ge T , | W i | ≤ − βi + ef T , (16) and matrices | A i | and | B | are bounded as | A i | ≤ − β ( i − 1)+ ge T , | B | ≤ ef T . (17) Because AB = ( A + · · · + A k + V k )( B + · · · + B k + W k ) , we have AB − i = k − i + j = A i B j = i = A i W k − i + + V k B. From the last equation, ( ), and ( ), the truncation error is bounded as AB − i = k − i + j = A i B j i = | A i | | W k − i + | + | V k | | B | i = − β ( i − 1)+ ge T · − β ( k − i +1)+ ef T + · − βk + ge T · ef T = n i = − βk gf T + n · − βk gf T = 4( k + 1) n − βk gf T . (18) If | g i −| a ij || for ≤ j ≤ n and | f j −| b ij || for ≤ i ≤ n are very small, we can assume that ngf T ≈| A || B | . (19) In this case, we have AB − i = k − i + j = A i B j ≲ ( k + 1) − βk | A || B | . (20) We next focus on an error in the accumulation, the second term in ( ). If A i and B i are obtained by Algorithm , | A | = i = | A i | , | B | = i = | B i | (21) are satisfied. Because we compute the sum of k ( k + 1) floating-point matrices, the following bound is immediately obtained from ( ) and ( ): i = k − i + j = A i B j − T k \u0012 k ( k + 1) − i = k − i + j = | A i B j | \u0012 k ( k + 1) − u | A || B | . (22) We will show that k ′ ( ≤ k ) exists such that k ′ s = k ′ − s + t = A s B t ∈ F m × n . (23) Then, the bound ( ) is improved. From Figure , we have ( A s ) ij ∈ − sβ + g i Z , ( B t ) ij ∈ − tβ + f j Z , (24) and these derive ( A s ) iℓ ( B t ) ℓj ∈ − β ( s + t )+ g i f j Z . (25) Figure shows an image of ( ). It follows from this that Prepared using sagej.cls Journal Title XX(X) Figure 15. An image of ( ) k ′ s = k ′ − s + t = A s B t ∈ − β ( k ′ +1)+ g i f j Z . (26) From Figure , the elements of the matrices are bounded | A s | ij ≤ − β ( s − 1)+ g i , | B t | ij ≤ − β ( t − 1)+ f j , so that we have | A s B t | ij ≤ n − β ( s + t − 2)+ g i f j . (27) Therefore, from ( ), we have the following bound on a dot product: k ′ s = k ′ − s + t = A s B t k ′ + s = i + j = s A i B j k ′ + s = i + j = s | A i || B j | k ′ + s = i + j = s n − ( i + j − 2) β + g i f j k ′ + s = i + j = s n − ( s − 2) β + g i f j k ′ + s = ( s − 1) n − ( s − 2) β + g i f j . If β ≥ , k ′ + s = ( s − 1) n − ( s − 2) β + g i f j ≤ n g i f j Finally, for β ≥ , we have k ′ s = k ′ − s + t = A s B t ≤ n · g i f j (28) From ( ), ( ), and ( ), if u · ufp ( n ) ≤ − β ( k ′ +1)+ (29) is satisfied, then ( ) holds. Let k ′ max be the maximum k ′ satisfying ( ). No rounding error occurs for i = , , . . . , k ′ max . Therefore, we have k = k − i + j = A i B j − T k \u0012 k ( k + 1) − k ′ max ( k ′ max + 1) − u | A || B | . (30) Summarizing, from ( ) and ( ), we have | AB − T k | ≤ 4( k + 1) n − βk gf T \u0012 k ( k + 1) − k ′ max ( k ′ max + 1) − u | A || B | . If ( ) is satisfied, then we have | AB − T k | ≲ ( k + 1) − βk | A || B | \u0012 k ( k + 1) − k ′ max ( k ′ max + 1) − u | A || B | . 5. Error bound for Algorithms and We focus on i + j = s A ′′ i B ′′ j , A ′′ i ∈ I m × n β + , B ′′ j ∈ I n × p β + . (31) We consider r such that the following holds i + j = s s ≤ min( r,k +1) A ′′ i B ′′ j ∈ I m × p (32) Let E m,n be the matrix I m × n whose elements are all ones. From the definitions in ( ), | A ′′ i | ≤ ( β − 1) E m,n , | B ′′ i | ≤ ( β − 1) E n,p , and we have i + j = s A ′′ i B ′′ j i + j = s | A ′′ i || B ′′ j | ≤ ( s − 1) n ( β − 1) E m,p . Since the largest number in I is − , if s ≤ r for r := max( , − β −⌈ log n ⌉ ) in ( ), we have ( s − 1) n ( β − 1) ≤ ( r − 1) n ( β − 1) ≤ ( r − 1) · ⌈ log n ⌉ · β = r · ⌈ log n ⌉ · β − ⌈ log n ⌉ · β ≤ − , which implies that ( ) holds. Note that ( s − 1) n ( β − 1) < − for β ≥ . This is why constant r was set as in ( ). Prepared using sagej.cls Uchino, Ozaki, and Imamura If we define the number of the terms, w , for the accumulation as w := \u0018 k \u0019 \u0012 k − r \u0016 k − we have | AB − T | ≤ 4( k + 1) n − βk gf T + ( w − 1) u | A || B | . If ( ) is satisfied, then we have | AB − T | ≲ ( k + 1) − βk | A || B | + ( w − 1) u | A || B | .",
    "Conclusion": "We proposed three implementation methods for accelerating the Ozaki scheme using the INT8 Tensor Core. In the original implementation, ozIMMU , provided by Ootomo et al., the ratio of the accumulation in FP64 is not negligible, as it accounts for approximately 40– % of the total computation time. The proposed methods ozIMMU EF and ozIMMU H reduce the computation time ratio of the accumulation to approximately 10– % and achieve a 1.2to 1.6-fold speedup. With future architectures expected to achieve high speed in lowerprecision matrix multiplication, the computation time ratio of the accumulation in FP64 is expected to increase. Thus, these proposed methods contribute to leveraging the performance of future architectures more effectively than ozIMMU . The proposed methods ozIMMU RN and ozIMMU H offer alternative splitting methods using floating-point arithmetic in round-to-nearest-even mode and produce more accurate results than ozIMMU for the same number of slices. Acknowledgements We thank Dr. Hiroyuki Ootomo from NVIDIA for his helpful comments on the implementation of ozIMMU. Declaration of conflicting interests The authors declare no competing interests. Funding This study was partially supported by JSPS Grant-in-Aid for JSPS Fellows No. 22KJ2741, JSPS Grant-in-Aid for Research Activity Start-up No. 24K23874, and JSPS KAKENHI Grant No. 23K28100. Supplemental material Not applicable.",
    "Author Biographies": "Yuki Uchino postdoctoral researcher RIKEN CCS. He received his Ph.D. in engineering from Shibaura Institute of Technology in 2024. His research interests include reliable computing, numerical linear algebra, and highly accurate algorithms. He won the student poster presentation award at the 38th JSST Annual International Conference on Simulation Technology (JSST 2019) and the student presentation awards at JSST and International Workshop on Reliable Computing and Computer-Assisted Proofs (ReCAP 2022). Katsuhisa Ozaki is a full professor in the Department of Mathematical Sciences at Shibaura Institute of Technology. He received his Ph.D. in engineering from Waseda University in",
    "2007. He was an Assistant Professor (2007-2008) and a Visiting": "Lecturer (2008-2009) at Waseda University. At Shibaura Institute of Technology, he has served as an Assistant Professor (20102013) and an Associate Professor (2013-2019), and has currently been a Professor since 2019. His research interests include reliable computing, particularly addressing rounding error problems in finite-precision arithmetic. He mainly focuses on numerical linear algebra and develops fast and accurate algorithms. Toshiyuki Imamura is a team leader of Large-scale Parallel Numerical Computing Technology Team at RIKEN R-CCS, and is responsible for developing numerical libraries on Fugaku. He received his Diploma and Doctorate in Applied Systems and Sciences from Kyoto University in and 2000. He was a Researcher at CCSE, JAERI (1996-2003), a visiting scientist at HLRS (2002), and an associate professor at the University of Electro-Communications (2003-2012). His research interests include HPC, auto-tuning technology, and parallel eigenvalue Prepared using sagej.cls Journal Title XX(X) computation. His research group won the HPL-MpX ranking (20202021) and was nominated as the Gordon Bell Prize finalist in SC05, SC06, and SC20. Prepared using sagej.cls"
  },
  "sections_summary": {
    "Performance Enhancement": "of the Ozaki Scheme on Integer Matrix Multiplication Unit Journal Title XX(X): – ©The Author(s) Reprints and permission: sagepub.co.uk/journalsPermissions.nav SAGE Yuki Uchino , Katsuhisa Ozaki , and Toshiyuki Imamura",
    "Abstract": "Researchers aimed to achieve high-performance and accuracy for general matrix multiplications using NVIDIA GPUs' low-precision units, similar to next-gen architectures. The Ozaki scheme uses transformations to perform higher-precision multiplications with lower precision multiplications and additions. This paper proposes alternative approaches to reduce these operations, improving performance on next-gen architectures through numerical experiments.",
    "Introduction": "Recent machine learning models require fast and energy-efficient processing, leading to the development of high-performance low-precision floating-point and integer matrix multiplication units. This study focuses on optimizing matrix multiplication algorithms for these architectures, including a scheme called the Ozaki scheme that converts higher-precision matrix products into sums of lower-precision products. Another approach is using lower/mixed-precision matrices to leverage recent architectures' computational power, which involves extracting and shifting lower-precision versions of the original matrix.",
    "2 Department of Mathematical Sciences, Shibaura Institute of Technol-": "Here's a concise overall summary combining the provided information:\n\nThe Ozaki scheme is a matrix multiplication routine that leverages INT8 Tensor Core to emulate FP64 GEMM (Generalized Matrix Multiplication) using lower-precision arithmetic. The routine consists of four main steps: extracting k lower-precision matrices from input B, computing A i B j in mixed/mixed precision, reversing and accumulating results into D using higher-precision arithmetic, and computing output C with scalars α, β, and matrix C.\n\nPrevious studies have built upon the Ozaki scheme, including an error-free transformation proposed by Ozaki et al. ( ), improvements using alternative splitting methods by Mukunoki et al. ( ), and Ootomo's ozIMMU. A new splitting method is proposed in this work, which uses Minamihata's technique to improve accuracy. The algorithm terminates when the input matrix A becomes singular and employs a calculation method to avoid errors caused by binary logarithm in floating-point arithmetic.",
    "Output: A ′": "Here is a concise overall summary:\n\nA new method accelerates ozIMMU computation on FP64 by reducing additions through matrix sum optimization. The algorithm groups indices (i, j) such that i + j = g and computes sums of matrices at each step, expressing inner sums as combinations of β values and other elements. It leverages the INT8 Tensor Core for error-free summation with certain constraints (r ≥ k), improving upon previous splitting methods by reducing summands in FP64 accumulation.",
    "Methodology": "The Ozaki scheme is split between FP64 and INT8 using floating-point arithmetic, specifically round-to-nearest-even mode with Minamihata's technique, to achieve error-free group-wise accumulation for a matrix A of size m x n and a natural number k.",
    "Input: A ′": "The text describes a mixed-precision splitting method for tensor operations. It involves computing the output of a tensor operation in a high precision format and then approximating it by combining results from lower precision formats using bit masking and shifting.\n\nA set of signed integers I_N is defined, where each integer is within a certain range. The Ozaki scheme uses this definition to describe an algorithm for splitting tensor operations into smaller parts with different precision levels.",
    "Output: A ′′": "Here is a concise overall summary of the implementation and comparison of four matrix multiplication algorithms for GPU acceleration:\n\nFour matrix multiplication algorithms (ozIMMU RNk, ozIMMU EFk, ozIMMU Hk) were compared to Ootomo's implementation (ozIMMUk) in terms of accuracy on matrices with random numbers and normal distributions. While ozIMMU outperforms other methods due to its splitting method via bit masking, it suffers from rounding errors for large matrix sizes.\n\nCompared to FP64 matrix multiplication, ozIMMU RN and ozIMMU H produce comparable results while being faster than Ootomo's implementation for certain values of n and k. ozIMMU EF and ozIMMU H surpass Ootomo's performance for larger values of k. ozIMMU methods generally perform better or are comparable to FP64 matrix multiplication for specific values of n and k.\n\nFor the RTX 4090 GPU, all methods outperform FP64 matrix multiplication for k ≥ 1, with ozIMMU H and EF producing similar performances that are more accurate than ozIMMU. On the GH200 GPU, ozIMMU methods are faster than FP64 for n ≥ 3, but slower for n < 3. Additionally, ozIMMU RN is not as slow on RTX 4090 as it is on other hardware.",
    "Input: A ′′": "The algorithm finds the maximum absolute values of elements in a matrix by iteratively computing shifts using GEMM (General Matrix Multiplication) on INT8 Tensor Core, and accumulating these shifts to form the final result. The process is repeated k times, with the maximum absolute values being computed at most k times.",
    "Experiments": "Here's a concise overall summary:\n\nThe error bound for matrix multiplication can be estimated using two formulas: a deterministic error bound and a probabilistic error bound, both of which depend on the number of iterations (k), the norms of the matrices involved (|A||B|), and a parameter β. The errors arise from truncation and accumulation processes in computing the matrix product.\n\nThe overall error can be bounded by expressing it as a combination of terms involving the truncated matrices A, B, and V, W. By considering properties of these matrices, such as bounds on their elements, an improved bound can be achieved when certain conditions are met.\n\nFor algorithms, an additional condition u·ufp(n)≤−β(k′+1)+29 is satisfied to obtain a tighter bound, which simplifies to |AB−T|≤4(k+1)n−βkGF(Tk(k+1)−k′max(k′max+1)−u)|A||B|. This inequality becomes even more efficient when β ≥.",
    "Conclusion": "Three methods were proposed to accelerate the Ozaki scheme using INT8 Tensor Core: ozIMMU EF, ozIMMU H, and ozIMMU RN. These methods reduce the computation time ratio of accumulation in FP64 by approximately 10–20%, achieving a 1.2-1.6-fold speedup compared to ozIMMU.",
    "Author Biographies": "Yuki Uchino, a postdoctoral researcher at RIKEN CCS, has a Ph.D. in engineering from 2024 and research interests in reliable computing, numerical linear algebra, and highly accurate algorithms. Katsuhisa Ozaki is a full professor at Shibaura Institute of Technology with a Ph.D. in engineering from Waseda University.",
    "2007. He was an Assistant Professor (2007-2008) and a Visiting": "Toshiyuki Imamura is a Professor at RIKEN R-CCS, with expertise in reliable computing, numerical linear algebra, and parallel computing. He has held various academic positions, including Assistant Professor, Associate Professor, and Lecturer, and has received his Diploma and Doctorate from Kyoto University. His research focuses on fast and accurate algorithms for numerical libraries, as well as high-performance computing and auto-tuning technology."
  },
  "tables": [],
  "images": [
    "processed/images/2409.13313v1_page2_img0.png",
    "processed/images/2409.13313v1_page2_img1.png",
    "processed/images/2409.13313v1_page2_img2.png",
    "processed/images/2409.13313v1_page4_img0.png",
    "processed/images/2409.13313v1_page6_img0.png",
    "processed/images/2409.13313v1_page7_img0.png",
    "processed/images/2409.13313v1_page7_img1.png",
    "processed/images/2409.13313v1_page7_img2.png",
    "processed/images/2409.13313v1_page7_img3.png",
    "processed/images/2409.13313v1_page7_img4.png",
    "processed/images/2409.13313v1_page7_img5.png",
    "processed/images/2409.13313v1_page7_img6.png",
    "processed/images/2409.13313v1_page8_img0.png",
    "processed/images/2409.13313v1_page8_img1.png",
    "processed/images/2409.13313v1_page8_img2.png",
    "processed/images/2409.13313v1_page10_img0.png"
  ],
  "status": "completed"
}