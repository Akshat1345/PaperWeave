{
  "metadata": {
    "title": "Universal scaling between wave speed and size enables nanoscale   high-performance reservoir computing based on propagating spin-waves",
    "authors": [
      "Satoshi Iihama",
      "Yuya Koike",
      "Shigemi Mizukami",
      "Natsuhiko Yoshinaga"
    ],
    "abstract": "Neuromorphic computing using spin waves is promising for high-speed nanoscale devices, but the realization of high performance has not yet been achieved. Here we show, using micromagnetic simulations and simplified theory with response functions, that spin-wave physical reservoir computing can achieve miniaturization down to nanoscales keeping high computational power comparable with other state-of-art systems. We also show the scaling of system sizes with the propagation speed of spin waves plays a key role to achieve high performance at nanoscales.",
    "published": "",
    "arxiv_id": "2301.02193v1",
    "categories": [
      "physics.app-ph",
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.02193v1",
    "pdf_file": "data/pdfs/2301.02193v1.pdf",
    "pdf_filename": "2301.02193v1.pdf"
  },
  "processing_info": {
    "processed_at": "2025-10-28T10:41:16.734504",
    "is_large_pdf": true,
    "sections_found": 9,
    "tables_found": 21,
    "images_found": 32
  },
  "sections_text": {
    "Abstract": "arXiv:2301.02193v1 [physics.app-ph] Jan Universal scaling between wave speed and size enables nanoscale high-performance reservoir computing based on propagating spin-waves Satoshi Iihama, , Yuya Koike, , , Shigemi Mizukami, , Natsuhiko Yoshinaga , ∗ Sendai, 980-8578, Japan Katahira 2-1-1, Sendai, 980-8577, Japan Sendai, 980-8579, Japan Sendai, 980-8577, Japan",
    "5 MathAM-OIL, AIST, Sendai, 980-8577, Japan": "∗ To whom correspondence should be addressed; E-mail: yoshinaga@tohoku.ac.jp. Neuromorphic computing using spin waves is promising for high-speed nanoscale devices, but the realization of high performance has not yet been achieved. Here we show, using micromagnetic simulations and simpliﬁed theory with response functions, that spin-wave physical reservoir computing can achieve miniaturization down to nanoscales keeping high computational power comparable with other state-of-art systems. We also show the scaling of system sizes with the propagation speed of spin waves plays a key role to achieve high performance at nanoscales.",
    "Introduction": "Non-local magnetization dynamics in a nanomagnet, spin-waves, can be used for processing information in an energy-efﬁcient manner since spin-waves carry information in a magnetic material without Ohmic losses ( ). The wavelength of the spin-wave can be down to the nanometer scale, and the spin-wave frequency becomes several GHz to THz frequency, which are promising properties for nanoscale and high-speed operation devices. Recently, neuromorphic computing using spintronics technology has attracted great attention for the development of future low-power consumption artiﬁcial intelligence ( ). Spin-waves can be created by various means such as magnetic ﬁeld, spin-transfer torque, spin-orbit torque, voltage induced change in magnetic anisotropy and can be detected by the magnetoresistance effect ( ). Therefore, neuromorphic computing using spin waves may have a potential of realisable devices. Reservoir computing (RC) is a promising neuromorphic computation framework. RC is a variant of recurrent neural networks (RNNs) and has a single layer, referred to as a reservoir, to transform an input signal into an output ( ). In contrast with the conventional RNNs, RC does not update the weights in the reservoir. Therefore, by replacing the reservoir of an artiﬁcial neural network with a physical system, for example, magnetization dynamics, we may realize a neural network device to perform various tasks, such as time-series prediction ( 4, ), short-term memory ( 6, ), pattern recognition, and pattern generation. Several physical RC has been proposed: spintronic oscillators ( 8, ), optics ( ), photonics ( 11, ), ﬂuids, soft robots, and others (see reviews ( 13– )). Among these systems, spintronic RC has the advantage in its potential realization of nanoscale devices at high speed of GHz frequency with low power consumption, which may outperform conventional electric computers in future. So far, spintronic RC has been considered using spin-torque oscillators ( 8, ), magnetic skyrmion ( ), and spin waves in garnet thin ﬁlms ( 17– ). However, the current performance of spintronic RC still remains poor compared with the Echo State Network (ESN) ( 6, ), idealized RC systems. The biggest issue is a lack of our understanding of how to achieve high performance in the RC systems. To achieve high performance, the reservoir has to have a large degree of freedom, N . However, in practice, it is difﬁcult to increase the number of physical nodes, N p , because it requires more wiring of multiple inputs. In this respect, wave-based computation in continuum media has attracting features. The dynamics in the continuum media have large, possibly inﬁnite, degrees of freedom. In fact, several wave-based computations have been proposed ( 20, ). The challenge is to use the advantages of both wave-based computation and RC to achieve highperformance computing of time-series data. For spin wave-based RC, so far, the large degrees of freedom are extracted only by using a large number of input and/or output nodes ( 19, ). Here, to propose a realisable spin wave RC, we use an alternative route; we extract the information from the continuum media using a small number of physical nodes. Along this direction, using N v virtual nodes for the dynamics with delay was proposed to increase N in ( ). This idea was applied in optical ﬁbres with a long delay line ( ) and a network of oscillators with delay ( ). Nevertheless, the mechanism of high performance remains elusive, and no uniﬁed understanding has been made. The increase of N = N p N v with N v does not necessarily improve performance. In fact, RC based-on STO struggles with insufﬁcient performance both in experiments ( ) and simulations ( ). The photonic RC requires a large size of devices due to the long delay line ( 11, ). In this work, we show nanoscale and high-speed RC based on spin wave propagation with a small number of inputs can achieve performance comparable with the ESN and other state-of-art RC systems. More importantly, by using a simple theoretical model, we clarify the mechanism of the high performance of spin wave RC. We show the scaling between wave speed and system size to make virtual nodes effective.",
    "Experiments": "We consider the Landau-Lifshitz-Gilbert equation for the magnetization ﬁeld m ( x , t ) , ∂ t m ( x , t ) = − m × h eﬀ − m × ( m × h eﬀ ) + σ ( x , t ) m × ( m × m f ) (31) We normalize both the magnetic and effective ﬁelds by saturation magnetization as m = M /M s and h eﬀ = H eﬀ /M s . This normalization applies to all the ﬁelds including external and anisotropic ﬁelds. We also normalize the current density as σ ( x , t ) = J ( x , t ) /j for the current density J ( x ) and the unit of current density j =",
    "1 U k 2": "· · · U k n n . (26) Therefore, MC and IPC are essentially a reconstruction of β k ,k , ··· ,k n from ˜˜ β ( i ) k ,k , ··· ,k n with i ∈ [ , N ] . This can be done by regarding β k ,k , ··· ,k n as a T + T ( T − 1) / + · · · -dimensional vector, and using the matrix M associated with the readout weights as β k ,k , ··· ,k n = M · ˜˜ β (1) k ,k , ··· ,k n ˜˜ β (2) k ,k , ··· ,k n ... ˜˜ β ( N ) k ,k , ··· ,k n (27) MC corresponds to the reconstruction of β k ,k , ··· ,k n for P i k i = , whereas the second-order IPC is the reconstruction of β k ,k , ··· ,k n for P i k i = . If all of the reservoir states are independent, we may reconstruct N components in β k ,k , ··· ,k n . In realistic cases, the reservoir states are not independent, and therefore, we can estimate only < N components in β k ,k , ··· ,k n . Prediction of chaotic time-series data Following ( ), we perform the prediction of time-series data from the Lorenz model. The model is a three-variable system of ( A ( t ) , A ( t ) , A ( t )) yielding the following equation dA dt = 10( A − A ) (28) dA dt = A ( − A ) − A (29) dA dt = A A −",
    "Discussion": "Figure shows reports of reservoir computing in literature with multiple nodes plotted as a function of the length of nodes L and products of wave speed and delay time vτ for both photonic and spintronic RC. For the spintronic RC, the dipole interaction is considered for wave propagation in which speed is proportional to both saturation magnetization and thickness of the ﬁlm ( )(See supplementary information sec. C). For the photonic RC, the characteristic speed is the speed of light, v ∼ m s − . Symbol size corresponds to MC taken from the literature [See details of plots in supplementary information sec. D]. Plots are roughly on a broad oblique (a) (b) (c) (d) wave speed (log m/s) characteristic size (log nm) 1. 3. 2. 2. 3. 4. damping time 1. wave speed (log m/s) characteristic size (log nm) 1. 3. 2. 2. 3. 4. IPC damping time 1. wave speed (log m/s) characteristic size (log nm) 1. 3. 2. 2. 3. 4. 1. 4. 5. wave speed (log m/s) characteristic size (log nm) 1. 3. 2. 2. 3. 4. IPC 1. 4. 5. time response function ( ) ( ) ( ) ( ) ( ) time dense sparse memorise (e) Figure 6: Scaling between characteristic size and propagating wave speed obtained by response function method. MC (a,c) and IPC (b,d) as a function of the characteristic length scale between physical nodes R and the speed of wave propagation v . The results with the response function for the dipole interaction (a,b) and for the Gaussian function (5) (c,d) are shown. (e) Schematic illustration of the response function and its relation to wave propagation between physical nodes. When the speed of the wave is too fast, all the response functions are overlapped (dense regime), while the response functions cannot cover the time windows when the speed of the wave is too slow (sparse regime). line with a ratio L/ ( vτ ) ∼ 1. Therefore, the photonic RC requires a larger system size, as long as the delay time of the input τ = N v θ is the same order ( τ = . − ns in our spin wave RC). As can be seen in Fig. 6, if one wants to reduce the length of physical nodes, one must reduce wave speed or delay time; otherwise the information is dense, and the reservoir cannot memorize many degrees of freedom (See Fig. 6(e)). Reducing delay time is challenging since the experimental demonstration of the photonic reservoirs has already used the short delay close to the instrumental limit. Also, reducing wave speed in photonics systems is challenging. On the other hand, the wave speed of propagating spin-wave is much lower than the speed of light and can be tuned by conﬁguration, thickness and material parameters. If one reduces wave speed or delay time over the broad line in Fig. 7, information becomes sparse and cannot be used efﬁciently(See Fig. 6(e)). Therefore, there is an optimal condition for high-performance RC. The performance is comparable with other state of the art techniques, which are summarized in Fig. 8. For example, for the spintronic RC, MC ≈ ( ) and NRMSE ≈ . ( ) in the NARMA10 task are obtained using N p ≈ physical nodes. The spintronic RC with one physical node but with − virtual nodes do not show high performance; MC is less than (the bottom left points in Fig. 8). This fact suggests that the spintronic RC so far cannot use virtual nodes effectively. On the other hand, for the photonic RC, comparable performances are achieved using N v ≈ virtual nodes, but only one physical node. As we discussed, however, the photonic RC requires mm system sizes. Our system achieves comparable performances using ≲ physical nodes, and the size is down to nanoscales keeping the − GHz computational speed. We also demonstrate that the spin wave RC can perform time-series prediction and reconstruction of an attractor for the chaotic data. To our knowledge, this has not been done in nanoscale systems. Our results of micromagnetic simulations suggest that our system can be physically imThis work ( = 1. ns) This work ( = 0. ns) Spintronic RC ( ) ( ) Photonic RC ( ) ( ) ( ) ( ) ( ) (m) Length, L (m) Dense Sparse Figure 7: Reports of reservoir computing using multiple nodes are plotted as a function of the length between nodes and characteristic wave speed ( v ) times delay time ( τ ) for photonics system (open symbols) and spintronics system (solid symbols). The size of symbols corresponds to memory capacity, which is taken from literature ( 12,19,22,32– ) and this work. The gray scale represents memory capacity evaluated by using the response function method [Eq. (5)]. 0. This work (Calc. , N v = 8, θ = 0. ns) This work (Calc., N v = 8, θ = 0. ns) Spintronic RC (Calc.) ( ) Photonic RC ( ) ( N v = 50) ( ) ( N v = 50) ( ) ( N v = 50) Normalized root mean square error, NRMSE for NARMA10 task Number of physical nodes, N p This work (Calc., N v = 8, θ = 0. ns) This work (Calc., N v = 8, θ = 0. ns) Spintronic RC (Calc.) ( ) ( ) ( ) Spintronic RC (Exp.) ( ) ( N v = 250) ( ) ( N v = 40) Photonic RC ( ) ( N v = 50) ( ) ( N v = 50) ( ) ( N v = 50) Memory capacity, MC Number of physical nodes, N p (a) (b) Figure 8: Reservoir computing performance compared with different systems. (a) Memory capacity, MC reported plotted as a function of physical nodes N p . (b) Normalized root mean square error, NRMSE for NARMA10 task is plotted as a function of N p . Open blue symbols are values reported using photonic RC while solid red symbols are values reported using spintronic RC. MC and NRMSE for NARMA10 task are taken from Refs. ( 9,19,22,36, ) for spintronic RC and Refs. ( 32–34,38, ) for photonic RC. plemented. All the parameters in this study are feasible using realistic materials ( 40– ). Nanoscale propagating spin waves in a ferromagnetic thin ﬁlm excited by spin-transfer torque using nanometer electrical contacts have been observed ( 44– ). Patterning of multiple electrical nanocontacts into magnetic thin ﬁlms was demonstrated in mutually synchronized spintorque oscillators ( ). In addition to the excitation of propagating spin-wave in a magnetic thin ﬁlm, its non-local magnetization dynamics can be detected by tunnel magnetoresistance effect at each electrical contact, as schematically shown in Fig. 1(c), which are widely used for the development of spintronics memory and spin-torque oscillators. In addition, virtual nodes are effectively used in our system by considering the speed of propagating spin-wave and distance of physical nodes; thus, high-performance reservoir computing can be achieved with the small number of physical nodes, contrary to many physical nodes used in previous reports. This work provides a way to realize nanoscale high-performance reservoir computing based on propagating spin-wave in a ferromagnetic thin ﬁlm. There is an interesting connection between our study to the recently proposed next-generation RC ( 28, ), in which the linear ESN is identiﬁed with the NVAR (nonlinear vectorial autoregression) method to estimate a dynamical equation from data. Our formula of the response function (3) results in the linear input-output relationship with a delay Y n + = a n U n + a n − U n − + . . . (see Sec. A in Supplementary Information). More generally, with the nonlinear readout or with higher-order response functions, we have the input-output relationship with delay and nonlinearity Y n + = a n U n + a n − U n − + . . . + a n,n U n Y n + a n,n − U n U n − + . . . (see Sec. B in Supplementary Information). These input-output relations are nothing but Volterra series of the output as a function of the input with delay and nonlinearity ( ). The coefﬁcients of the expansion are associated with the response function. Therefore, the performance of RC falls into the independent components of the matrix of the response function, which can be evaluated by how much delay the response functions between two nodes cover without overlap. The results would be helpful to a potential design of the network of the physical nodes. We should note that the polynomial basis of the input-output relation in this study originates from spin wave excitation around the stationary state m z = . When the input data has a hierarchical structure, another basis may be more efﬁcient than the polynomial expansion. Another setup of magnetic systems may lead to a different basis. We believe that our study shows simple but clear intuition of the mechanism of high-performance RC, that can lead to the exploration of another setup for more practical application of the physical RC. Materials and Methods Micromagnetic simulations We analyze the LLG equation using the micromagnetic simulator mumax ( ). The LLG equation for the magnetization M ( x , t ) yields ∂ t M ( x , t ) = − γµ + α M × H eﬀ − αγµ M s ( + α ) M × ( M × H eﬀ ) ℏ Pγ",
    "4 M 2 s eD J ( x , t ) M × ( M × m f )": "(6) We consider the effective magnetic ﬁeld as H eﬀ = H ext + H demag + H exch , (7) H ext = H e z (8) H ms = − π | r − r ′ | d r ′ (9) H exch = A ex µ M s ∆ M , (10) where H ext is the external magnetic ﬁeld, H ms is the magnetostatic interaction, and H exch is the exchange interaction with the exchange parameter A ex . The size of our system is L = nm and D = nm. The number of mesh points is in the x and y directions, and in the z direction. We consider Co MnSi Heusler alloy ferromagnet, which has a low Gilbert damping and high spin polarization with the parameter A ex = . pJ/m, M s = kA/m, and α = × − ( 40,41,41– ). Out-of-plane magnetic ﬁeld µ H = . T is applied so that magnetization is pointing out-of-plane. The spin-polarized current ﬁeld is included by the Slonczewski model ( ) with polarization parameter P = and spin torque asymmetry parameter λ = with the reduced Planck constant ℏ and the charge of an electron e . The uniform ﬁxed layer magnetization is m f = e x . We use absorbing boundary layers for spin waves to ensure the magnetization vanishes at the boundary of the system ( ). We set the initial magnetization as m = e z . The reference time scale in this system is τ = /γµ M s ≈ ps, where γ is the gyromagnetic ratio, µ is permeability, and M s is saturation magnetization. The reference length scale is the exchange length l ≈ nm. The relevant parameters are Gilbert damping α , the time scale of the input time series θ , and the characteristic length between the input nodes R . The injectors and detectors of spin are placed as cylindrical nanocontacts embedded in the region with their radius a and height D . We set a = nm unless otherwise stated. The input time series is uniform random noise U n ∈U ( , . 5) . The injected density current is set as j ( t n ) = j c U n with j c = × − / ( πa )A / m . Under a given input time series of the length T , we apply the current during the time θ , and then update the current at the next step. The same input current with different ﬁlters is injected for different virtual nodes (see Learning with reservoir computing). The total simulation time is, therefore, TθN v . Learning with reservoir computing Our RC architecture consists of reservoir state variables X ( t + ∆ t ) = f ( X ( t ) , U ( t )) (11) and the readout Y n = W · ˜˜ X ( t n ) . (12) In our spin wave RC, the reservoir state is chosen as x -component of the magnetization X = m x, ( t n ) , . . . , m x,i ( t n ) , . . . , m x,N p ( t n ) \u0001 T , (13) for the indices for the physical nodes i = , , . . . , N p . Here, N p is the number of physical nodes, and each m x,i ( t n ) is a T -dimensional row vector with n = , , . . . , T . We use a timemultiplex network of virtual nodes in RC ( ), and use N v virtual nodes with time interval θ . The expanded reservoir state is expressed by N p N v × T matrix ˜ X as (see Fig.2(b)) ˜ X = ( m x, ( t n, ) , m x, ( t n, ) , . . . , m x, ( t n,k ) , . . . , m x, ( t n,N v ) , . . . , m x,i ( t n, ) , m x,i ( t n, ) , . . . , m x,i ( t n,k ) , . . . , m x,i ( t n,N v ) , . . . , m x,N p ( t n, ) , m x,N p ( t n, ) , . . . , m x,N p ( t n,k ) , . . . , m x,N p ( t n,N v ) \u0001 T , (14) where t n,k = (( n − 1) N v − ( k − 1)) θ for the indices of the virtual nodes k = , , . . . , N v . The total number of rows is N = N p N v . We use the nonlinear readout by augmenting the reservoir state as ˜˜ X = ˜ X ˜ X ◦ ˜ X (15) where ˜ X ( t ) ◦ ˜ X ( t ) is the Hadamard product of ˜ X ( t ) , that is, component-wise product. The readout weight W is trained by the data of the output Y ( t ) W = Y · ˜˜ X † (16) where X † is pseudo-inverse of X . In the time-multiplexing approach, the input time-series U = ( U , U , . . . , U T ) ∈ R T is translated into piece-wise constant time-series ˜ U ( t ) = U n with t = ( n − 1) N v θ + s under k = , . . . , T and s = [ , N v θ ) (see Fig. 2(a)). This means that the same input remains during the time period τ = N v θ . To use the advantage of physical and virtual nodes, the actual input J i ( t ) at the i th physical node is ˜ U ( t ) multiplied by τ -periodic random binary ﬁlter B i ( t ) . Here, B i ( t ) ∈{ , } is piece-wise constant during the time θ . At each physical node, we use different realizations of the binary ﬁlter as in Fig. 2(a). Unless otherwise stated, We use steps of the input time-series as burn-in. After these steps, we use steps for training and steps for test for the MC, IPC, and NARMA10 tasks. NARMA task The NARMA10 task is based on the discrete differential equation, Y n + = αY n + βY n p = Y n − p + γU n U n − + δ. (17) Here, U n is an input taken from the uniform random distribution U ( , . 5) , and y k is an output. We choose the parameter as α = . , β = . , γ = . , and δ = . . In RC, the input is U = ( U , U , . . . , U T ) and the output Y = ( Y , Y , . . . , Y T ) . The goal of the NARMA10 task is to estimate the output time-series Y from the given input U . The training of RC is done by tuning the weights W so that the estimated output ˆ Y ( t n ) is close to the true output Y n in terms of squared norm | ˆ Y ( t n ) − Y n | . The performance of the NARMA10 task is measured by the deviation of the estimated time series ˆ Y = W · ˜˜ X from the true output Y . The normalized root-mean-square error (NRMSE) NRMSE ≡ n ( ˆ Y ( t n ) − Y n ) n Y (18) Performance of the task is high when NRMSE ≈ . In the ESN, it was reported that NRMSE ≈ . for N = and NRMSE ≈ . for N = ( ). The number of node N = was used for the speech recognition with ≈ . word error rate ( ), and time-series prediction of sptiotemporal chaos ( ). Therefore, NRMSE ≈ . is considered as reasonably high performance in practical application. We also stress that we use the same order of nodes (virtual and physical nodes) N = to achieve NRMSE ≈ . . Memory capacity and information processing capacity Memory capacity (MC) is a measure of the short-term memory of RC. This was introduced in ( ). For the input U n of random time series taken from the uniform distribution, the network is trained for the output Y n = U n − k . The MC is computed from MC k = ⟨ U n − k , W · X ( t n ) ⟩ ⟨ U n ⟩⟨ ( W · X ( t n )) ⟩ . (19) This quantity is decaying as the delay k increases, and MC is deﬁned as MC = k max k = MC k . (20) Here, k max is a maximum delay, and in this study we set it as k max = . The advantage of MC is that when the input is independent and identically distributed (i.i.d.), and the output function is linear, then MC is bounded by N , the number of internal nodes. Information processing capacity (IPC) is a nonlinear version of MC ( ). In this task, the output is set as Y n = P d k ( U n − k ) (21) where d k is non-negative integer, and P d k ( x ) is the Legendre polynomials of x order d k . We may deﬁne IPC d ,d ,...,d T − = ⟨ Y n , W · X ( t n ) ⟩ ⟨ Y n ⟩⟨ ( W · X ( t n )) ⟩ . (22) and then compute j th order IPC as We may deﬁne IPC j = d k s . t .j = P k d k IPC d ,d ,...,d T . (23) When j = , the IPC is, in fact, equivalent to MC, because P ( x ) = and P ( x ) = x . In this case, Y n = U n − k for d i = when i = k and d i = otherwise. (23) takes the sum over all possible delay k , which is nothing but MC. When j > , IPC captures all the nonlinear transformation and delays up to the j th polynomial order. For example, when j = , the output can be Y n = U n − k U n − k or Y n = U n − k + const . In this study, we focus on j = because the second-order nonlinearity is essential for the NARMA10 task (see Sec. A in Supplementary Information). The relevance of MC and IPC is clear by considering the Volterra series of the input-output relation, Y n = k ,k , ··· ,k t β k ,k , ··· ,k n U k",
    "3 A 3": "(30) The parameters are chosen such that the model exhibits chaotic dynamics. Similar to the other tasks, we apply the different masks of binary noise for different physical nodes, B ( l ) i ( t ) ∈ {− , } . Because the input time series is three-dimensional, we use three independent masks for A , A , and A , therefore, l ∈{ , , } . The input for the i th physical node after the mask is given as B i ( t ) ˜ U i ( t ) = B (1) i ( t ) A ( t )+ B (2) i ( t ) A ( t )+ B (3) i ( t ) A ( t ) . Then, the input is normalized so that its range becomes [ , . 5] , and applied as an input current. Once the input is prepared, we may compute magnetization dynamics for each physical and virtual node, as in the case of the NARMA10 task. We note that here we use the binary mask of {− , } instead of { , } used for other tasks. We found that the { , } does not work for the prediction of the Lorenz model, possibly because of the symmetry of the model. The ground-truth data of the Lorenz time-series is prepared using the Runge-Kutta method with the time step ∆ t = . . The time series is t ∈ [ − , 75] , and t ∈ [ − , − 50] is used for relaxation, t ∈ ( − , 0] for training, and t ∈ ( , 75] for prediction. During the training steps, we compute the output weight by taking the output as Y = ( A ( t + ∆ t ) , A ( t + ∆ t ) , A ( t + ∆ t )) . After training, the RC learns the mapping ( A ( t ) , A ( t ) , A ( t )) → ( A ( t + ∆ t ) , A ( t + ∆ t ) , A ( t + ∆ t )) . For the prediction steps, we no longer use the ground-truth input but the estimated data ( ˆ A ( t ) , ˆ A ( t ) , ˆ A ( t )) . Using the ﬁxed output weights computed in the training steps, the time evolution of the estimated time-series ( ˆ A ( t ) , ˆ A ( t ) , ˆ A ( t )) is computed by the RC.",
    "4 M 2": "s eπa Dµ ℏ P . We apply the current density at the nanocontact as J ( x , t ) = j c ˜ U ( t ) N p i = χ a ( | x − R i | ) (32) Here χ a ( x ) is a characteristic function χ a ( x ) = when x ≤ a and χ a ( x ) = otherwise. We expand the solution of (31) around the uniform magnetization m ( x , t ) = ( , , 1) without current injection as m ( x , t ) = m ( x , t ) + ǫ m (1) ( x , t ) + O ( ǫ ) . (33) Here, m ( x , t ) = ( , , 1) and ǫ ≪ is a small parameter corresponding to the magnitude of the input σ ( x , t ) . The ﬁrst-order term corresponds to a linear response of the magnetization to the input σ , whereas the higher-order terms describe nonlinear responses, for example, m (2) ( x , t ) ∼ σ ( x , t ) σ ( x , t ) . Because our input is driven by the spin torque with ﬁxed layer magnetization in the x -direction, m f = e x , only m x and m y appear in the ﬁrst-order term O ( ǫ ) . Deviation of m z from m z = appears in O ( ǫ ) . Therefore, for the ﬁrst-order term m (1) , we may deﬁne the complex magnetization m = m x + im y . (34) Here, we will show the magnetization is expressed by the response function G ij ( t ) . The input at the j th physical node affects the magnetization at the i th physical node as m i ( t ) dτG ii ( t − τ ) σ i ( τ ) + P i ̸ = j dτG ij ( t − τ ) σ j ( τ ) . (35) The input for the j th physical node is expressed by σ j ( t ) = j c B j ( t ) ˜ U j ( t ) . Because different physical nodes have different masks discussed in Learning with reservoir computing in Methods. When the wave propagation is dominated by the exchange interaction, the response function for the same node is G ii ( t − τ ) = π e − ˜ h ( α + i )( t − τ ) − e − a 4( α + i )( t − τ ) (36) and for different nodes, it becomes G ij ( t − τ ) = a π e − ˜ h ( α + i )( t − τ ) e − | R i − R j | 4( α + i )( t − τ ) 2( α + i )( t − τ ) . (37) When the wave propagation is dominated by the dipole interaction, the response function for the same node is G ii ( t − τ ) = π e − ˜ h ( α + i )( t − τ ) − + + a ( d/ 4) ( α + i ) ( t − τ ) + a ( d/ 4) ( α + i ) ( t − τ ) (38) and for different nodes it becomes G ij ( t − τ ) = a π e − ˜ h ( α + i )( t − τ ) ( d/ 4) ( α + i ) ( t − τ ) + | R i − R j | ( d/ 4) ( α + i ) ( t − τ ) \u0011 / . (39) Clearly, G ii (0) → and G ij (0) → , while G ii ( ∞ ) → and G ij ( ∞ ) → . Once the magnetization is expressed in the form of (35), we may compute the reservoir state X under the input U . Then, we may use the same method as in Learning with reservoir computing, and estimate the output ˆ Y . Similar to the micromagnetic simulations, we evaluate the performance by MC, IPC, and NARMA10 tasks. We may extend the analyzes for the higher-order terms in the expansion of (33). In Sec.B in Supplementary Materials, we show the second-order term m (2) ( x , t ) has only the z -component, and moreover, it is dependent only on the ﬁrst-order terms. As a result, the second-order term is expressed as m (2) z ( x , t ) = − ( m (1) x ) + ( m (1) y ) \u0001 (40) To compute the response functions, we linearize (31) for the complex magnetization m ( x , t ) ∂ t m ( x , t ) = L m + σ ( x , t ) , (41) where the linear operator is expressed as L = − ˜ h + ∆ ( α + i ) . (42) In the Fourier space, the linearized equation becomes ∂ t m k ( t ) = L k m k + σ k ( t ) , (43) with L k = − ˜ h + k \u0011 ( α + i ) . (44) The solution of ((43)) is obtained as m k ( t ) = dτe L k ( t − τ ) σ k ( τ ) . (45) We have N p cylindrical shape inputs with radius a and the i th input is located at R i . The input function is expressed as σ ( x ) = N p i = χ a ( | x − R i | ) . (46) We are interested in the magnetization at the input m i ( t ) = m ( x = R i , t ) , which is m i = ( π ) dτe − ˜ h ( α + i )( t − τ ) dke − k ( α + i )( t − τ ) e i k · ( R i − R j ) πaJ ( ka ) σ j ( t ) = a π dτe − ˜ h ( α + i )( t − τ ) dke − k ( α + i )( t − τ ) J ( k | R i − R j | ) J ( ka ) σ j ( t ) (47) For the same node, | R i − R j | = , and we may compute the integral explicitly as (36). When ka ≪ , we may assume J ( ka ) ≈ ka/ , and ﬁnally, come up with (37). When the thickness d of the material is thin, the dispersion relation becomes L k = − ˜ h ( α + i ) + k ˜ h \u0013 \u0012 + k ˜ h + βk ˜ h (48) where β = d . (49) We assume for k ≪ β ˜ h , then the linearized operator becomes L k = − ( α + i ) ˜ h + kd (50) leading to (38) and (39). Acknowledgements: S. M. thanks to CSRN at Tohoku University. Numerical simulations in this work were carried out in part by AI Bridging Cloud Infrastructure (ABCI) at National Institute of Advanced Industrial Science and Technology (AIST), and by the supercomputer system at the information initiative center, Hokkaido University, Sapporo, Japan. Funding: This work is support by JSPS KAKENHI grant numbers 21H04648, 21H05000 to S.M., by JST, PRESTO Grant Number JPMJPR22B2 to S.I., X-NICS, MEXT Grant Number JPJ011438 to S.M., and by JST FOREST Program Grant Number JPMJFR2140 to N.Y. Author Contributions S.M., N.Y., S.I. conceived the research. S.I., Y.K., N.Y. carried out simulations. N.Y., S.I. analyzed the results. N.Y., S.I., S.M. wrote the manuscript. All the authors discussed the results and analysis. Competing Interests The authors declare that they have no competing ﬁnancial interests. Data and materials availability: All data are available in the main text or the supplementary materials. Connection between the NARMA10 task and MC/IPC In this section, we discuss the necessary properties of reservoir computing to achieve high performance of the NARMA10 task. In short, the NARMA10 task is dominated by the memory of nine step previous data and second-order nonlinearity. We discuss these properties in two methods. The ﬁrst method is based on the extended Dynamic Mode Decomposition (DMD) ( ) and the higher-order DMD ( ). The second method is a regression of the input-output relationship. We will discuss the details of the two methods. Our results are consistent with previous studies; the requirement of memory was discussed in ( ), and the second-order nonlinear terms with a time delay in ( ). The NARMA10 task is based on the discrete differential equation, Y n + = αY n + βY n i = Y n − i + γU n U n − + δ. (51) Here, U n is an input at the time step n taken from the uniform random distribution U ( , . 5) , and Y n is an output. We choose the parameter as α = . , β = . , γ = . , and δ = . . In the ﬁrst method, we estimate the transition matrix A from the state variable Y n = ( Y , Y , . . . , Y n ) to Y n + = ( Y , Y , . . . , Y n + ) yielding Y n + = A · Y n . (52) We may extend the notion of the state variable to contain delayed data and polynomials of the output with time delay as Y n = ( Y n , Y n − , . . . , Y , Y n Y n , Y n Y n − , . . . , Y Y ) . (53) Including the delay terms following from the higher-order DMD ( ), while the polynomial nonlinear terms are used as a polynomial dictionary in the extended DMD ( ). Here, (53) contains all the combination of the second-order terms with time delay, Y n − i Y n − i with the integers i and i in ≤ i ≤ l ≤ n − . We may straightforwardly include higher-order terms in powers in (53). In the NARMA10 task, the output Y n + is also affected by the input U n . Therefore, the extended DMD is generalized to include the control as ( ) Y n + = ( A B ) · \u0012 Y n U n (54) where the state variable corresponding to the input includes time delay and nonlinearity, and is described as U n = ( U n , U n − , . . . , U , U n U n , U n U n − , . . . , U U ) . (55) We denote the generalized transition matrix as Ξ = ( A B ) . (56) The idea of DMD is to estimate the transition matrix from the data. This is done by taking pseudo inverse of the state variables as ˆ Ξ = Y k + · Y k U k \u0013 † (57) Here, M † is the pseudoinverse of the matrix M . This is nothing but a least-square estimation for the cost function of l.h.s minus r.h.s of (54). We may include the Tikhonov regularization term. Note that for the extended DMD ( ) and the higher-order DMD ( ), the transition matrix Ξ is further decomposed into characteristic modes associated with its eigenvalues. The decomposition gives us a dimensional reduction of the system. The estimation of the transition matrix is also called nonlinear system identiﬁcation, particularly, nonlinear autoregression with exogenous inputs (NARX). In this work, we focus on the estimation of the input-output relationship, and do not discuss the dimensional reduction. For time-series prediction, we estimate the function Y n + = f ( Y n , Y n − , . . . , Y ) , and we do not need the input U n in (54). Even in this case, we may consider a similar estimation of Ξ (in fact, A ). This estimation is the method used in the next-generation RC ( ). The second method is based on the Volterra series of the state variable Y n by the input U n . In this method, we assume that the state variable is independent of its initial condition. Then, we may express the state variable as Y n = G · U n . (58) Note that U n includes the input and its polynomials with a time delay as in (55). Similar to the ﬁrst method, we estimate G by ˆ G = Y t · U † t . (59) The estimated ˆ G gives us information on which time delay and nonlinearity dominate the state variable. delay NRMSE 0. 0. 0. 0. 1. test training linear (A) (B) (C) (D) (E) (F) delay NRMSE 0. 0. 0. 1. 0. second-order nonlinearity delay NRMSE 0. 0. 0. 1. 0. third-order nonlinearity delay NRMSE 0. 0. 0. 0. 1. linear delay NRMSE 0. 0. 0. 1. 0. second-order nonlinearity delay NRMSE 0. 0. 0. 1. 0. third-order nonlinearity Figure 9: (A-C) the estimation based on the extended DMD, (D-F) the estimation based on the Volterra series. The dictionary of each case is (A,D) ﬁrst-order (linear) delay terms, (B,E) up to second-order delay terms, and (C,F) up to third-order delay terms. The results of the two estimation methods are shown in Fig. 9. Both approaches suggest that memory of ≈ steps is enough to get high performance, and further memory does not improve the error. The second-order nonlinear term shows a reasonably small NRMSE of ≈ . . Including the third-order nonlinearity improves the error, but there is a sign of overﬁtting at a longer delay because the number of the state variables is too large. It should also be noted that even with the linear terms, the NRMSE becomes ≈ . . This result implies that although NRMSE ≈ . is often considered good performance, nonlinearity of the data is not learned at the error of this order. A. The MC and IPC tasks as Volterra series for linear and nonlinear readout In (3) and (4) in the main text, we show that the magnetization at the input region is expressed by the response function. The magnetization at the time t n corresponding to the input U n at the n step is expressed as m ( t n ) = a n U n + a n − U n − + · · · , (60) where the coefﬁcients a n can be computed from the response function. We ﬁrst consider the linear case, but we will generalize the expression for the nonlinear case. Because we use virtual nodes, the input U n at the step n continues during the time period t ∈ [ t n , t n + ) discretized by N v steps as ( t n, , t n, , . . . , t n,N v ) , and is multiplied by the ﬁlter of the binary noise (see Fig. and Methods in the main text). Therefore, the magnetization is expressed by the response functions G ( t − t ′ ) is formally expressed as m ( t n ) = N p [( G (0) + G ( θ ) + · · · G ( θ ( N v − 1))) σ i ( t n ) + ( G ( θN v ) + G ( θ ( N v + 1)) + · · · G ( θ ( N v − 1))) σ i ( t n − ) + · · · ] , (61) where σ i ( t n ) ∝ U n is the non-dimensionalized current injection at the time t n at the i th physical node, which is proportional to U n . Therefore, (61) results in the expression of (60). Our input is taken from a uniform random distribution. Therefore, the inner product of the reservoir state, which is nothing but magnetization, and (delayed) input to learn MC is ⟨ m ( t n ) , U n ⟩ = n = m ( t n ) U n = a n ⟨ U n ⟩ + O ( /T ) . (62) Similarly, the variance of the magnetization is equal to the variance of the input with the coefﬁcient associated with m ( t n ) . We may express the MC and IPC tasks in a matrix form as ˜ S ≈ W · G · ( S ◦ W in ) . (63) Here, S is the matrix associated with the original input, and ˜ S is the delayed one. The output weight is denoted by W , and W in is the matrix associated with the mask of binary noise. The goal of MC and IPC tasks is to approximate the delayed input ˜ S by the reservoir states G · S . Here, the reservoir states are expressed by the response function G and input denoted by S . We deﬁne delayed input ˜ S ∈ R K × T ˜ S = U n U n + U n + · · · U n − U n U n + · · · U n − U n − U n · · · ... ... ... ...  . (64) Here, T is the number of the time series, and K is the total length of the delay that we consider. The i th row shows the i − delayed time series. The input S ∈ R TN v × T to compute the reservoir states are expressed as S = σ ( t n ) σ ( t n + ) σ ( t n + ) · · · ... ... ... ... σ ( t n ) σ ( t n + ) σ ( t n + ) · · · σ ( t n − ) σ ( t n ) σ ( t n + ) · · · ... ... ... ... σ ( t n − ) σ ( t n − ) σ ( t n ) · · · ... ... ... ... (65) Note that σ ( t n ) ∝ U n upto constant. Due to time multiplexing, each row is repeated N v times, and then the time series is delayed in the next row. After multiplying the input ﬁlter W in , the input is fed into the response function. The input ﬁlter W in ∈ R TN v × T is a stack of constant row vectors with the length T . The N v different realizations of row vectors are taken from binary noise, and then the resulting N v × T matrix is repeated T times in the row direction. This input is multiplied by the coefﬁcients of the Volterra series G ∈ R N × TN v G = G (1) (0) · · · G (1) ( θ ( N v − 1)) G (1) ( θN v ) · · · G (1) ( θ ( N v − 1)) · · · G (2) (0) · · · G (2) ( θ ( N v − 1)) G (2) ( θN v ) · · · G (2) ( θ ( N v − 1)) · · · ... ... ... ... ... ... ... G ( N ) (0) · · · G ( N ) ( θ ( N v − 1)) G ( N ) ( θN v ) · · · G ( N ) ( θ ( N v − 1)) · · · (66) (63) implies that by choosing the appropriate W , we can get a canonical form of G . If the canonical form has N × N identity matrix in the left part of W · G , then the reservoir reproduces the time series up to N − delay. This means that the rank of the matrix G , or the number of independent rows, is the maximum number of steps of the delay. This is consistent with the known fact that MC is bounded by the number of independent components of reservoir variables ( ). Next we extend the Volterra series of the magnetization, including nonlinear terms. The magnetization is expressed as m ( t n ) = a n σ ( t n ) + a n − σ ( t n − ) + · · · + a n,n σ ( t n ) σ ( t n ) + a n,n − σ ( t n ) σ ( t n − ) + · · · . (67) The delayed input ˜ S is rewritten as ˜ S = U n U n + U n + · · · U n − U n U n + · · · ... ... ... ... U n U n U n + U n + U n + U n + · · · U n U n − U n + U n U n + U n + · · · ... ... ... ... (68) The matrix ˜ S contains all the nonlinear combinations of the input series ( U n , U n + , · · · ) . Accordingly, we should modify S and also G to include the nonlinear response functions. Note that to guarantee the orthogonality, Legendre polynomials (or other orthogonal polynomials) should be used instead of polynomials in powers. Nevertheless, up to the second order of nonlinearity, which is relevant to consider the performance of NARMA10 (see Sec. A), the difference is only in the constant terms ( P ( x ) = x − ). Because we subtract the mean value of the time series of all the input, output, and reservoir states, these constant terms do not change our conclusion. With nonlinear terms, (66) is extended as G = ( G lin , G nonl ) . Still, the rank of the matrix remains N at most. This is the reason why the total sum of IPC, including all the linear and nonlinear delays, is bounded by the number of independent reservoir variables. When G nonl = , the reservoir can memorize only the linear delay terms, but MC can be maximized to be N . On the other hand, when G nonl ̸ = , it is possible that MC is less than N , but the reservoir may have ﬁnite IPC. When the readout is nonlinear, we use the reservoir state variable as X = M ◦ M (69) where ◦ is the Hadamard product. If M is linear in the input, G has a structure of G = G lin G nonlin (70) In this case, rank( G ) = rank( G lin ) + rank( G nonlin ) . Learning with multiple variables In the main text, we use only m x for the readout as in (13)-(15). The readout is nonlinear and has both the information of m x and m x . In this section, we consider the linear readout, but use both m x and m z for the output in micromagnetic simulations. We begin with the linear readout only with m x . The results of the MC and IPC tasks are shown in Fig. 10(a,b). We obtain a similar performance for the MC task with the result in the main text (Fig. 3). On the other hand, the performance for the IPC task in Fig. 10(a) is signiﬁcantly poorer than the result in Fig. 3(a). This result demonstrates that the linear readout only with m x does not learn the nonlinearity effectively. Note that in the theoretical model with the response function, the IPC is exactly zero when we use the linear readout only with m x . The discrepancy arises from the expansion (33) around m = ( , , 1) in the main text. Strictly speaking, the expansion should be made around m under the constant input ⟨ σ ⟩ averaged over time at the input nanocontact. This reference state is inhomogeneous in space, and is hard to compute analytically. Due to this effect, m x in the micromagnetic simulations contain small nonlinearity. Next, we consider the linear readout with m x and m z . As seen in Fig. 10(c,d), m z carries nonlinear information, and enhances the IPC and learning performance of NARMA10 compared with linear readout only with m x (Fig. (a,b)). The performance is IPC ≈ under α = × − , which is comparable value with the results in the main text (Fig. 3(a,c)) where the readout is ( m x , m x ) . Also, high performance for NARMA10 task, NRMSE ≈ . , can be obtained using variables ( m x , m z ) . These results show that adding m z into the readout has a similar effect to adding m x . Similarity between m x and m z can be understood by using the theoretical formula with the response function in the main text. We continue the expansion (33) at the second order, and obtain ∂ t m (2) ( x , t ) = − m (1) × ∆ m (1) − α m (1) × ˜ h m (1) − ∆ m (1) \u0011 × e z + σ ( x , t ) m (1) × e y . (71) This result suggests that m (2) contains only the z component, and is slaved by m (1) , which does not have z component. Therefore, m (2) can be computed as m (2) z ( x , t ) = − ( m (1) x ) + ( m (1) y ) \u0001 (72) Because m x and m y carry similar information, m z in the readout has a similar effect with m in the readout. Speed of propagating spin wave using dipole interaction Propagating spin wave when magnetization is pointing along ﬁlm normal is called magnetostatic forward volume mode, and its dispersion relation can be described by the following equation ( ). ω ( k ) = γµ ( H − M s ) H − M s − e − kd (73) Then, one can obtain the group velocity at k ∼ as, v g = dω dk ( k = 0) = γµ M s d (74) In the magneto-static spin wave driven by dipole interaction, group velocity is proportional to both M s and d . v g ∼ m/s is obtained when the following parameters are used: µ H = 1. T, M s = 1. × A/m, d = nm. The same estimation is used for calculating the speed of information propagation for spin reservoirs in Refs. ( ) and ( ), which are used to plot Fig. in the main text. Details of reservoir computing scaling compared with literature In this section, details of Fig. shown in the main text are described. MC and NRMSE for NARMA10 tasks using photonic and spintronic RC are reported in Refs. ( 12,32–35,38, ) for photonic RC and ( 9,19,22,25,36,37,57, ) for spintronic RC. Table and shows reports of MC for photonic and spintronic RC with different length scales, which are plotted in Fig. in the main text. Table 1: Report of photonic RC with different length scales used in Fig. in the main text Reports Length, L Time interval, τ vτ Duport et al. ( ) 1. km µ s 2. km Dejonckheere et al. ( ) 1. km µ s 2. km Vincker et al. ( ) m 1. µ s m Takano et al. ( ) mm ps mm 1. Sugano et al. ( ) mm ps mm Note: speed of light, v = × m/s is used. Table 2: Report of spin reservoirs with different length scales used in Fig. in the main text Reports τ vτ Nakane et al. ( ) µ m ns 2. km/s 4. µ m Dale et al. ( ) nm ps m/s nm This work nm 1. ns m/s nm Note: v is calculated based on magneto-static spin wave using Eq. 74. Other data E. N v and N p dependence of performance Fig. shows N v and N p dependencies of MC, IPC and NRMSE for NARMA10 task. As N v and N p are increased, MC and IPC increase. Then, NARMA10 prediction task becomes better with increasing N v and N p . MC and NRMSE for NARMA10 with different N p with ﬁxed N v = are compared with other reservoirs shown in Fig. in the main text. E. exchange interaction In the main text, we use the dipole interaction to compute the response function as (38) and (39). In this section, we show the result using the exchange interaction shown in (36) and (37). Figure shows the results."
  },
  "sections_summary": {
    "Abstract": "[Skipped - Large PDF]",
    "5 MathAM-OIL, AIST, Sendai, 980-8577, Japan": "[Skipped - Large PDF]",
    "Introduction": "[Skipped - Large PDF]",
    "Experiments": "[Skipped - Large PDF]",
    "1 U k 2": "[Skipped - Large PDF]",
    "Discussion": "[Skipped - Large PDF]",
    "4 M 2 s eD J ( x , t ) M × ( M × m f )": "[Skipped - Large PDF]",
    "3 A 3": "[Skipped - Large PDF]",
    "4 M 2": "[Skipped - Large PDF]"
  },
  "tables": [
    {
      "page": 8,
      "table_index": 0,
      "content": [
        [
          null,
          "(cid:8)\n0 (cid:5)n,3 (cid:5)n,5 (cid:5)n,7"
        ],
        [
          "",
          "(cid:5)\nn"
        ],
        [
          "",
          null
        ],
        [
          "",
          null
        ]
      ]
    },
    {
      "page": 8,
      "table_index": 1,
      "content": [
        [
          ""
        ],
        [
          ""
        ]
      ]
    },
    {
      "page": 10,
      "table_index": 0,
      "content": [
        [
          "α = 5×10-4\nNon-linear, IPC\nLinear, MC"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 10,
      "table_index": 1,
      "content": [
        [
          "α = 5×10-4"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 10,
      "table_index": 2,
      "content": [
        [
          "Training, Test α = 5×10-4"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 12,
      "table_index": 0,
      "content": [
        [
          "α = 5 × 10-4\nC (cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16) (cid:17)(cid:18)(cid:19)(cid:20) (cid:21)(cid:22)(cid:23)\nNon-linear, IPC\nLinear, MC"
        ],
        [
          "No connection"
        ]
      ]
    },
    {
      "page": 12,
      "table_index": 1,
      "content": [
        [
          "α = 5 × 10-4\nK"
        ],
        [
          "= 0 )"
        ]
      ]
    },
    {
      "page": 12,
      "table_index": 2,
      "content": [
        [
          "Training, Test\n(cid:24) (cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31) !\"#$ %&’"
        ],
        [
          "No-connection"
        ]
      ]
    },
    {
      "page": 13,
      "table_index": 0,
      "content": [
        [
          "training",
          "prediction"
        ]
      ]
    },
    {
      "page": 13,
      "table_index": 1,
      "content": [
        [
          "",
          ""
        ]
      ]
    },
    {
      "page": 13,
      "table_index": 2,
      "content": [
        [
          "",
          ""
        ]
      ]
    },
    {
      "page": 49,
      "table_index": 0,
      "content": [
        [
          "α = 5×10-4\nMC\nIPC"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 49,
      "table_index": 1,
      "content": [
        [
          "Training, Test α = 5×10-4"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 49,
      "table_index": 2,
      "content": [
        [
          "α = 5×10-4\nIPC\nMC"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 49,
      "table_index": 3,
      "content": [
        [
          "Training, Test α = 5×10-4"
        ],
        [
          "α = 5×10-3"
        ],
        [
          "α = 5×10-2"
        ]
      ]
    },
    {
      "page": 50,
      "table_index": 0,
      "content": [
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ]
      ]
    },
    {
      "page": 50,
      "table_index": 1,
      "content": [
        [
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          ""
        ]
      ]
    },
    {
      "page": 50,
      "table_index": 2,
      "content": [
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ]
      ]
    },
    {
      "page": 50,
      "table_index": 3,
      "content": [
        [
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          ""
        ]
      ]
    },
    {
      "page": 50,
      "table_index": 4,
      "content": [
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ],
        [
          ""
        ]
      ]
    },
    {
      "page": 50,
      "table_index": 5,
      "content": [
        [
          "a = 5×10-4"
        ],
        [
          "a = 5×10-3"
        ],
        [
          "a = 5×10-2"
        ]
      ]
    }
  ],
  "images": [
    "processed/images/2301.02193v1_page5_img0.png",
    "processed/images/2301.02193v1_page8_img0.png",
    "processed/images/2301.02193v1_page8_img1.png",
    "processed/images/2301.02193v1_page8_img2.png",
    "processed/images/2301.02193v1_page8_img3.png",
    "processed/images/2301.02193v1_page8_img4.png",
    "processed/images/2301.02193v1_page12_img0.png",
    "processed/images/2301.02193v1_page12_img1.png",
    "processed/images/2301.02193v1_page17_img0.png",
    "processed/images/2301.02193v1_page17_img1.png",
    "processed/images/2301.02193v1_page17_img2.png",
    "processed/images/2301.02193v1_page17_img3.png",
    "processed/images/2301.02193v1_page17_img4.png",
    "processed/images/2301.02193v1_page17_img5.png",
    "processed/images/2301.02193v1_page17_img6.png",
    "processed/images/2301.02193v1_page17_img7.png",
    "processed/images/2301.02193v1_page17_img8.png",
    "processed/images/2301.02193v1_page17_img9.png",
    "processed/images/2301.02193v1_page17_img10.png",
    "processed/images/2301.02193v1_page17_img11.png",
    "processed/images/2301.02193v1_page17_img12.png",
    "processed/images/2301.02193v1_page17_img13.png",
    "processed/images/2301.02193v1_page17_img14.png",
    "processed/images/2301.02193v1_page17_img15.png",
    "processed/images/2301.02193v1_page17_img16.png",
    "processed/images/2301.02193v1_page17_img17.png",
    "processed/images/2301.02193v1_page17_img18.png",
    "processed/images/2301.02193v1_page17_img19.png",
    "processed/images/2301.02193v1_page17_img20.png",
    "processed/images/2301.02193v1_page17_img21.png",
    "processed/images/2301.02193v1_page17_img22.png",
    "processed/images/2301.02193v1_page17_img23.png"
  ],
  "status": "completed"
}