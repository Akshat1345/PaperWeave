{
  "metadata": {
    "title": "Trustworthy Transfer Learning: A Survey",
    "authors": [
      "Jun Wu",
      "Jingrui He"
    ],
    "abstract": "Transfer learning aims to transfer knowledge or information from a source domain to a relevant target domain. In this paper, we understand transfer learning from the perspectives of knowledge transferability and trustworthiness. This involves two research questions: How is knowledge transferability quantitatively measured and enhanced across domains? Can we trust the transferred knowledge in the transfer learning process? To answer these questions, this paper provides a comprehensive review of trustworthy transfer learning from various aspects, including problem definitions, theoretical analysis, empirical algorithms, and real-world applications. Specifically, we summarize recent theories and algorithms for understanding knowledge transferability under (within-domain) IID and non-IID assumptions. In addition to knowledge transferability, we review the impact of trustworthiness on transfer learning, e.g., whether the transferred knowledge is adversarially robust or algorithmically fair, how to transfer the knowledge under privacy-preserving constraints, etc. Beyond discussing the current advancements, we highlight the open questions and future directions for understanding transfer learning in a reliable and trustworthy manner.",
    "published": "",
    "arxiv_id": "2412.14116v1",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "http://arxiv.org/pdf/2412.14116v1",
    "pdf_file": "data/pdfs/2412.14116v1.pdf",
    "pdf_filename": "2412.14116v1.pdf"
  },
  "processing_info": {
    "processed_at": "2025-10-28T10:16:19.586221",
    "sections_found": 67,
    "tables_found": 21,
    "images_found": 51
  },
  "sections_text": {
    "Jun Wu": "wujun4@msu.edu Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA",
    "Jingrui He": "jingrui@illinois.edu School of Information Sciences, University of Illinois at Urbana-Champaign, Champaign, IL, USA",
    "Abstract": "Transfer learning aims to transfer knowledge or information from a source domain to a relevant target domain. In this paper, we understand transfer learning from the perspec- tives of knowledge transferability and trustworthiness. This involves two research questions: How is knowledge transferability quantitatively measured and enhanced across domains? Can we trust the transferred knowledge in the transfer learning process? To answer these questions, this paper provides a comprehensive review of trustworthy transfer learning from various aspects, including problem definitions, theoretical analysis, empirical algorithms, and real-world applications. Specifically, we summarize recent theories and algorithms for understanding knowledge transferability under (within-domain) IID and non-IID assump- tions. In addition to knowledge transferability, we review the impact of trustworthiness on transfer learning, e.g., whether the transferred knowledge is adversarially robust or algo- rithmically fair, how to transfer the knowledge under privacy-preserving constraints, etc. Beyond discussing the current advancements, we highlight the open questions and future directions for understanding transfer learning in a reliable and trustworthy manner 1 .",
    "1. Introduction": "Standard machine learning assumes that training and testing samples are independently and identically drawn (IID). With this IID assumption, modern machine learning models (e.g., deep neural networks (LeCun et al., 2015)) have achieved promising performance in a variety of high-impact applications. However, this IID assumption is often violated in real-world scenarios, especially when samples are collected from different sources and environments (Pan & Yang, 2010; Wu et al., 2024). Transfer learning has been introduced to tackle the distribution shifts between training (source domain) and testing (target domain) data sets. In contrast to standard machine learning involving samples from a single domain, transfer learning focuses on modeling heterogeneous data collected from different domains. The intuition behind transfer learning is to bridge the gap between source and target data by discovering and transferring their shared knowledge (Pan & Yang, 2010). Compared to learning from the target domain alone, the transferred knowledge could significantly improve the prediction performance on the target domain, especially when the target domain has limited or no labeled data (Ben-David et al., 2010; Tripuraneni et al., 2020). In recent 1. This work was mainly completed when Jun Wu was a PhD student at UIUC. 1",
    "Q2: Knowledge Trustworthiness": "Figure 1: A motivating example of trustworthy transfer learning decades, by instantiating the learning models with modern neural networks, a deep transfer learning paradigm has been introduced with enhanced transferability capabilities (Yosinski et al., 2014). As illustrated in (Pan & Yang, 2010), transfer learning is a general term to describe the transfer of knowledge or information from source to target domains. Depending on the data and model assumptions, it can lead to various specific problem settings, such as data-level knowledge transfer (domain adaptation (Ben-David et al., 2010; Ganin et al., 2016; Mansour et al., 2009a), out-of-distribution generalization (Blanchard et al., 2011; Muandet et al., 2013), and self-taught learning (Raina et al., 2007)) with available source samples and model-level knowledge transfer (fine-tuning (Shachaf et al., 2021), source-free adaptation (Liang et al., 2020a; Aghbalou & Staerman, 2023), knowledge distillation (Hin- ton et al., 2015)) with a pre-trained source hypothesis. The generalization performance of transfer learning techniques under various data and model assumptions has been studied over the past decades (Tripuraneni et al., 2020; Zhao et al., 2019b; Minami et al., 2023; Mohri et al., 2019). In addition to generalization performance, it is crucial to understand the trustworthiness (Eshete, 2021) of the transferred knowledge in the transfer learning process, especially in safety-critical applications such as self-driving cars and medical diagnosis. It is explained (Varshney, 2022) that â€œtrust is the relationship between a trustor and a trustee: the trustor trusts the trusteeâ€. In the context of transfer learning, the trustor can be the owners/users/regulators of either source or target domain. The trustee can be the transfer learning model itself, or the knowledge transferred from the source domain to the target domain. As summarized in earlier studies (Eshete, 2021; Varshney, 2022; Kaur et al., 2023), various trustworthiness properties can encourage the â€œtrustorâ€ to trust the â€œtrusteeâ€ in real scenarios, including adversarial robustness, privacy, fairness, transparency, etc. Therefore, in this paper, we focus on trustworthy transfer learning (Wu & He, 2023a) that aims to understand transfer learning from the perspective of both knowledge transferability and knowledge trustworthiness. Figure 1 provides a motivating example of trustworthy transfer learning in precision agri- culture (Adve et al., 2024). In this example, a target farmer aims to train a model over the collected sorghum data. The task is to predict the biochemical traits (e.g., Nitrogen content, 2 chlorophyll, etc.) of sorghum samples using the leaf hyperspectral reflectance (Wang et al., 2023b; Wu et al., 2022). Nevertheless, it is expensive and time-consuming to collect the labeled training samples. A feasible solution is to leverage knowledge from a relevant maize data set collected by a source farmer. This transfer learning process might involve several trustworthy concerns from source and target farmers. To name a few, will the privacy of source data be leaked in transfer learning? How does the poisoned and biased source knowledge negatively affect the prediction performance on the target domain? What is the fundamental trade-off between transfer performance and trustworthy properties? More generally, from the perspective of data and AI model markets (Pei et al., 2023), this em- phasizes the importance of establishing trustworthiness between customers and sellers when purchasing AI models and sharing personal data. This survey provides a comprehensive review of state-of-the-art theoretical analysis and algorithms for trustworthy transfer learning. More specifically, we summarize recent theo- ries and algorithms for understanding knowledge transferability from two aspects: IID and non-IID transferability. IID transferability assumes that the samples within each domain are independent and identically distributed. In this scenario, we review three major quan- titative metrics for evaluating the transferability across domains, including (data-level) distribution discrepancy , (task-level) take diversity , and (model-level) transferability esti- mation . In contrast, non-IID transferability considers a more relaxed assumption that the samples within each domain can be interdependent, e.g., connected nodes in graphs (Kipf & Welling, 2017), word occurrence in texts (Lee et al., 2018), temporal observations in time series (Purushotham et al., 2017), etc. We then review how transferability across domains can be quantitatively measured and enhanced in these complex scenarios. In addition to knowledge transferability, we also review the impact of trustworthiness on transfer learning techniques, including privacy, adversarial robustness, fairness, transparency, etc. Finally, we will highlight the open questions and future directions of trustworthy transfer learning. The rest of this paper is organized as follows. Section 2 presents the main notation and the general problem definition of trustworthy transfer learning. Section 3 and Section 4 summarize the knowledge transferability and trustworthiness in various transfer learning scenarios, respectively. Section 5 provides the applications of transfer learning techniques in real-world applications, and Section 6 summarizes the open questions and future trends of trustworthy transfer learning. Finally, we conclude this survey in Section 7.",
    "2. Preliminaries": "In this section, we provide the main notation and general problem definition of trustworthy transfer learning.",
    "Notation": "In the paper, we let X and Y denote the input space and output space, respectively. Given a source domain D S and a target domain D T , we denote the probability density (or mass) functions of the source and target domains as p S and p T (or P S and P T ) over X Ã— Y , respectively. In the context of deep transfer learning, a hypothesis function f : X â†’Y can often be decomposed into two components: a feature extraction function g : X â†’ R d and a prediction function h : R d â†’Y . We let F be the class of hypothesis functions (with 3",
    "Uncertainty Quantification": "Uncertainty quantification is essential for decision-making and optimization in machine learning and artificial intelligence (Naeini et al., 2015). For example, high-stakes applica- tions such as medical diagnostics (Begoli et al., 2019) and autonomous driving (Michelmore et al., 2020) require both accurate class predictions and quantification of prediction uncer- tainty. Generally, there are two types of prediction uncertainty (HÂ¨ullermeier & Waegeman, 2021): aleatoric (data) uncertainty involving the inherent randomness and variability in the data, and epistemic (model) uncertainty caused by a lack of knowledge about the opti- mal model parameters. These uncertainties can be formally explained using the Bayesian posterior distribution (Chan et al., 2020). Definition 10 (Aleatoric and Epistemic Uncertainty (Chan et al., 2020)) Given a model f with parameter Î¸ and a test sample x âˆ— , the Bayesian posterior distribution over x âˆ— can be formulated as: p ( y âˆ— | x âˆ— , D ) | {z } Total uncertainty = Z p ( y âˆ— | x âˆ— , f ) | {z } Aleatoric uncertainty Â· p ( f | D ) | {z } Epistemic uncertainty df (23) where D denotes the set of training samples. The calibration of uncertainty estimates is vital in determining the trustworthiness of model outputs. A well-calibrated model should provide accurate predictions when it is confident and indicate high uncertainty when it is likely to be incorrect. Thus, calibration can be considered as an orthogonal metric for accuracy when evaluating machine learning systems. In particular, Snoek et al. (2019) conduct a systematic evaluation of traditional uncertainty quantification models under distribution shifts. They observe that the quality of uncertainty consistently degrades with increasing distribution shifts between source and target domains. To solve this problem, various frameworks have been proposed to improve uncertainty quantification under distribution shifts across domains. (1) Temperature Scaling: Park et al. (2020) derive an upper bound on the expected cali- bration error in the target domain in terms of the importance-weighted classification error and the error of a domain discriminator. Building on the idea of temperature scaling (Guo et al., 2017), they propose a calibration algorithm by minimizing the up- per bound over source and target samples. Similarly, Wang et al. (2020) develop an adaptive importance weighting approach with lower bias and variance of the estimated calibration errors to improve the uncertainty quantification under the covariate shift as- sumption. Instead, Zou et al. (2023) focus on learning two calibration functions based on a real in-distribution calibration set and a synthetic out-of-distribution calibration set respectively, and then adaptively combine the two calibrators. Hu et al. (2024) optimize the calibration objective function (i.e., temperature scaling optimization (Guo et al., 2017)) using a labeled pseudo-target set created via mixup (Zhang et al., 2018) over pseudo-labeled target samples. 38 (2) Conformal Prediction: The model predicts a set of labels instead of a single label (Ro- mano et al., 2020; Lei et al., 2018; Angelopoulos & Bates, 2021). Assuming the true importance weights (e.g., w âˆ— ( x, y ) = p T ( x, y ) /p S ( x, y )) are known, Tibshirani et al. (2019) and Podkopaev and Ramdas (2021) study the weighted conformal predictions under covariate shifts and label shifts, respectively. Based on jackknife+ (Barber et al., 2021), Prinster et al. (2022, 2023) formulate the sampling weighted jackknife+ predic- tion interval to handle covariate shifts with finite-sample coverage guarantee. Cauchois et al. (2024) design prediction sets that are robust against all distribution shifts with bounded f -divergence. Gibbs and Cand`es (2021, 2024) further investigate prediction sets in an online setting where the data distribution can shift continuously over time. In addition, Park et al. (2022) construct probably approximately correct (PAC) prediction sets under bounded covariate shifts in the scenarios with known importance weights and an uncertainty set of possible importance weights. The follow-up work (Si et al., 2024) constructs prediction sets with PAC guarantees in the presence of label shifts. The crucial idea is to compute confidence intervals of importance weights (Lipton et al., 2018) through Gaussian elimination. (3) Bayesian Learning: Chan et al. (2020) develop an approximate Bayesian inference approach based on posterior regularization that captures the distribution difference be- tween source and target domains. Zhou and Levine (2021) study the uncertainty quan- tification problem in the context of test-time adaptation. They develop a probabilistic graphical model for covariate shift scenarios, followed by an instantiated ensemble ap- proach to estimate the uncertainty of trained models over test samples. In addition, transferable Gaussian processes (Yu & Chu, 2007; Bonilla et al., 2007; Cao et al., 2010; Maddox et al., 2021; Wu et al., 2022) can be applied to model uncertainty in the target domain by leveraging knowledge from the source domain.",
    "Transfer": "Figure 11: Transferability of group fairness across domains. (a) The target domain has labeled samples with sensitive attributes. (b) The target domain has unlabeled samples with sensitive attributes. (c) The target domain has only unlabeled samples without sensitive attributes. Fair transfer learning that integrates the aforementioned group fairness criteria has been studied in recent years (Giguere et al., 2022; Dutt et al., 2024). For example, Zemel et al. (2013) and Madras et al. (2018) propose learning fair intermediate representations by encoding the data as accurately as possible while obscuring information about sensitive attributes. They demonstrate the transferability of these fair representations across dif- ferent tasks. Later, Schrouff et al. (2022) empirically investigate the connections between compound distribution shifts (e.g., the co-occurrence of demographic, covariate, and label shifts) and fairness transfer in real-world medical applications via a joint causal frame- work (Mooij et al., 2020). Furthermore, Chen et al. (2022) provide a generic Lipshitz upper bound for group fairness when the underlying distribution shifts (e.g., covariate shift or label shift between source and target domains) are constrained. Specifically, most existing works (Coston et al., 2019; Biswas & Mukherjee, 2021; Zhao et al., 2024) dive into under- standing the transferability of fairness, by considering various learning scenarios based on the availability of class labels and sensitive attribute information in the target domain. Fig- ure 11 illustrates three scenarios for group fairness transfer when training data are available in the target domain. Another related scenario is the domain generalization (Pham et al., 2023) where no target samples are available during training. (1) Labeled target samples with sensitive attributes: Assuming that the target domain con- tains a few labeled samples with sensitive attributes, Schumann et al. (2019) provide the generalization error bounds of group fairness (e.g., equality of opportunity, and equalized odds) in the target domain in terms of fairness-aware distribution discrep- ancy between source and target domains. Oneto et al. (2020b, 2020a) theoretically show the generalization error bound of group fairness (e.g., demographic parity) across domains from the perspective of multi-task learning via low-rank matrix factorization or parameter decoupling. Similarly, Slack et al. (2020) propose a fair meta-learning algorithm to transfer the fairness across domains. 34 (2) Unlabeled target samples with sensitive attributes: When the target domain has only unlabeled samples with sensitive attributes, Rezaei et al. (2021) propose minimizing both the expected log-loss and the pseudo-label aware fairness penalty over the worst- case approximation of the target distribution to mitigate covariate shifts across domains and ensure fairness (e.g., demographic parity, equality of opportunity, and equalized odds) in the target domain. Havaldar et al. (2024) leverage representation matching across sensitive groups to enforce fairness and sample reweighting to mitigate covariate shifts across domains. Inspired by the theory of self-training (Wei et al., 2021a; Cai et al., 2021b), An et al. (2022) theoretically analyze the transferability of group fairness across domains based on the consistency loss of a machine learning model under input transformations. Then they propose a self-training algorithm with fair consistency regularization to improve fairness transfer in the presence of subpopulation shifts. In contrast, Roh et al. (2023) formalize the notion of correlation shift over labels and sensitive attributes and employ a weighted sampling strategy in data preprocessing to mitigate correlation shifts across domains. (3) Only unlabeled target samples with missing sensitive attributes: Coston et al. (2019) study a more general learning scenario where the target domain is associated with only unlabeled samples with missing sensitive attributes. To improve group fairness (e.g., demographic parity) in the target domain, they develop fairness-guided sample reweighting approaches by enforcing the similarity of group-wise weighting scores across all pairs of groups. (4) No target samples: An extreme situation occurs when no target samples are avail- able, commonly referred to as domain generalization or out-of-distribution generaliza- tion (Blanchard et al., 2011; Gulrajani & Lopez-Paz, 2021). In this scenario, only source domain data is provided to learn a fair predictor for unseen target domains. To solve this problem, Singh et al. (2021) develop a causal inference framework to minimize the worst-case prediction error under group fairness constraints. Similarly, Mandal et al. (2020) focus on optimizing a fair predictor by minimizing the worst-case error across weighted combinations of the training data. Later, Pham et al. (2023) theoretically derive the upper bounds on generalization error and unfairness in the target domain in terms of source error/unfairness, the domain discrepancy among source domains, and the domain discrepancy between source and unseen target domains. Motivated by this theoretical analysis, they propose an invariant representation learning algorithm to improve the transfer of fairness and accuracy via density matching.",
    "Problem Definition": "Transfer learning (Pan & Yang, 2010) refers to the knowledge or information transfer from the source domain to the target domain such that the prediction performance on the target domain could be significantly improved as compared to learning from the target domain alone. Moreover, in the following definition, we generalize standard transfer learning (Pan & Yang, 2010) to trustworthy transfer learning (Wu & He, 2023a). Definition 1 (Trustworthy Transfer Learning) Given a source domain D S and a tar- get domain D T , trustworthy transfer learning aims at improving the generalization and trust- worthiness of a learning algorithm f ( Â· ) on the target domain, by leveraging latent knowledge from the source domain. The source and target domains might involve different learning tasks (Pan & Yang, 2010; Tripuraneni et al., 2020) or data modalities (Shen et al., 2023; Bugliarello et al., 4 2022). There are two key components in trustworthy transfer learning: knowledge transfer- ability and trustworthiness. Specifically, knowledge transferability measures how the source knowledge can be successfully transferred to the target domain. In contrast, knowledge trustworthiness aims to answer whether transfer learning techniques provide reliable and trustworthy results in the target domain. Figure 2 provides a brief summarization of trust- worthy transfer learning regarding knowledge transferability and trustworthiness (discussed in Section 3 and Section 4).",
    "3. Knowledge Transferability": "This section summarizes the knowledge transferability in different scenarios.",
    "IID Transferability": "Here we summarize different transferability indicators, including distribution discrepancy, task diversity, and transferability estimator.",
    "Distribution Discrepancy": "Distribution discrepancy quantitatively measures the distribution shifts between two do- mains in the distribution space, when the source and target domains share the same input and output spaces (this scenario is also known as domain adaptation (Pan & Yang, 2010)). There are different types of distribution shifts (Wiles et al., 2022), including covariate shift (Shimodaira, 2000) (feedback covariate shift (Fannjiang et al., 2022; Prinster et al., 2023)), label/target shift (Lipton et al., 2018; Zhang et al., 2013), concept shift (Redko et al., 2019), etc. The covariate shift holds that the conditional probability p ( y | x ) is shared across domains, but the marginal p ( x ) is different. Label shift assumes that the conditional prob- ability p ( x | y ) is shared across domains, while the marginal label distribution p ( y ) changes. Concept shift involves changes in the conditional probability p ( x | y ) (or p ( y | x )) changes, while the marginal distribution p ( y ) (or p ( x )) is fixed. The integral probability metric (IPM) (MÂ¨uller, 1997; Sriperumbudur et al., 2010; Zhang et al., 2012) is a general frame- work for quantifying the difference between two distributions, and it can be instantiated by various statistical discrepancy measures (Sriperumbudur et al., 2010), e.g., total variation distance, Wasserstein distance, maximum mean discrepancy (Gretton et al., 2012), etc. Definition 2 (Integral Probability Metric (MÂ¨uller, 1997)) Let P S and P T be the prob- ability distributions of the source D S and target D T domains, respectively. The integral probability metric between P S and P T is defined as: d IPM ( D S , D T ) = sup h âˆˆH Z M h d P S âˆ’ Z M h d P T (1) where M is a measurable space and H is a class of real-valued bounded measurable functions on M . The concept of distribution discrepancy is the key to theoretically understanding how knowledge can be transferred from source to target domains. For example, the seminal 5 work of Ben-David et al. (2010) derives a generalization bound for domain adaptation using a tractable H âˆ† H -divergence. Many follow-up works have developed refined generalization bounds by introducing various discrepancy measures. The following theorem provides a unified view of such generalization bounds based on a notion of discrepancy d ( D S , D T ). Theorem 1 (Unified Generalization Bound) Let H denote the hypothesis space, and E S ( h ) , E T ( h ) be the expected prediction error of a hypothesis h âˆˆH on the source and target domains, respectively. d ( Â· , Â· ) measures the difference between source and target distribution probabilities (see more instantiations below). Then for any hypothesis h âˆˆH , we can have a unified view of generalization error in the target domain: E T ( h ) â‰¤E S ( h ) + d ( D S , D T ) + â„¦ where â„¦ represents the redundant terms (depending on how d ( D S , D T ) is instantiated), e.g., the difference of labeling functions across domains (Ben-David et al., 2010; Acuna et al., 2021), the complexity of hypothesis space H (Mansour et al., 2009a; Zhang et al., 2012), number of training samples (Ben-David et al., 2010; Redko et al., 2017), etc. We have the following observations regarding this unified view of generalization error. (1) The complexity of the class of hypothesis functions H plays a crucial role in deriving tight generalization error bounds. Various metrics have been applied to quantify this complex- ity (Redko et al., 2019), including the Vapnik-Chervonenkis (VC) dimension (Ben-David et al., 2006; Blitzer et al., 2007; Ben-David et al., 2010; Peng et al., 2019), Rademacher complexities (Acuna et al., 2021; Ghifary et al., 2016; Zhang et al., 2019; Mansour et al., 2009a; Mohri & MuËœnoz Medina, 2012), covering number (Zhang et al., 2019, 2012), etc. We refer the reader to the survey (Redko et al., 2019) for more discussion. (2) Generally, the discrepancy d ( D S , D T ) measures the difference between source and target distributions over the joint space X Ã— Y , when the distribution shifts occur across domains. In prac- tice, it is commonly seen that the discrepancy d ( D S , D T ) is defined over the input space X (when no label information is available in unsupervised domain adaptation), or over the joint space X Ã— Y (when (pseudo)labels for target samples are available). The first type of d ( D S , D T ), defined over the input space X , is often associated with the covariate shift assumption (Shimodaira, 2000) or the redundant term indicating the difference of labeling functions across domains, when deriving the generalization error bound. In the following, we summarize several commonly used discrepancy metrics d ( D S , D T ). â€¢ Total Variation Distance (Ben-David et al., 2006): The total variation distance (also referred to as L 1 divergence) between source and target domains can be defined as d TV ( D S , D T ) = sup B âˆˆB | P S [ B ] âˆ’ P T [ B ] | (2) where B is the set of measurable subsets under P S and P T . It is shown (Sriperumbudur et al., 2010) that the total variation distance can be considered as a special case of the integral probability metric. â€¢ H âˆ† H -divergence (Blitzer et al., 2007; Ben-David et al., 2006): It is illustrated (Ben- David et al., 2006; Ben-David et al., 2010) that the empirical estimate of the total 6 variation distance in Eq. (2) has two limitations. First, it cannot be accurately es- timated from finite samples of arbitrary distributions in practice. Second, it results in loose generalization bounds due to involving a supremum over all measurable sub- sets. To address these limitations, Ben-David et al. (2006) and Blitzer et al. (2007) introduce the following H âˆ† H -divergence: d H âˆ† H ( D S , D T ) = sup h,h â€² âˆˆH P S \u0002 h ( x ) Ì¸ = h â€² ( x ) \u0003 âˆ’ P T \u0002 h ( x ) Ì¸ = h â€² ( x ) \u0003 (3) It can be seen that this distribution difference is defined over the hypothesis-dependent subsets { I [ h ( x ) Ì¸ = h â€² ( x )] | h, h â€² âˆˆH} . â€¢ Discrepancy Distance (Mansour et al., 2009a): Mansour et al. (2009a) extend the H âˆ† H -divergence to a more general discrepancy distance for measuring distribution differences. d disc ( D S , D T ) = max h,h â€² âˆˆH E x âˆ¼ P S \u0002 â„“ \u0000 h ( x ) , h â€² ( x ) \u0001\u0003 âˆ’ E x âˆ¼ P T \u0002 â„“ \u0000 h ( x ) , h â€² ( x ) \u0001\u0003 (4) where â„“ ( Â· , Â· ) denotes a general loss function (though the derived generalization bounds require that â„“ ( Â· , Â· ) is symmetric and obeys the triangle inequality). When using 0- 1 classification loss, this discrepancy distance exactly recovers the H âˆ† H -divergence. The discrepancy distance can be flexibly applied to compare distributions across var- ious tasks, e.g., regression (Mansour et al., 2009a; Cortes & Mohri, 2011). â€¢ Y -discrepancy (Mohri & MuËœnoz Medina, 2012): It is notable that the discrepancy distance in Eq. (4) quantifies the difference between two marginal distributions over X , when the ground-truth labeling function is unknown in the target domain. Later, Mohri and MuËœnoz Medina (2012) further extend the discrepancy distance to the Y - discrepancy, which is defined over X Ã— Y as follows. d Y ( D S , D T ) = sup h âˆˆH E ( x,y ) âˆ¼ P S [ â„“ ( h ( x ) , y )] âˆ’ E ( x,y ) âˆ¼ P T [ â„“ ( h ( x ) , y )] (5) In practice, this discrepancy can be estimated using pseudo labels of the target data, when there are no labeled data in the target domain (Long et al., 2018; Courty et al., 2017). â€¢ Margin Disparity Discrepancy (Zhang et al., 2019): Zhang et al. (2019) extend the notion of discrepancy distance in Eq. (4) to margin disparity discrepancy (MDD) in multi-class classification settings. Specifically, MDD involves two key refinements: (1) the use of a margin-based loss function, and (2) the formulation of the discrepancy over both a hypothesis space H and a specific classifier h . d MDD ( D S , D T ) = sup h â€² âˆˆH | E x âˆ¼ P S [Î¦ Ï ( Ï h â€² ( x, h ( x )))] âˆ’ E x âˆ¼ P T [Î¦ Ï ( Ï h â€² ( x, h ( x )))] | (6) where the function Ï h â€² ( Â· , Â· ) defines the margin of a hypothesis h â€² , and the function Î¦ Ï ( Â· ) defines the margin-based loss over a threshold Ï > 0. 7 â€¢ f -divergence (Acuna et al., 2021): Building on the margin disparity discrepancy (MDD) (Zhang et al., 2019), Acuna et al. (2021) further develop a generic notion of the discrepancy based on the variational characterization of f -divergence (Nguyen et al., 2010). Specifically, the f -divergence is bounded by d f ( D S , D T ) = Z p T ( x ) Ï• \u0012 p S ( x ) p T ( x ) \u0013 dx â‰¥ sup T âˆˆT E x âˆ¼ P S [ T ( x )] âˆ’ E x âˆ¼ P T [ Ï• âˆ— ( T ( x ))] where Ï• ( Â· ) is a convex lower semi-continuous function that satisfies Ï• (1) = 0. Ï• âˆ— is the conjugate function of Ï• . T is a set of measurable functions. Based on this observation, Acuna et al. (2021) define a notion of f -divergence guided discrepancy as follows. d Ï• ( D S , D T ) = sup h â€² âˆˆH E x âˆ¼ P S \u0002 â„“ \u0000 h ( x ) , h â€² ( x ) \u0001\u0003 âˆ’ E x âˆ¼ P T \u0002 Ï• âˆ— \u0000 â„“ \u0000 h ( x ) , h â€² ( x ) \u0001\u0001\u0003 (7) The flexibility in choosing Ï• âˆ— enables f -divergence to recover many popular statistical divergences, e.g., Jensen-Shannon (JS) divergence, Kullback-Leibler (KL) divergence, Reverse KL (KL-rev) divergence, Pearson Ï‡ 2 divergence, etc. As a result, different choices of Ï• âˆ— can define various discrepancies from Eq. (7). Besides, it is shown that the notion of discrepancy in Eq. (7) can also recover MDD. â€¢ Generalized Discrepancy (Cortes et al., 2015): In contrast, Cortes et al. (2015) gen- eralize the discrepancy distance in Eq. (4) using reweighting techniques. That is, the difference between two distributions can be adjusted by multiplying the loss for each training example by a non-negative weight (Huang et al., 2006; Cortes et al., 2008; Zhang et al., 2013). Formally, for any hypothesis-dependent reweighting function U h , the generalized discrepancy is defined as follows. d DISC ( D S , D T ) = max h âˆˆH ,h â€²â€² âˆˆH â€²â€² E x âˆ¼ Ë†P S [ U h ( x ) Â· â„“ ( h ( x ) , f S ( x ))] âˆ’ E x âˆ¼ Ë†P T \u0002 â„“ \u0000 h ( x ) , h â€²â€² ( x ) \u0001\u0003 (8) where Ë† P S , Ë† P T denotes the empirical distributions of source and target domains, re- spectively, and f S denotes the source labeling function. â€¢ RÂ´enyi Divergence (Cortes et al., 2010; Mansour et al., 2009b): Furthermore, Man- sour et al. (2009b) and Cortes et al. (2010) derive the generalization bounds for adaptation approaches based on importance reweighting (e.g., sample reweighting for single-source adaptation (Cortes et al., 2010) and domain reweighting for multi-source adaptation (Mansour et al., 2009b)) based on the following RÂ´enyi divergence (RÂ´enyi, 1961). d Î± ( D T ||D S ) = 1 Î± âˆ’ 1 log X x âˆˆX P T ( x ) \u0012 P T ( x ) P S ( x ) \u0013 Î± âˆ’ 1 (9) where Î± â‰¥ 0. â€¢ Wasserstein Distance (Shen et al., 2018): In general, fro any p â‰¥ 1, the p -Wasserstein distance between two distributions can be defined as follows. d W p ( D S , D T ) = \u0012 inf Ï€ âˆˆ Î (P S , P T ) Z c ( x, x â€² ) p dÏ€ ( x, x â€² ) \u0013 1 /p (10) 8 where Î (P S , P T ) is the set of all measures over X Ã— X with marginals P S and P T , and c ( Â· , Â· ) is a distance function. When p = 1, 1-Wasserstein distance (also known as earth moverâ€™s distance) is one special case of the integral probability metric with H = { h : || h || L â‰¤ 1 } . Specifically, based on the Kantorovich-Rubinstein duality (Dudley, 2002), it holds that d W 1 ( D S , D T ) = inf Ï€ âˆˆ Î (P S , P T ) Z c ( x, x â€² ) dÏ€ ( x, x â€² ) = sup || h || L â‰¤ 1 E P S [ h ( x )] âˆ’ E P T [ h ( x )] (11) where || h || L = sup x Ì¸ = x â€² | h ( x ) âˆ’ h ( x â€² ) | /c ( x, x â€² ). This enables a practical empirical esti- mation of the 1-Wasserstein distance using gradient descent optimization (Arjovsky et al., 2017). Therefore, Shen et al. (2018) and Redko et al. (2017) apply the 1- Wasserstein distance to analyze distribution shifts between source and target domains. â€¢ Maximum Mean Discrepancy (Tzeng et al., 2014; Long et al., 2015): Tzeng et al. (2014) and Long et al. (2015) leverage the maximum discrepancy discrepancy (MMD) (Gretton et al., 2012) to measure the distribution difference between source and target domains. MMD can be considered as another special case of the integral probability metric by instantiating the hypothesis space with a unit ball in a reproducing kernel Hilbert space associated with kernel k ( Â· , Â· ). Given a kernel function k ( Â· , Â· ), the MMD between source and target distribution can be defined as: d MMD ( D S , D T ) = E x S ,x â€² S âˆ¼ P S \u0002 k \u0000 x S , x â€² S \u0001 \u0003 âˆ’ 2 E x S âˆ¼ P S ,x T âˆ¼ P T \u0002 k ( x S , x T ) \u0003 + E x T ,x â€² T âˆ¼ P T \u0002 k \u0000 x T , x â€² T \u0001 \u0003 (12) Redko et al. (2019) further show the generalization error bounds based on MMD. â€¢ Cauchy-Schwarz Divergence (Yin et al., 2024): Recently, Yin et al. (2024) use the Cauchy-Schwarz (CS) divergence (Principe, 2010) to theoretically understand the knowledge transferability across domains. d CS ( D S , D T ) = âˆ’ log \u0000R p S ( x ) p T ( x ) dx \u0001 2 R p 2 S ( x ) dx Â· R p 2 T ( x ) dx ! (13) It is shown (Yin et al., 2024) that this CS divergence can lead to tighter generalization error bounds than KL divergence (Nguyen et al., 2022b). Besides, the empirical estimate of the CS divergence is closely related to MMD (Gretton et al., 2012). From the perspective of empirical estimation, the discrepancy measures can be broadly cat- egorized into two groups. The first group includes statistical discrepancy measures (Nguyen et al., 2022b; Sun & Saenko, 2016; Chen et al., 2021), such as Maximum Mean Discrepancy (MMD) (Tzeng et al., 2014; Long et al., 2015) and Wasserstein distance (Courty et al., 2016; Redko et al., 2017; Fatras et al., 2021), which can be directly estimated from finite samples. The second group is based on adversarial learning (Ganin et al., 2016; Saito et al., 2018; Tzeng et al., 2017; Hoffman et al., 2018b; Zhang et al., 2019; Acuna et al., 2021), which requires an additional neural network to optimize an adversarial objective. More recently, Kashyap et al. (2021) and Yuan et al. (2022b) provide empirical comparisons of 9 various discrepancy measures in natural language processing and computer vision tasks. It is noted that when there are no labeled samples in the target domain, one common strategy in designing practical domain adaptation algorithms is to minimize the discrepancy across domains over X . However, it has been shown (Ben-David et al., 2010; Zhao et al., 2019b; Wu et al., 2019; Johansson et al., 2019) that exact marginal distribution matching might lead to negative transfer in practice. The notion of distribution discrepancy has been applied to understand knowledge trans- ferability in various realistic adaptation scenarios, including single-source adaptation (Cortes et al., 2010; Ben-David et al., 2010; Zhang et al., 2019; Acuna et al., 2021; Nguyen et al., 2022b), multi-source adaptation (Mansour et al., 2021; Hoffman et al., 2018a; Wu et al., 2024), open-set adaptation (Fang et al., 2020; He et al., 2023), domain generalization (Muan- det et al., 2013; Blanchard et al., 2011) (also known as out-of-distribution generalization), privacy-preserving federated adaptation (Peng et al., 2020), dynamic adaptation (Kumar et al., 2020; Wu & He, 2022), etc.",
    "Task Diversity": "Task diversity (Tripuraneni et al., 2020; Watkins et al., 2023) is another tool for theoretically understanding the performance of transfer learning. It enables a relaxed data assumption that the source and target domains can have different output spaces, i.e., each domain can be associated with a different learning task (Pan & Yang, 2010). In the context of transfer learning, it assumes that a generic nonlinear feature representation function is shared across all tasks. Then each task is associated with a shared representation learning function and a task-specific prediction function. In (Tripuraneni et al., 2020), task diversity is defined to characterize how the worst-case representation difference can be controlled when the task-averaged representation difference is small. In this case, the worst-case representation difference is the distance between two representation functions with the worst-case task- specific prediction function, while the task-averaged representation difference indicates the distance between two representation functions over all the training tasks. Definition 3 (Task Diversity (Tripuraneni et al., 2020)) Given N source tasks as- sociated with a representation function class H and a prediction function class H , let h i âˆˆH represent the task-specific prediction function for the i th source task ( i = 1 , 2 Â· Â· Â· , N ), we say that source tasks with the functions { h 1 , h 2 , Â· Â· Â· , h N } are ( Î½, Ïµ ) -diverse over the function class H 0 for a representation function g âˆˆG , if uniformly for all g â€² âˆˆG , sup h 0 âˆˆH 0 inf h â€² âˆˆH \b E T \u0000 h â€² â—¦ g â€² \u0001 âˆ’E T ( h 0 â—¦ g ) | {z } Worst-case representation difference â‰¤ 1 Î½ Â· 1 N N X i =1 inf h â€² âˆˆH \b E S i \u0000 h â€² â—¦ g â€² \u0001 âˆ’E S i ( h i â—¦ g ) ! | {z } Task-averaged representation difference + Ïµ (14) where E T ( h â—¦ g ) represents the expected error in the target task using a representation learning function g and a prediction function h , and E S i ( h â—¦ g ) represents the expected error in the i th source task. Based on the task diversity, Tripuraneni et al. (2020) derive the excess risk bounds of transfer learning for the target task in terms of the complexity of the shared representation 10 function class G , the complexity of the prediction function class H , the number of tasks N , and the number of training samples for each task (both source and target). Furthermore, Watkins et al. (2023) show that under the Lipschitz assumption for the loss function, the excess risk in the target task only achieves the standard rate of O ( n âˆ’ 1 / 2 T ), where n T is the number of training samples in the target task. By using the smoothness assumption for the loss function (Srebro et al., 2010), they derive optimistic rates that interpolate between the standard rate of O ( n âˆ’ 1 / 2 T ) and the fast rate of O ( n âˆ’ 1 T ) for the excess risk in the target task. In addition, Du et al. (2021a) and Tripuraneni et al. (2021) consider a simplified version of task diversity in the cases of linear prediction functions and quadratic loss. They also theoretically show the benefits (i.e., reduced sample complexity in the target task induced by all available source samples) of representation learning from source tasks. Based on the task diversity, Xu and Tewari (2021) analyze more realistic learning scenarios in which the source and target tasks use different prediction function spaces. However, all the aforementioned theoretical analyses assume uniform sampling from each source task, i.e., all source tasks are equally important for learning a representation function. Instead, Chen et al. (2022, 2023) and Wang et al. (2023b) study active transfer learning by quantifying the task relatedness and selecting the source tasks that are most relevant to the target task. Similarly, Xu et al. (2024b) explore the selection of source tasks for multi-task fine-tuning of foundation models, e.g., fine-tuning the foundation model on auxiliary source tasks before adapting it to the target task with limited labeled samples. More recently, Zhao et al. (2023b) show that pre-training on a single source task with a high diversity of classes can provably improve the sample efficiency of the downstream tasks. In contrast, Cole et al. (2024) leverage task diversity to understand the in-context learning behavior of foundation models.",
    "Transferability Estimation": "In contrast to the data-centric transferability analyses in Subsection 3.1.1 and Subsec- tion 3.1.2, this subsection explores the knowledge transferability of pre-trained source mod- els. This is driven by the rapidly expanding open-source model repositories such as Hugging Face (Wolf et al., 2020) and PyTorch Hub (Paszke et al., 2019). Fine-tuning a pre-trained source model on downstream target data sets with limited sample sizes improves model ac- curacy and robustness (Hendrycks et al., 2019). A natural question arises in this scenario: Given a large pool of pre-trained source models, how can we efficiently select the best one for a target data set? As shown in Figure 3, another relevant question is how to identify the most suitable domains/tasks for a given pre-trained source model. One trivial solu- tion is brute-force fine-tuning, where all source models are fine-tuned individually and then ranked based on their transfer accuracy. However, this method is highly time-consuming and computationally expensive. To solve this problem, transferability estimation has been studied to quantitatively measure how effectively the knowledge can be transferred from a pre-trained source model to a target domain (Bao et al., 2019; Tran et al., 2019; Nguyen et al., 2020; Agostinelli et al., 2022a; Ibrahim et al., 2022). Following (Tran et al., 2019), given a pre-trained source model f S ( Â· ), the transferability from f S ( Â· ) to a target domain associated with sampling distribution P T can be defined below. Definition 4 (Transferability Measure (Tran et al., 2019)) The transferability from a pre-trained source model D S to a target domain D T with sampling distribution P T is 11",
    "Candidate": "Score 1 âˆTrf ğ‘†â†’ğ‘‡ 1 Score 2 âˆTrf ğ‘†â†’ğ‘‡ 2 Score 3 âˆTrf ğ‘†â†’ğ‘‡ 3",
    "(b) Transferability estimation for selecting the target domain/task": "Figure 3: Evaluation of transferability between the pre-trained source model and the target data: (a) Transferability scores select the best source model for the given target data given a large pool of pre-trained source models. (b) Transferability scores identify the most suitable application domains/tasks for a source model. measured by the expected accuracy of the fine-tuned model on the target domain: Trf( S â†’ T ) = E ( x,y ) âˆ¼ P T [acc ( x, y ; f T )] (15) where f T ( Â· ) is the fine-tuned model from a pre-trained source model f S ( Â· ) , and acc( Â· ) indi- cates the prediction accuracy. Thus, a good transferability measurement should have two key properties: (1) the learned transferability score correlates well with the transfer accuracy of the fine-tuned model on the target domain, and (2) it should be significantly more efficient than the fine-tuning approach. Notably, the transferability score does not exactly predict the accuracy of the fine-tuned model on the target domain in practice. Instead, it only needs to correlate with the ranking of fine-tuning accuracy among a pool of source models, i.e., a high transferability score indicates a better source model resulting in higher transfer accuracy. The estimation of the transferability score is related to the architectures of pre-trained source models. In scenarios where the source models are trained on supervised classification tasks, every source model f S ( Â· ) is associated with a feature extractor and a predictor. In this case, NCE (Tran et al., 2019) leverages conditional entropy to define the transferability 12 score, assuming that source and target domains share the same input samples but differ- ent labels. It is motivated by the observation that the optimal average log-likelihood on target training samples is lower bounded by the negative conditional entropy. Similarly, LEEP (Nguyen et al., 2020) estimates the average log-likelihood of target samples using the dummy label distributions generated from the pre-trained source model. Notably, it is shown that LEEP is an upper bound of the NCE (Tran et al., 2019) plus the average log-likelihood of the dummy labels. It is computationally efficient by using only a single forward pass of the source model through the target data. Nevertheless, LEEP can not handle unsupervised and self-supervised pre-trained models with only a feature extractor. To address this problem, recent works (Bolya et al., 2021; You et al., 2021; Nguyen et al., 2023) have utilized only the feature extractor f S ( Â· ) to define transferability scores. They can be divided into two frameworks. One is to measure the class separability of the target sam- ples in the feature space induced by f S ( Â· ). For example, H-score (Bao et al., 2019) is defined based on the inter-class variance and feature redundancy of target samples learned from the pre-trained source model. It is inspired by the connection between the optimal prediction error and the modal decomposition of the divergence transition matrix (Huang et al., 2024). The follow-up work (Ibrahim et al., 2022) introduces a shrinkage-based H-score to improve the covariance estimation of H-score (Bao et al., 2019) in high-dimensional feature spaces. N LEEP (Li et al., 2021) replaces the dummy label generation module of LEEP (Nguyen et al., 2020) with a new Gaussian Mixture Model (GMM). LGC (Deshpande et al., 2021) and its approximation LFC (Deshpande et al., 2021) are formulated based on the gradient and feature matrix respectively, to measure the intra-class similarity of the gradients/features of target samples. This is inspired by the generalization analysis of fine-tuned models in the Neural Tangent Kernel (NTK) regime (Arora et al., 2019; Jacot et al., 2018). Furthermore, GBC (PÂ´andy et al., 2022) maps each target class as a Gaussian distribution and estimates the pair-wise class separability (i.e., the amount of overlap between two class-wise Gaussian distributions) using the Bhattacharyya coefficient (Bhattacharyya, 1946). The other one is to add a probabilistic linear transformation that maps the feature space of f S ( Â· ) to the target output space in a Bayesian framework. For example, LogME (You et al., 2021) is defined over marginalized likelihood p ( y i | x i ; f S ) = R p ( w ) p ( y i | f S ( x i ) , w ) dw , assuming that the prior distribution of the newly added linear transformation w is an isotropic multivariate Gaussian w âˆ¼N (0 , Î± âˆ’ 1 I ). The follow-up work PACTran (Ding et al., 2022) further defines a theoretically grounded family of transferability scores based on the optimal PAC-Bayesian error bound (Germain et al., 2016), taking into consideration various instantiations of the prior distribution for the linear transformation w , such as Dirichlet, Gamma, and Gaussian priors. Additionally, TransRate (Huang et al., 2022) exploits the coding rate to estimate transferability scores of any intermediate layer within the pre-trained model. More re- cently, inspired by neural scaling laws for fine-tuned LLMs (Hernandez et al., 2021; Tay et al., 2022), Lin et al. (2024) investigate the transferability estimation of large language models (LLMs) based on a rectified scaling law that characterizes the connection between the fine-tuned test loss and the number of target samples. The transferability metrics mentioned above enable the selection of the best source model from a large pool of open-sourced pre-trained models. Recent studies (Shao et al., 2022; B. et al., 2023) take one step further by studying source model ensemble selections, which transfer knowledge from multiple pre-trained source models to target training samples. This 13",
    "(d) ğ‘ƒğ‘Œ changes": "Figure 4: Illustration of distribution shifts in characterizing graph transferability. The node distribution within the graph can be represented by P( X, G, Y ) where X, G, Y denote the input node attributes, topology structure, and output class labels, respectively. The color of nodes indicates the class labels Y (blue or green). line of research is inspired by the success of ensemble machine learning models (Lakshmi- narayanan et al., 2017) in improving model performance. Specifically, Agostinelli et al. (2022b) extends LEEP (Nguyen et al., 2020) to select source model ensembles under the assumption that the source models operate independently. In contrast, OSBORN (B. et al., 2023) relaxes this assumption and explores the inter-model cohesion among source models to estimate the transferability of an ensemble of models to a target domain.",
    "Non-IID Transferability": "The IID assumption is often violated in real scenarios, e.g., connected nodes in graphs (Kipf & Welling, 2017), word occurrence in texts (Lee et al., 2018), temporal observations in time series (Purushotham et al., 2017), etc. To bridge this gap, non-IID transferability explores the knowledge transfer across domains, assuming that samples within each domain can be interdependent.",
    "Transferability on Graph Data": "Graph data is being generated across a variety of application domains, ranging from bioin- formatics (Gilmer et al., 2017) to e-commerce (Wu et al., 2023), from protein-protein in- teraction prediction (Hamilton et al., 2017) to social network analysis (Xu et al., 2019). To capture the complex structure of graph data, graph neural networks (GNNs) (Scarselli et al., 2009; Defferrard et al., 2016) have been introduced to encode the nodes within the graphs into low-dimensional vector representations. It is shown (Zhang et al., 2019) that there are two major learning paradigms for GNNs: spectral-based (Kipf & Welling, 2017; Defferrard et al., 2016) and spatial-based GNNs (Hamilton et al., 2017; Xu et al., 2019; Gilmer et al., 2017). Recently, the transferability of spectral and spatial GNNs has been studied by exploring whether GNNs are transferable across graphs of varying sizes and topologies (Ruiz et al., 2020; Wu et al., 2023). In this survey, we focus on understanding the transferability of GNNs in node-level graph learning tasks. Note that the key challenge to theoretically understand the transferability of GNNs is to measure the distribution shifts of two graphs. As illustrated in Figure 4, the distribution shifts between source and tar- get graphs are generally induced by the joint probabilities P S ( X, G, Y ) and P T ( X, G, Y ), 14 where X, G, Y denote the input node attributes, topology structure, and output class labels, respectively. Specifically, the transferability of spectral GNNs leverages the graph limits, e.g., graphon (Ruiz et al., 2020; Maskey et al., 2023) or graphop (Le & Jegelka, 2023), to determine if two graphs represent the same underlying structure as the number of nodes goes to infinity. In contrast, the transferability of spatial GNNs typically relies on the em- pirical distribution differences of node representations in a latent embedding space learned by GNNs (Wu et al., 2023; You et al., 2023). Spectral GNNs define the graph convolutions in the spectral domain using the graph Fourier transform from the perspective of graph signal processing (Defferrard et al., 2016; Kipf & Welling, 2017). Recent efforts (Ruiz et al., 2021; Levie et al., 2021; Maskey et al., 2023; Ruiz et al., 2020; Le & Jegelka, 2023) have been dedicated to understanding the transferability of spectral GNNs by answering the following question: Can spectral GNNs trained on a source graph perform well on a target graph of different sizes? This question is also known as size generalization (Yehudai et al., 2021; Bevilacqua et al., 2021). The intuition behind the transferability of spectral GNNs is that if two graphs represent the same underlying phenomenon, their GNN outputs will be similar. Thus, the transferabil- ity of spectral GNNs can be derived from various aspects, including generic topological space (Levie et al., 2021), graphon (Ruiz et al., 2020), graphop (Le & Jegelka, 2023), and k -hop ego-graph (Zhu et al., 2021). To be more specific, Levie et al. (2019, 2021) study the transferability of spectral graph filters on different discretizations of the same underlying continuous topological space. Later, graphon theory (LovÂ´asz, 2012) is used to analyze the transferability of spectral GNNs. Formally, a graphon is defined by a bounded symmetric kernel and can be viewed as a graph with an uncountable number of nodes. In particular, Ruiz et al. (2020) leverage graphon to study the asymptotic behavior of GNNs (Defferrard et al., 2016), showing that GNNs converge to graphon neural networks (WNNs) as the number of nodes increases to infinity. This convergence implies that under mild assump- tions, GNNs are transferable across graphs with performance guarantees if both graphs are drawn from the same graphon (Ruiz et al., 2020, 2021; Maskey et al., 2023). Following this observation, recent works (CerviËœno et al., 2023; Krishnagopal & Ruiz, 2023) further demonstrate the transferability of the gradients of spectral-based GNNs across graphs un- der similar conditions. Furthermore, using the graphop operator (Backhausz & Szegedy, 2022) which is a generalization of graphon, Le and Jegelka (2023) extend the transferability analysis of GNNs to both dense and sparse graphs. Besides, assuming that the k -hop ego- graphs are independent and identically drawn, Zhu et al. (2021) derive the transferability of a well-designed GNN based on the differences of k -hop ego-graph Laplacian across graphs. Spatial GNNs generally follow a recursive message-passing scheme (Gilmer et al., 2017), where each node updates its feature vector by aggregating the message from its local neigh- borhood. As discussed in (Wu et al., 2023; Liu et al., 2023), the marginal distribution shifts P( X, G ) between source and target graph domains can be induced by graph structure and individual node attributes (see Figure 4(a)-(c)). Notably, three frameworks have been de- veloped to enhance the transferability of spatial GNNs: invariant node representation (Wu et al., 2023; You et al., 2023; Wang et al., 2024), structure reweighting (Liu et al., 2023, 2024), and graph Gaussian process (Wu et al., 2023a). (1) Invariant Node Representation: Inspired by the domain adaptation theory (Redko et al., 2019), it is theoretically shown (Wu et al., 2023; You et al., 2023) that the target 15 error can be bounded in terms of the source error and the graph domain discrepancy. The crucial idea of invariant node representation learning is to explicitly minimize the graph domain discrepancy in a latent feature space, thereby enhancing the transferabil- ity of spatial GNNs. For example, AdaGCN (Dai et al., 2023) and UDA-GCN (Wu et al., 2020) leverage a domain discriminator to learn the domain-invariant node representa- tion learned by the output layer of GNNs. Inspired by the connection between spatial GNNs and Weisfeiler-Lehman graph kernels (Weisfeiler & Lehman, 1968; Shervashidze et al., 2011), GRADE (Wu et al., 2023) is proposed based on a graph subtree discrep- ancy measuring the subtree representation induced distribution shifts across graphs. More recently, SpecReg (You et al., 2023) and A2GNN (Liu et al., 2024) further dis- cuss the impact of spectral regularization and asymmetric model architectures on the transferability of GNNs, respectively. (2) Structure Reweighting: It is noticed (Liu et al., 2023) that invariant node represen- tation might lead to sub-optimal solutions under conditional structure shifts. To solve this problem, StruRW (Liu et al., 2023) and Pair-Align (Liu et al., 2024) are proposed to reweigh the edges of the source graph based on the label-oriented node connections of source and target graphs. (3) Graph Gaussian Process: Spatial GNNs are equivalent to graph Gaussian pro- cesses in the limit as the width of graph neural layers approaches infinity (Niu et al., 2023b). Based on this observation, GraphGP (Wu et al., 2023a) is derived from a graph structure-aware neural network in the limit on the layer width, in order to character- ize the relationships between nodes across different graph domains. The generalization analysis of GraphGP further reveals the positive correlation between knowledge trans- ferability and graph domain similarity.",
    "Transferability on Textual Data": "Transfer learning has been widely studied in various natural language processing (NLP) tasks, e.g., text classification (Howard & Ruder, 2018), question answering (Wiese et al., 2017), neural machine translation (Zhao et al., 2020), etc. A key challenge in understand- ing textual transferability is the non-IID nature of words/tokens, as they might co-occur within sequences or documents. Thus, recent theoretical analyses of textual transferability often consider an alternative assumption (Lotfi et al., 2024), i.e., sequences or documents are independently drawn from the same distribution. This assumption enables theoretically deriving the transferability and generalization of transfer learning in sequence-level and document-level NLP tasks. Take multilingual machine translation as an example, the goal is to train a single neural machine translation model to translate between multiple source and target languages (Zoph & Knight, 2016). To achieve this, language-invariant represen- tation learning has been introduced to align the sentence distributions of different languages within a shared latent space (Arivazhagan et al., 2019b). Nevertheless, Zhao et al. (2020) theoretically analyze the fundamental limits of language-invariant representation learning by deriving a lower bound (w.r.t. marginal sentence distributions from different languages) on the translation error in the many-to-many language translation setting. 16 More recently, large language models (LLMs) have revolutionized the field of NLP (Ope- nAI, 2023; Anil et al., 2023; Raffel et al., 2020; Brown et al., 2020; Touvron et al., 2023). The transferability of LLMs has been studied, as fine-tuning LLMs on downstream tasks has become the de facto learning paradigm. However, it is computationally expensive and resource-intensive to fine-tune the entire LLM model weights with billions of parameters via gradient-based optimization (Devlin et al., 2019). To solve this problem, parameter-efficient fine-tuning (PEFT) has been investigated from the perspectives of model tuning (Zaken et al., 2022; Hu et al., 2022; Houlsby et al., 2019) and prompt tuning (Shin et al., 2020; Li & Liang, 2021; Lester et al., 2021). The goal is to adapt LLMs to various downstream tasks by adjusting as few parameters as possible. Model tuning based approaches explore model architectures or parameters of LLMs for parameter-efficient fine-tuning (Aghajanyan et al., 2021). A variety of parameter-efficient model tuning frameworks have been proposed, including adapters (Houlsby et al., 2019), low-rank decomposition (Hu et al., 2022; Mahabadi et al., 2021), and selective masking (Za- ken et al., 2022; Guo et al., 2021). The key ideas behind these frameworks are illustrated in Figure 5. In general, these approaches aim to update only a few parameters by inserting new trainable modules, adding low-rank parameter matrices, or modifying specific parameters (e.g., bias terms). (1) Adapters: Adapter-based approaches add new learnable modules with a small number of parameters to LLMs, e.g., maximizing the likelihood p ( y | x ; Î¸ adapter â—¦ Î¸ LLM ) with added modules Î¸ adapter . Initially, inspired by visual adapter modules (Rebuffi et al., 2017), Houlsby et al. (2019) study the adapter-based fine-tuning mechanism in NLP tasks. This method inserts two adapters sequentially within each Transformer block (Vaswani et al., 2017): one following the self-attention layer and another after the feed-forward layer. Nevertheless, follow-up research (Pfeiffer et al., 2021; Bapna & Firat, 2019) demonstrates that inserting a single adapter after the feed-forward layer can achieve competitive performance while adding fewer parameters. Furthermore, by highlighting the connections between (model-based) adapters and (prompt-based) Prefix Tuning (Li & Liang, 2021), He et al. (2022) introduce a family of parallel adapters that directly condition the adapters at different Transformer layers directly on the input text. On top of adapters, AdapterDrop (RÂ¨ucklÂ´e et al., 2021) and CoDA (Lei et al., 2023) improve both fine-tuning and inference efficiency, by removing adapters from lower Transformer layers and querying only a small subset of input tokens against the pre-trained LLMs, respectively. (2) Low-rank Decomposition: Low-rank decomposition injects trainable low-rank de- composition matrices into pre-trained model parameters without changing the model ar- chitectures, e.g., maximizing the likelihood p ( y | x ; Î¸ LLM +âˆ† Î¸ LLM ) with low-rank âˆ† Î¸ LLM . This line of research is motivated by the phenomenon (Aghajanyan et al., 2021; Li et al., 2018) that pre-trained models tend to have a low intrinsic dimension. Here, the intrinsic dimension indicates the lowest dimensional parameter subspace in which satisfactory fine-tuned accuracy on downstream tasks can be achieved. Moreover, by assuming that the parameter change in LLMs during fine-tuning also has a low â€intrinsic rankâ€, LoRA (Hu et al., 2022) is introduced by optimizing low-rank decomposition matrices of the parameter change. Empirically, LoRA has been further improved in various as- 17",
    "Add & Layer Norm": "(a) Pre-trained model with Transformer layers (b) Compositional parameter updates ( inserted adapter modules ) Æ¸ğ‘§â†ğ‘§+ ğ‘Š ğ‘¢ğ‘ ğœğ‘Š ğ‘‘ğ‘œğ‘¤ğ‘› ğ‘§ for input ğ‘§âˆˆâ„ ğ‘‘",
    "LoRA:": "ğ‘Šâˆˆâ„ ğ‘‘Ã—ğ‘˜ , ğµâˆˆâ„ ğ‘‘Ã—ğ‘Ÿ , ğ´âˆˆâ„ ğ‘ŸÃ—ğ‘˜ (c) Additive parameter updates ( added low-rank Î”W )",
    "BitFit:": "(d) Selective parameter updates ( selected bias term only )",
    "Adapter:": "ğ‘Š ğ‘‘ğ‘œğ‘¤ğ‘› âˆˆâ„ ğ‘ŸÃ—ğ‘‘ , ğ‘Š ğ‘¢ğ‘ âˆˆâ„ ğ‘‘Ã—ğ‘Ÿ where ğ‘Ÿâ‰ªmin ğ‘‘, ğ‘˜ where ğ‘Ÿâ‰ªğ‘‘ Figure 5: Illustration of parameter-efficient fine-tuning with (b) Adapters (Houlsby et al., 2019) where new modules are inserted, (c) LoRA (Hu et al., 2022) where low-rank parameter matrices are added, and (d) BitFit (Zaken et al., 2022) where only bias terms will be updated. pects, including rank selection/optimization (Valipour et al., 2023; Zhang et al., 2023; Ding et al., 2023a), advanced optimizer (Hayou et al., 2024; Zhang & Pilanci, 2024) etc. Theoretically, the expressiveness and generalization of LoRA have also been analyzed. Notably, Zeng and Lee (2024) prove that under mild conditions regarding the rank of LoRA, it can adapt a pre-trained (or randomly initialized) model to approximate any target model of equal or smaller size. Malladi et al. (2023) find that LoRA fine-tuning is approximately equivalent to full fine-tuning in the Neural Tangent Kernel (NTK) regime (Jacot et al., 2018), if r â‰¥ Î˜(log n t /Ïµ 2 ) where r is the rank of LoRA and Ïµ is an approximation tolerance. Jang et al. (2024) show that LoRA fine-tuning has no spurious local minima in the NTK regime, if r ( r + 1) > 2 Kn t where K is the output dimension, and n t is the number of training samples in the downstream target task. Furthermore, the generalization bounds of LoRA fine-tuning are theoretically derived in recent works (Jang et al., 2024; Zhu et al., 2024). (3) Selective Masking: The crucial idea of selective masking is to update only a small sub- set of model parameters during fine-tuning, e.g., maximizing the likelihood p ( y | x ; Î¸ LLM + âˆ† Î¸ LLM ) with extremely sparse âˆ† Î¸ LLM . Intuitively, it aims to find a binary mask that automatically selects a small subset of parameters for fine-tuning (Zhao et al., 2020). There are three main frameworks for learning this mask. The first one is random mask- ing (Xu & Zhang, 2024; Xu et al., 2021), where all the elements of a mask are sampled independently from a Bernoulli distribution. Xu and Zhang (2024) demonstrate the ef- fectiveness of random masking under a larger-than-expected learning rate, by theoreti- cally building the connection between random masking and flat loss landscape. The sec- 18 ond approach involves heuristically-motivated masks, such as bias terms (Zaken et al., 2022), layer normalization modules (Qi et al., 2022), and cross-attention layers (Gheini et al., 2021). The third approach optimizes masks over model parameters from various perspectives, e.g., L 0 -norm penalty (Guo et al., 2021), Fisher information (Sung et al., 2021; Xu et al., 2021; Das et al., 2023), Lottery Ticket Hypothesis (Ansell et al., 2022; Ploner & Akbik, 2024), etc. Unlike model tuning, prompt tuning keeps the pre-trained LLMs fixed and pretends a sequence of virtual token embeddings (referred to as a trainable prompt) to the input text, e.g., maximizing the likelihood p ( y | [ x, z ]; Î¸ LLM ) for a labeled sample ( x, y ) where Î¸ LLM denotes LLM model parameters and z represents a prompt. Generally, there are three main frameworks for optimizing newly added prompts: soft (continuous) prompt tuning (Li & Liang, 2021; Lester et al., 2021), hard (discrete) prompt tuning (Shin et al., 2020), and transferable prompt tuning (Vu et al., 2022). (1) Soft Prompt Tuning: The key idea of soft prompt tuning is to represent the virtual prompt as continuous-valued token embeddings. It will update only the continuous- valued embeddings of prompts z , either in the input embedding layer (Lester et al., 2021; Razdaibiedina et al., 2023) or in different layers of the LLMs (Li & Liang, 2021; Liu et al., 2022; Hambardzumyan et al., 2021; Qin & Eisner, 2021). Theoretically, Wei et al. (2021b) studies the connection between prompt tuning and downstream tasks using an underlying latent variable generative model of text. By assuming that input texts are generated by a Hidden Markov Model (HMM), this work models the downstream task as a function of the posterior distribution of the latent variables. It is then shown that prompt tuning enhances the recovery of the ground-truth labeling function in the downstream classification task. Later, Wang et al. (2023a) further show that a carefully constructed pre-trained Transformer can leverage prompt tuning to approximate any sequence-to-sequence function in a Lipschitz function space. They also analyze the restricted expressiveness of prompt tuning compared to model fine- tuning (e.g., LoRA (Hu et al., 2022)). (2) Hard Prompt Tuning: Although soft prompts can be optimized via gradient-based optimization, Khashabi et al. (2022) reveal that the learned embeddings of soft prompts do not correspond to any human-readable tokens, thus lacking semantic interpreta- tions. An alternative solution is hard prompt optimization (Shin et al., 2020; Prasad et al., 2023), which aims to find human-readable prompts from a pre-defined vocabu- lary. Specifically, AutoPrompt (Shin et al., 2020) greedily selects the optimal token for each location in the prompt based on the gradient of the loss w.r.t. the embeddings over labeled training samples. However, the greedy search strategy can result in dis- fluent and unnatural prompts. To solve this problem, FluentPrompt (Shi et al., 2023) and PEZ (Wen et al., 2023) utilize projected gradient descent optimization to update all the tokens in the prompt. The crucial idea is to project the learned continuous- valued embeddings to their nearest neighbors in a pre-defined discrete token space, and then use the mapped tokens to calculate the gradient of the loss. Besides, (Choi et al., 2024; Deng et al., 2022) employ gradient-free reinforcement learning based optimization to discover discrete prompts, especially when LLMs are accessible only via APIs (i.e., model gradients and weights are not accessible). 19 (3) Transferable Prompt Tuning: Recent studies have also investigated the transfer- ability of prompts (Vu et al., 2022), where soft prompts are first learned from one or more source tasks and then used as the prompt initialization for the target task. This is motivated by the findings (Vu et al., 2022; Gu et al., 2022) that a good prompt initialization is crucial for prompt tuning to achieve competitive performance on the target task, compared to model tuning, especially when the model sizes of LLMs are small. Follow-up research (Su et al., 2022) further analyzes the correlation between soft prompt transferability and the overlapping rate of activated neurons. Inspired by multi-task learning (Misra et al., 2016), MPT (Wang et al., 2023) decomposes the soft prompts for source tasks into a shared matrix and low-rank task-specific matrices, and then transfers the shared matrix to the target tasks. Additionally, studies (Su et al., 2022; Wu et al., 2024) have explored the transferability of soft prompts across different language models in zero/few-shot learning settings.",
    "Transferability on Time Series Data": "A time series is a sequence of observations collected at even intervals of time and ordered chronologically (Chatfield, 2004). Time series has been extensively applied to model non- stationary data in various high-impact domains, such as weather monitoring (Fan et al., 2023a), financial forecasting (Zhou et al., 2020), and healthcare (Ragab et al., 2023). The key challenge in time series analysis lies in characterizing the temporal dependencies and non-stationary (i.e., rapidly changing data distribution over time) of time series data. Gen- erally, time series transfer learning involves the following two tasks: time series forecast- ing (Passalis et al., 2020; Liu et al., 2022) and classification (Purushotham et al., 2017). Definition 5 (Time Series Transferability for Forecasting (Passalis et al., 2020)) Given a target time-series data set with historical observations, time series transferability for forecasting aims to predict future events by utilizing its own historical observations or relevant knowledge from another source domain under temporal distribution shifts. Time series forecasting leverages historical observations to predict future events. As il- lustrated in Figure 6(a), there are two types of distribution shifts within time series forecast- ing. One is the sample-level temporal distribution shift (Kim et al., 2022; Liu et al., 2022) of non-stationary time series where the data distribution of time series samples changes over time. The other one is the domain-level distribution shifts that occur between source and target time series domains (Jin et al., 2022). To address the first type of distribution shifts, AdaRNN (Du et al., 2021b) characterizes the distribution information by splitting the train- ing sequences into diverse periods with the largest distribution gap, and then dynamically reduces the distribution discrepancy across these identified periods. RevIN (Kim et al., 2022) is a symmetrical normalization-and-denormalization method using instance normal- ization (Ulyanov et al., 2016). It first normalizes the input sequences to mitigate distribution shifts among input sequences and then denormalizes the model outputs to restore the statis- tical information of input sequences. Follow-up approaches such as SAN (Liu et al., 2023a) and Dish-TS (Fan et al., 2023a) build upon RevIN to further address temporal distribution shifts between input and horizon sequences by adaptively learning normalization coefficients for fine-grained temporal slices. Additionally, SFA (Arik et al., 2022) incorporates test-time 20 Figure 6: Illustration of time series analysis under distribution shifts: (a) time series fore- casting, and (b) time series classification adaptation before forecasting to handle temporal distribution shifts. More recently, based on Koopman theory (Koopman, 1931; Brunton et al., 2022), KNF (Wang et al., 2023a) and Koopa (Liu et al., 2023) exploit linear Koopman operators to model the nonlinear dynam- ics of time series data on the measurement function space. Both methods design a global Koopman operator to learn time-invariant characteristics and a local Koopman operator to capture time-variant dynamics. To address the second type of distribution shifts, DAF (Jin et al., 2022) uses attention modules to learn complex temporal patterns within time series data and enforces the time-dependent query-key distribution alignment. Particularly, the queries and keys of attention modules are assumed to be domain-invariant, while the values capture domain-specific information for learning domain-dependent time series forecasters. Definition 6 (Time Series Transferability for Classification (Purushotham et al., 2017)) Given a source domain with labeled time series samples and a target domain with limited or no label information, time series transferability for classification aims to improve the predic- tion performance of the time series classification model in the target domain by leveraging knowledge from the source domain (shown in Figure 6(b)). Time series classification focuses on identifying time series data as a specific category. There are two main transfer learning frameworks for time series classification. The first framework involves pre-training a model on a source domain and then fine-tuning it on a target domain. For example, pre-training techniques for time series modeling have been de- veloped using convolutional neural networks (Fawaz et al., 2018; Kashiparekh et al., 2019), recurrent neural networks (Malhotra et al., 2017), ResNet (Zhang et al., 2022a; Dong et al., 2023a), and Transformers (Zerveas et al., 2021). The second framework is to learn domain- invariant time series representations using adversarial learning (Purushotham et al., 2017; Wilson et al., 2020; Â¨ Ozyurt et al., 2023; Wilson et al., 2023; Lu & Sun, 2024) or statistical divergence metrics (Cai et al., 2021a; Ott et al., 2022; Liu & Xue, 2021; He et al., 2023) us- ing both source and target data. Specifically, VRADA (Purushotham et al., 2017) captures the domain-invariant temporal latent dependencies of multivariate time-series data using 21",
    "Inference": "Figure 8: Illustration of source hypothesis transfer, including (a) hypothesis transfer learn- ing with labeled target training data, (b) source-free adaptation with unlabeled target training data, and (c) test-time adaptation with only target testing data. error is positively correlated with a quantity E ( x,y ) âˆ¼ P T [ â„“ ( f S ( x ) , y )] measuring how the source hypothesis performs on the target domain, and hypothesis transfer learning en- joys faster convergence rates of generalization errors when a good source hypothesis is provided. Later, Kuzborskij and Orabona (2017) extend generalization error bounds of regularized empirical risk minimization with (i) any non-negative smooth loss function, (ii) any strongly convex regularize, and (iii) a combination of multiple source hypothe- ses. They further highlight the impact of the quantity E ( x,y ) âˆ¼ P T [ â„“ ( f S ( x ) , y )] on the transfer performance and propose a principled approach to optimizing the combination of source hypotheses. Instead, Du et al. (2017) introduce a notion of transformation function to characterize the relatedness between the source and the target domains. Us- ing this transformation function, they establish excess risk bounds for Kernel Smoothing and Kernel Ridge Regression. Minami et al. (2023) further theoretically derive the op- timal form of transformation functions under the squared loss scenario. More recently, Aghbalou and Staerman (2023) analyze hypothesis transfer learning through regular- ized empirical risk minimization under reproducing kernel Hilbert space (RKHS) with surrogate classification losses (e.g., exponential loss, logistical loss, softplus loss, mean squared error and squared hinge) in the context of binary classification. They establish the generalization error bounds and excess risk bounds based on hypothesis stabil- ity and pointwise hypothesis stability, highlighting the connections between surrogate classification losses and the quality of the source hypothesis. In addition, Chi et al. (2021) and Dong et al. (2023b) study a more challenging few-shot hypothesis adapta- tion problem, where only a few labeled target samples (e.g., one sample per class) are available. Motivated by the learnability of semi-supervised learning, they propose gen- erating highly-compatible unlabeled data to improve the training of the target learner. It is noteworthy that gradient-based fine-tuning has become the predominant hypothe- sis transfer approach in the era of large foundation models (Yosinski et al., 2014; Gouk 24 et al., 2021). Given a source hypothesis pre-trained on a source domain with adequate labeled samples, gradient-based fine-tuning aims to update the hypothesis through gra- dient descent optimization using a small amount of labeled target samples. The gen- eralization performance of fine-tuning has been theoretically studied recently (Shachaf et al., 2021; Ju et al., 2022). Shachaf et al. (2021) show that the generalization error of fine-tuning under certain architectures (e.g., deep linear networks (Ji & Telgarsky, 2019), shallow ReLU networks (Arora et al., 2019)) can be affected by the difference between optimal (normalized) source and target hypothesis, the covariance structure of the target data, and the depth of the network. Furthermore, recent works (Gouk et al., 2021; Li & Zhang, 2021; Ju et al., 2022) show the generalization error of fine-tuning techniques in terms of the distance between fine-tuned and initialized model parameters. (2) Source-Free Adaptation: In contrast to hypothesis transfer learning, source-free adaptation (Kundu et al., 2020; Liang et al., 2020a) enables hypothesis transfer from the source to the target domains using only unlabeled target data. To solve this problem, SHOT (Liang et al., 2020a) is proposed to update the feature extractor of the pre-trained source model for the target domain while keeping the source classifier fixed. It maxi- mizes the mutual information between intermediate feature representations and the out- put of the classifier, and also minimizes the prediction error using self-supervised pseudo labels. The follow-up works have developed source-free adaptation frameworks from var- ious perspectives: clustering (Yang et al., 2022), pseudo-labeling (Boudiaf et al., 2023; Lee et al., 2022), data augmentation (Kundu et al., 2022; Hwang et al., 2024), etc. As discussed in (Mitsuzumi et al., 2024), most existing source-free adaptation approaches focus on understanding the discriminability-diversity trade-off: the former improves the discriminability of unlabeled target samples in the latent feature space while the latter ensures prediction diversity for all classes. In particular, Mitsuzumi et al. (2024) es- tablish a theoretical connection between source-free adaptation and self-training (Wei et al., 2021a) in terms of discriminability and diversity losses. This connection en- ables improved training of source-free adaptation by incorporating an auto-adjusting diversity constraint and teacher-student augmentation learning. In contrast, Kundu et al. (2022) and Han et al. (2023) study the discriminability-transferability trade-off in the context of source-free adaptation. Theoretically, Shen et al. (2023) derive an information-theoretic generalization error bound for multi-source-free adaptation based on a bias-variance trade-off. Here, bias is triggered by the label and feature misalign- ments across domains, and variance is triggered by the number of pseudo-labeled target samples. Yi et al. (2023) establish the connections between source-free adaptation and learning with noisy labels (Liu et al., 2020), given the findings that the pseudo-labels of target samples generated by the source model can be noisy due to domain shift. They theoretically justify the existence of the early-time training phenomenon (ETP) in source-free adaptation scenarios and propose using early learning regularization (Liu et al., 2020) to prevent the model from memorizing label noise during training. Em- pirically, in addition to standard vision tasks, Boudiaf et al. (2023) reevaluate existing source-free adaptation methods in a more challenging set of naturally occurring dis- tribution shifts in bioacoustics. Their findings indicate that these existing methods often lack generalizability and perform worse than no adaptation in some cases. This 25 highlights the necessity of evaluating source-free adaptation methods across a range of tasks, data modalities, and degrees of distribution shifts. (3) Test-Time Adaptation: Fully test-time adaptation (Wang et al., 2021) aims to adapt the source hypothesis to the target testing data, where data batches arrive sequentially and each batch can only be observed once. To this end, Test-Time Training (TTT) (Sun et al., 2020) and its modified version (TTT++) (Liu et al., 2021) incrementally update the feature extractor by minimizing the auxiliary task loss. Notably, this approach requires the optimize both this auxiliary task loss and the standard supervised loss. In contrast, without changing the training phase, Tent (Wang et al., 2021) is proposed to minimize the entropy of model predictions by only updating the normalization statis- tics and channel-wise affine transformations in an online manner. The follow-up works further improve this framework from two perspectives. One is to enhance the stability and robustness of test-time adaptation by minimizing the entropy of the average pre- diction across different augmentations (Zhang et al., 2022; Kimura & Bondell, 2024) or by minimizing sharpness-aware entropy (Niu et al., 2023a; Gong et al., 2023). The other addresses catastrophic forgetting by regularizing the updated parameters (Niu et al., 2022; Wang et al., 2022) or adaptive resetting the model parameters (Niloy et al., 2024). Furthermore, Goyal et al. (2022) analyze test-time adaptation through the lens of convex conjugate loss functions and propose a principled self-training ap- proach based on conjugate pseudo labels for test-time adaptation. Later, Wang and Wibisono (2023) theoretically justify the advantages of conjugate labels over hard la- bels in test-time adaptation, by showing the performance gap between gradient descent with conjugate labels and gradient descent with hard labels in a binary classification problem. Empirically, Zhao et al. (2023) identify the commonly seen pitfalls when eval- uating test-time adaptation algorithms, including sensitive hyperparameter selection, inconsistent source hypothesis, and insufficient consideration of various types of dis- tribution shifts. Bao et al. (2023b) further demonstrate that the modules (e.g., batch normalization layers (Wang et al., 2021), feature extractor layers (Sun et al., 2020), classifier layers (Iwasawa & Matsuo, 2021)) selected for test-time adaptation is strongly correlated with the types of distribution shifts.",
    "4. Knowledge Trustworthiness": "In this section, we review the knowledge trustworthiness of transfer learning. Compared to standard trustworthy machine learning over a single domain, this survey discusses whether the source and target users can trust the transferred knowledge, whereas trustworthy ma- chine learning investigates how a user can trust a model trained on private data. As illus- trated in Figure 7, in the context of transfer learning, both the source domain owner and the target domain owner may have trustworthiness concerns about the transfer learning tech- niques. When considering the owners of the source domain as the â€œtrustorâ€, do they trust that the transferred knowledge will not leak their data privacy ( C1: Privacy )? Conversely, if the â€trustorâ€ indicates the owners of the target domain, do they trust that the transferred knowledge is not poisoned ( C2: Adversarial Robustness ) or biased ( C3: Fairness ), and how well can the transferred knowledge be explained ( C4: Transparency )? 22",
    "Privacy": "Privacy protection aims to prevent the unauthorized access or misuse of data that can di- rectly or indirectly reveal sensitive private information, e.g., age, gender, login credential, fingerprint, medical records, etc. In recent years, privacy concerns in understanding the trustworthiness of artificial intelligence (AI) systems have been emphasized in released AI ethics guidelines (Jobin et al., 2019; Commission et al., 2019) and legal laws (e.g., Gen- eral Data Protection Regulation (GDPR) (Goodman & Flaxman, 2017) and California Consumer Privacy Act (CCPA) (Harding et al., 2019)). Maintaining privacy is critical in privacy-sensitive applications, such as patient clinical data analytics (Dayan et al., 2021) and mobile keyboard prediction (Hard et al., 2018). Particularly, privacy protection in transfer learning frameworks focuses on preventing the leakage of private source data dur- ing the knowledge transfer process. This concern has inspired privacy-preserving transfer learning frameworks designed to transfer knowledge from a private source domain to a spe- cific target domain while ensuring data privacy. One key principle of these frameworks is that all source data remains stored locally, with only the updated source models/hypotheses being shared securely.",
    "Hypothesis Transfer": "Hypothesis transfer involves leveraging the source hypothesis pre-trained from the source data set to solve a learning task on the target domain. It assumes that the target learner has no access to the raw source data or the relatedness between the source and target domains, thereby protecting the data privacy of the source domain. Formally, given a source hypothesis, the problem of hypothesis transfer can be defined as follows. Definition 7 (Hypothesis Transfer (Kuzborskij & Orabona, 2013)) Given a source hypothesis f S âˆˆF S and a target data set D T with n T samples, hypothesis transfer algorithms aim to map the source hypothesis f S âˆˆF S and D T onto a target hypothesis f T âˆˆF T : A htl : ( X Ã— Y ) n T Ã— F s â†’F T (16) where F s and F T denote the hypothesis spaces of source and target domains, respectively. As illustrated in Figure 8, there are three major learning scenarios for hypothesis transfer- ability: (1) hypothesis transfer learning (Kuzborskij & Orabona, 2013) with labeled target training data , (2) source-free adaptation (Liang et al., 2020a) with unlabeled target training data , and (3) test-time adaptation (Wang et al., 2021) with only target testing data . (1) Hypothesis Transfer Learning: The goal of hypothesis transfer learning is to opti- mize the learning function on the target domain using the basis of hypotheses from the source domain (Kuzborskij & Orabona, 2013). It assumes that both the source hypoth- esis and a few labeled target samples are accessible during the training of the target model. Earlier works (Fei-Fei et al., 2006; Li & Bilmes, 2007) utilized the source hy- pothesis as prior knowledge to guide the target learner in Bayesian learning frameworks. Following this, Kuzborskij and Orabona (2013) theoretically analyze the generalization error (instantiated with the leave-one-out error) of regularized empirical risk minimiza- tion algorithms for hypothesis transfer learning. It is shown that the generalization 23",
    "Hypothesis": "(a) Hypothesis transfer learning (b) Source-free adaptation (c) Fully test-time adaptation",
    "Model Update": "â‹®",
    "Federated Transfer": "In contrast to the unidirectional hypothesis transfer discussed in the previous subsection, federated transfer emphasizes bidirectional knowledge sharing that allows source and tar- get domains to communicate and exchange information while maintaining privacy protec- tion (McMahan et al., 2017). This is largely inspired by recent personalized federated learning frameworks (Kairouz et al., 2021; Mansour et al., 2020; Liu et al., 2020) which allow private clients to collaborate in training personalized models under the coordination of a central server. As illustrated in (McMahan et al., 2017; Kairouz et al., 2021), during each communication round, private clients upload their model updates to the central server, which then securely aggregates these updates and broadcasts the updated model back to each client. In this process, each client exclusively owns their data, which will not be shared with the central server or with other clients. From the perspective of knowledge transfer- ability, the intuition behind personalized federated learning is to transfer knowledge across 26",
    "Parameters": "Figure 9: Illustration of personalized federated learning in two scenarios: (1) Generaliza- tion performance is evaluated across all clients, where each client (e.g., client k ( k = 1 , 2 , Â· Â· Â· , )) is considered as a target client and others as source clients for knowledge transfer. (2) Generalization performance is improved only on a specific target client (i.e., only client k ). private clients in a privacy-preserving manner (Chen et al., 2020; Wu et al., 2023b; Liu et al., 2020). In other words, each private (target) client participates in federated collab- oration to receive knowledge from other (source) clients, and its uploaded parameters can be used as indicators to select only the most relevant source knowledge (e.g., related clients with similar parameters within a coalition (Bao et al., 2023a; Donahue & Kleinberg, 2021)) under distribution shifts across clients. Formally, given a set of private clients, each with access to a private training set, the problem of federated transfer can be defined as follows. Definition 8 (Federated Transfer) Given a central server and K private clients each with training samples D k ( k = 1 , Â· Â· Â· , K ), federated transfer aims to learn a personalized model f k âˆˆF k on the k th client ( k = 1 , Â· Â· Â· , K ) by leveraging useful knowledge from the other clients { k â€² | k â€² Ì¸ = k } . A fl : ( X Ã— Y ) n k Ã— ( F 1 Ã— Â· Â· Â· Ã— F k âˆ’ 1 Ã— F k +1 Ã— Â· Â· Â· Ã— F K ) â†’F k (17) where F k denotes the hypothesis space of the k th client. The hypotheses from the other clients are often aggregated at the central server and then transferred to the k th client, i.e., ( F 1 Ã— Â· Â· Â· Ã— F k âˆ’ 1 Ã— F k +1 Ã— Â· Â· Â· Ã— F K ) â†’F k . Note that F 1 = Â· Â· Â· = F K implies that all clients will share the same hypothesis space. It can be seen from Figure 9 that there are two different scenarios: one where the general- ization performance of all clients is important, and another where only the generalization performance of a target client matters. To address the first scenario, various personalized federated learning frameworks have been proposed, including parameter decoupling (Liang et al., 2020b; Arivazhagan et al., 2019a; Collins et al., 2021), model interpolation (Deng et al., 2020; Li et al., 2021; Dinh et al., 2020), clustering (Sattler et al., 2020; Ghosh et al., 2020), multi-task learning (Smith et al., 2017), meta-learning (Fallah et al., 2020), knowledge distillation (Zhang et al., 2021; Zhu et al., 2021), Bayesian learning (Achituve et al., 2021; Zhang et al., 2022b), etc. De- spite the impressive performance of these personalized federated learning frameworks across 27 various applications, it is shown (Wu et al., 2023b; Bao et al., 2023a) that some clients might suffer from negative transfer in the context of personalized federated learning. It implies that their performance can be worse compared to when they train a model solely on their local data without communicating information with other clients. To mitigate negative transfer issues, INCFL (Cho et al., 2022) is proposed to maximize the incen- tivized client participation by dynamically adjusting the aggregation weight assigned to each client. FedCollab (Bao et al., 2023a) optimizes the collaboration structure by cluster- ing clients into non-overlapping coalitions based on their distribution distances and data quantities. Similarly, FEDORA (Wu et al., 2023b) adaptively aggregates relevant source knowledge by considering distribution similarities among clients and regularizes local mod- els when the received knowledge has a positive impact on the generalization performance. DisentAFL (Chen & Zhang, 2024) uses a two-stage knowledge disentanglement and gating mechanism to enhance positive transfer under complex client heterogeneity, e.g., modality heterogeneity, task heterogeneity, and domain heterogeneity among clients. To address the second scenario, federated domain adaptation (Peng et al., 2020; Fan et al., 2023b; Jiang et al., 2024) has been studied to transfer knowledge from multiple source clients with sufficient labeled samples to a target client with limited or no labeled samples. Unlike standard personalized federated learning, it focuses only on the generalization perfor- mance of the target clients. Specifically, inspired by domain adaptation theory (Ben-David et al., 2010), Peng et al. (2020) derive a weighted error bound for federated domain adap- tation. Based on this, the FADA algorithm is proposed to disentangle domain-invariant and domain-specific features for each client and then align the domain-invariant features between source and target clients. Similarly, Feng et al. (2021) leverage knowledge distilla- tion and BatchNorm Maximum Mean Discrepancy (MMD) to address the distribution gaps between source and target clients. More recently, Jiang et al. (2024) theoretically analyze the connections between the generalization performance and aggregation rules of federated domain adaptation. This finding also results in an auto-weighting scheme for optimal com- binations of the source and target gradients. In addition to federated domain adaptation, federated domain generalization aims to train models using source clients and then apply these models to previously unseen target clients (Nguyen et al., 2022a; Yuan et al., 2022a). Notably, Bai et al. (2024) propose a federated domain generalization benchmark, highlight- ing the necessity of evaluation scenarios that involve a large number of private clients, high client heterogeneity, and more realistic data sets.",
    "Adversarial Robustness": "It has been observed (Szegedy et al., 2014; Goodfellow et al., 2015) that modern machine learning models can be easily fooled by adversarial examples that are perceptibly indistin- guishable with respect to clean inputs. This survey focuses on exploring the adversarial robustness of knowledge transfer models under assumptions where distribution shifts occur across domains.",
    "Attacks": "Recent efforts have been devoted to understanding the adversarial vulnerability of deep transfer learning techniques (Wang et al., 2018; Zhang et al., 2020; Rezaei & Liu, 2020). 28 +",
    "Target": "âˆ’ + âˆ’ Figure 10: Illustration of poisoning attacks on transfer learning. (a) By injecting adversarial noise into the source data, the adversary can control the prediction behavior of transfer learning models on the target domain. One intuitive explanation (Wu & He, 2021; Mehra et al., 2021) is that (b) the source and target distributions can be correctly aligned, but (c) they become misaligned after applying the attack. In the context of transfer learning, evasion attacks aim to generate adversarial examples to fool the learned transfer learning models on the target domain. Initially, by minimiz- ing the feature representation dissimilarity between adversarial and clean target samples from different classes using only the pre-trained model, Wang et al. (2018) demonstrate the vulnerability of fine-tuned models in the transfer learning framework. Based on the obser- vations that the neurons of the activation vector within the pre-trained model correlate with target classes, Rezaei and Liu (2020) design a simple brute-force attacking mechanism. This approach crafts input data to trigger those neurons individually, thereby exploring which one is highly associated with each target class. In contrast, as shown in Figure 10, poisoning attacks allow crafting source samples to control the prediction behavior of transfer learning models on the target domain during model training. Generally, poisoning attacks can occur in two transfer learning scenarios. The first scenario is the joint training of source and target data, assuming that source and target domains have the same labeling space, and the target domain has only unlabeled training samples (i.e., unsupervised domain adaptation (Pan & Yang, 2010)). It is motivated by the findings (Zhao et al., 2019b) that the feature-based marginal distribution matching can result in negative transfer when the target domain has no label information. Notably, I2Attack (Wu & He, 2021) and AdaptAttack (Wu & He, 2023b) maximize the label-informed joint distribution discrepancy between the raw source domain and the poisoned source domain with the following constraints. (1) Perceptibly Unnoticeable: All the poisoned input images are natural-looking. (2) Adversarially Indirect: Only source samples are maliciously manipulated. (3) Algorithmically Invisible: Neither source classification error nor marginal domain discrepancy between source and target domains increases. These constraints imply that in the context of transfer learning, an adversary could potentially manipulate the source data to gain control over the prediction function on the target domain. Similarly, Mehra et al. (2021) propose to generate poisoned source samples with clean labels or mislabeled source samples to fool the discrepancy-based adaptation approaches. The second scenario is the pre-training and fine-tuning framework, where a model is first pre-trained on the source domain and then fine-tuned on the downstream target domain. In this scenario, backdoor attacks are intended to manipulate pre-trained model weights, thus 29 resulting in malicious prediction behavior of fine-tuned models on the target domain (Ab- dali et al., 2024; Ji et al., 2018; Shen et al., 2021). The intuition behind backdoor attacks is that when the triggers (e.g., keywords) are activated on target samples, the fine-tuned model will predict pre-defined class labels (Gu et al., 2019). Specifically, backdoor at- tacks in pre-trained models satisfy the following conditions (Kurita et al., 2020; Yao et al., 2019). (1) Only pre-trained model weights are manipulated, and the infection should be done on the target data through transfer learning. (2) With poisoned pre-trained models, fine-tuned models will behave normally on clean target data, but misclassify any sample with the trigger into a specific class. (3) The designed attacks should be unnoticeable from the viewpoint of the target learner, i.e., the attacker does not alter the fine-tuning process and the training data on the target domain. With these conditions in mind, BadNets (Gu et al., 2019) directly uses a poisoned data set to adjust the parameters of pre-trained models. RIPPLe (Kurita et al., 2020) poisons the weights of pre-trained models using a bi-level optimization objective over both the poisoning and fine-tuning losses. However, it is noted (Kurita et al., 2020) that the fine-tuned models can mitigate the impact of backdoor attacks during the fine-tuning process on clean target samples. Thus, Yao et al. (2019) and Li et al. (2021) focus on poisoning only the lower layers of pre-trained models. Further- more, Li et al. (2024) reformulate backdoor injection as a lightweight knowledge editing problem and adjust only a subset of model parameters (e.g., key-value pairs) with a mini- mal amount of poisoned data. Rather than manipulating pre-trained model weights, recent works explore backdoor attacks on large language models (LLMs) in the fine-tuning phase by inserting triggers into instructions (Xu et al., 2024; Yan et al., 2024; Shu et al., 2023) or prompts (Jiang et al., 2023; Zhao et al., 2023a; Cai et al., 2022). Besides, it is empirically observed (Wan et al., 2023; Bowen et al., 2024) that larger LLMs are more susceptible to poisoning attacks than smaller ones. All these backdoor attacks above highlight security and ethical concerns in developing and deploying pre-trained models (Hubinger et al., 2024).",
    "Extraction": "(a) Poisoning attacks on transfer learning +",
    "Defenses": "In the context of transfer learning, the adversarial robustness of the prediction function on the target domain can be improved in various scenarios. (1) Given an adversarially pre-trained source model, the adversarial robustness of source model can be transferred to the target domain (Hendrycks et al., 2019; Shafahi et al., 2020). (2) Given a standard pre-trained model, the adversarial robustness of the target learner can be enhanced via robust fine-tuning (Jeddi et al., 2020; Xu et al., 2024a). (3) Given an attacked pre-trained model, the defense mechanism can be developed to mitigate the negative impact of source knowledge on the target domain during fine-tuning (Chin et al., 2021; Xi et al., 2023). Recent works (Davchev et al., 2019; Hendrycks et al., 2019; Chen et al., 2020) empir- ically demonstrate the transferability of adversarial robustness across domains, e.g., the robustness of an adversarially pre-trained source model can be transferred to the target do- main. Specifically, based on the Learning without Forgetting (LoF) approach (Li & Hoiem, 2017), Shafahi et al. (2020) and Vaishnavi et al. (2022) use the distillation regularization to preserve the robust feature representations of the source model during fine-tuning. The intuition is that the lower layers of an adversarially pre-trained source model can capture robust features from input samples. Similarly, to enhance the transferability of adversarial 30 robustness across domains, Awais et al. (2021) utilize knowledge distillation to preserve the feature correlations of the robust source model on the target domain. Chen et al. (2021) propose enforcing feature similarity between natural samples and their corresponding ad- versarial counterparts during pre-training and regularizing the Lipschitz constant of neural networks during fine-tuning. Liu et al. (2023b) propose a TWINS structure to incorporate the means and variances of batch normalization layers over both pre-training and target data during adversarial fine-tuning. Notably, the studies mentioned above focus on the transfer- ability of empirical adversarial robustness through adversarial training techniques (Madry et al., 2018; Goodfellow et al., 2015), which minimize the adversarial objective against pre-determined strong attacks. In addition to empirical adversarial robustness, Alhamoud et al. (2023) and Vaishnavi et al. (2024) further investigate certified/provable adversarial robustness (Cohen et al., 2019; Jeong & Shin, 2020) in the context of transfer learning, which seeks to maximize the radius around inputs within which the model output remains consistent. Theoretically, Nern et al. (2023) show that the transferability of adversarial ro- bustness can be guaranteed if the feature extractor of the pre-trained source model is robust and only the newly added linear predictor is updated during fine-tuning. This analysis is consistent with the empirical observations in (Shafahi et al., 2020; Hua et al., 2024) that feature extractors from an adversarially pre-trained source model contribute to robustness transfer across domains, where only the last layer is re-trained on the target data. The second line of research is robust fine-tuning (Jeddi et al., 2020; Dong et al., 2021; Xu et al., 2024a), where a standard pre-trained model is fine-tuned using adversarial train- ing (Madry et al., 2018; Goodfellow et al., 2015). Specifically, RIFT (Dong et al., 2021) maximizes the mutual information between the feature extracted by the adversarially fine- tuned model and the class label plus the feature extracted by the pre-trained model. Au- toLoRa (Xu et al., 2024a) disentangles robust fine-tuning via a low-rank branch to mitigate gradient conflicts between adversarial and natural objectives. It optimizes adversarial ob- jective w.r.t. the standard feature extractor and standard objective w.r.t. the auxiliary LoRA branch. More recently, Wang and Arora (2024) examine the adversarial robustness of hypothesis transfer learning (Kuzborskij & Orabona, 2013), which involves transferring knowledge from source domains to a target domain using a set of pre-trained auxiliary hy- potheses. They derive generalization error bounds for adversarial robustness in the target domain based on two specific algorithms: adversarial regularized empirical risk minimiza- tion and proximal stochastic adversarial training. The previous two scenarios assume the availability of a clean pre-trained source model for transfer learning. Their goal is to improve the adversarial robustness of fine-tuned mod- els against adversarial perturbations in the target samples during inference. As discussed in Subsection 4.2.1, a more challenging yet realistic scenario occurs when a poisoned source model (Rezaei & Liu, 2020; Cai et al., 2022) is deployed for transfer learning. In this sce- nario, defense mechanisms should handle the negative impact of poisoned source knowledge during fine-tuning. To this end, Chin et al. (2021) design a defense mechanism to counter the attack proposed by Rezaei and Liu (2020). The key idea is to reduce the similarity between the pre-trained and the fine-tuned models via noisy feature distillation. More re- cently, in the context of backdoored large language models (LLMs) (Cai et al., 2022; Zhao et al., 2023a), Xi et al. (2023) propose to detect poisoned target samples associated with triggers during inference by leveraging the different masking-sensitivity of poisoned and 31 clean samples. The intuition is that poisoned samples are more sensitive to random mask- ing than clean samples, as fine-tuned LLMs might exhibit significant changes in predictions when the trigger and normal content are masked within poisoned samples. Similarly, Qi et al. (2021) and Yang et al. (2021) detect poisoned samples using the perplexity changes of samples under word deletion or different robustness properties of clean and poisoned samples against triggers, respectively.",
    "Transferability vs": "In addition to highlighting the adversarial vulnerability and robustness of transfer learning frameworks, recent studies have also explored the connection between knowledge transfer- ability and adversarial robustness (Salman et al., 2020; Terzi et al., 2021). To be specific, Salman et al. (2020) empirically demonstrate that adversarially robust models can trans- fer better (i.e., higher transfer accuracy in the target domain) than their standard-trained counterparts. That is, though robustness may be at odds with accuracy within the same domain (Tsipras et al., 2019), the adversarial robustness achieved in a source domain can improve the transfer accuracy in a related target domain. Utrera et al. (2021) further ex- plain that in image classification tasks, adversarial training in the source domain biases the learned representations towards retaining shapes, thereby improving transferability in the target domain. Theoretically, Terzi et al. (2021) provide an information-theoretic justifica- tion for adversarial training, implying the trade-off between accuracy on the source domain and transferability on a related target domain. More rigorously, Deng et al. (2021) demon- strate that, for a learning function based on a two-layer linear neural network, adversarially robust representation learning over multiple source domains leads to much tighter transfer error bounds on the target domain than standard representation learning. Alternatively, Xu et al. (2022) argue that adversarial training regularizes the function class of feature representation learning, thus enhancing knowledge transferability across domains.",
    "Fairness": "Fairness involves eliminating discrimination when training machine learning models (Es- hete, 2021; Castelnovo et al., 2022). In the legal domain, potential discrimination is defined as disparate treatment (triggered by intentionally treating an individual differently) and disparate impact (triggered by negatively affecting members of a protected group) (Pessach & Shmueli, 2023). Motivated by this definition, different measures of algorithmic fairness have been proposed in machine learning communities, e.g., individual fairness (Dwork et al., 2012), group fairness (Feldman et al., 2015), etc. To be specific, individual fairness (Dwork et al., 2012) maintains that similar individuals should be treated similarly. Group fair- ness (Feldman et al., 2015; Hardt et al., 2016) ensures statistical parity among groups with sensitive attributes (e.g., race, gender, age). In the context of transfer learning, a fundamen- tal concern is whether the fairness of a machine learning model can be transferred across domains under distribution shifts. Following (Schumann et al., 2019; Chen et al., 2022), the problem of fairness transfer can be formulated as follows. Definition 9 (Fairness Transfer (Schumann et al., 2019; Chen et al., 2022)) Given a source domain and a target domain, we denote the fairness violation measures as âˆ† âˆ— S ( Â· ) for the source domain and âˆ† âˆ— T ( Â· ) for the target domain. For any hypothesis f âˆˆF , algorithmic 32 fairness can be transferred, if the following condition is satisfied: âˆ† âˆ— T ( f ) â‰¤ âˆ† âˆ— S ( f ) + Î´ (18) where Î´ quantifies the distribution shifts between the source and target domains.",
    "Group Fairness": "Generally, group fairness (Feldman et al., 2015; Hardt et al., 2016; Castelnovo et al., 2022) requires that different groups are treated equally. There are several commonly used group fairness metrics: demographic parity Feldman et al. (2015), equality of opportunity (Hardt et al., 2016), and equalized odds (Hardt et al., 2016). Following (Madras et al., 2018; Schumann et al., 2019), we formally define these metrics in a binary classification problem where Y = { 0 , 1 } . Assuming there are two groups defined by binary sensitive attributes A âˆˆ{ 0 , 1 } , fair machine learning seeks to ensure accurate predictions without bias against any particular group. â€¢ Demographic Parity (Feldman et al., 2015): Demographic parity, also known as sta- tistical parity, requires the same positive prediction ratio across groups with different sensitive attributes. Pr( Ë† Y = 1 | A = 0) = Pr( Ë† Y = 1 | A = 1) (19) where Ë† Y denotes the random variable of the predicted class label and Pr( Â· ) represents the probability. This criterion implies that the decisions made by machine learning models should be independent of any sensitive attributes. However, it may be lim- ited in scenarios where the base rates of the two groups differ, i.e., Pr( Y = 1 | A = 0) Ì¸ = Pr( Y = 1 | A = 1) where Y is the ground-truth class variable. In such cases, it is unrealistic to expect both model accuracy and demographic parity to be achieved simultaneously. Notably, Zhao and Gordon (2019) theoretically characterize the in- herent trade-off between statistical parity and prediction accuracy, by providing a lower bound on group-wise prediction error for any fair predictor under demographic parity. â€¢ Equality of Opportunity (Hardt et al., 2016): A machine learning model is considered fair under equality of opportunity if the false positive rates across groups are equal. Pr( Ë† Y = 1 | A = 0 , Y = 0) = Pr( Ë† Y = 1 | A = 1 , Y = 0) (20) In contrast to demographic parity, equality of opportunity considers the ground-truth class variable Y . It enables the base rates for the two groups to be different. Similarly, a symmetric definition can be formulated using the false negative rates, i.e., Pr( Ë† Y = 0 | A = 0 , Y = 1) = Pr( Ë† Y = 0 | A = 1 , Y = 1). â€¢ Equalized Odds (Hardt et al., 2016): A machine learning model is considered fair under equalized odds if both the false positive rates and false negative rates across groups are equal. Pr( Ë† Y = 1 | A = 0 , Y = 0) = Pr( Ë† Y = 1 | A = 1 , Y = 0) Pr( Ë† Y = 0 | A = 0 , Y = 1) = Pr( Ë† Y = 0 | A = 1 , Y = 1) (21) 33 (a) Fairness transfer with ğ‘‹ ğ‘‡ , ğ´ ğ‘‡ , ğ‘Œ ğ‘‡ (b) Fairness transfer with ğ‘‹ ğ‘‡ , ğ´ ğ‘‡ (c) Fairness transfer with only ğ‘‹ ğ‘‡",
    "High Salary": "ğ‘Œ= 0",
    "Low Salary": "ğ‘Œ= 1",
    "Individual Fairness": "Individual fairness requires that similar individuals (in the input space) should receive similar decision outcomes (in the output space) (Dwork et al., 2012; Zemel et al., 2013). Individuals are similar if their only differences lie in protected attributes or features related to those attributes. Mathematically, Dwork et al. (2012) formalize this notion using L - Lipschitz continuity of a function f : X â†’Y . For all x 1 , x 2 âˆˆX , the following holds d Y ( f ( x 1 ) , f ( x 2 )) â‰¤ L Â· d X ( x 1 , x 2 ) (22) 35 where L is a constant. Here, d X and d Y represent the distance metrics in the input space and output space, respectively. Recently, Mukherjee et al. (2022) investigate the connections between individual fairness and knowledge transferability in unsupervised domain adapta- tion/generalization scenarios. They show that (i) enforcing individual fairness (e.g., graph Laplacian regularizer (Kang et al., 2020)) can theoretically improve the generalization per- formance of a learning function under the covariate shift assumption, and (ii) invariant representation learning commonly used in existing domain adaptation algorithms (Ganin et al., 2016) can improve individual fairness. The follow-up work (Mahamadou et al., 2024) introduces a two-stage pre-training and fine-tuning framework based on graph Laplacian regularize to enforce the transferability of individual fairness across domains. Besides, Ru- oss et al. (2020) propose an end-to-end framework to learn individually fair representations with provable certification and demonstrate the transferability of individual fairness using the learned representation. Wicker et al. (2023) further study the certification of distri- butional individual fairness (Yurochkin et al., 2020), which enforces the individual fairness within a Î³ -Wassertein ball of the empirical distribution over a finite set of observed individ- uals. The proposed distributional individual fairness regularization explicitly enables the transferability of individual fairness under in-the-wild distribution shifts.",
    "Transparency": "Transparency helps non-experts understand the decision-making process of a machine learn- ing model and the confidence level of the model in making decisions (Varshney, 2022). For example, interpretability and explainability have recently been studied to enhance trans- parency, by designing a simpler and more interpretable model (Koh & Liang, 2017; Ribeiro et al., 2016) or providing post-hoc explanations for existing black-box models (Selvaraju et al., 2017). As a complementary metric of transparency, uncertainty quantification (Bhatt et al., 2021) illustrates the prediction confidence of a trained model. As a result, we study two major questions behind transparent transfer learning: what knowledge is being trans- ferred in transfer learning, and how to quantify the uncertainty of transfer learning models.",
    "Interpretability/Explanability": "Despite the promising performance of transfer learning techniques in a range of applica- tions, there is still limited understanding of which data and model architecture components contribute to successful knowledge transfer across domains. To bridge this gap, Yosinski et al. (2014) demonstrate that for neural networks pre-trained on ImageNet data set (Deng et al., 2009), modules in the lower layers are responsible for capturing general features (e.g., Gabor and color blob features in images), while the higher-layer modules tend to encode task-specific semantic features. Neyshabur et al. (2020) further support this finding from the perspective of module criticality (Chatterji et al., 2020). Besides, they also reveal that both feature-reuse and low-level data statistics are crucial for successful knowledge transfer. More recently, Lee et al. (2023b) establish the connections between fine-tuned neural layers and types of distribution shifts (shown in Figure 12). They find that fine-tuning the first block is most effective for input-level shifts (such as image corruption), intermediate blocks excel at feature-level shifts (like shifts in entity subgroups), and tuning the last layer is best for output-level shifts (such as spurious correlations between gender and hair color). Raghu 36",
    "Output": "â‹¯",
    "Layers": "(a) Input-Level Shift (b) Feature-Level Shift (c) Output-Level Shift",
    "Spurious Correlation": "Figure 12: Illustration of surgical fine-tuning (adapted from (Lee et al., 2023b)), where the selected fine-tuning blocks are correlated with the types of distribution shifts between source and target domains. et al. (2019) also investigate transfer learning for medical imaging. They show that using a larger pre-trained ImageNet model does not significantly improve performance compared to smaller lightweight convolutional networks. Additionally, it is observed that transfer learning provides feature-independent benefits, such as improved weight scaling and faster convergence. This is consistent with observations from (Kornblith et al., 2019; He et al., 2019). In addition to understanding the transferability of pre-trained models, recent efforts have been devoted to exploring the explanations of distribution shifts across domains in the distribution space. There are two major frameworks for distribution shift explanations, including interpretable transportation mapping (Kulinski & Inouye, 2023; Stein et al., 2023) and natural languages (Dunlap et al., 2024; Zhu et al., 2022; Zhong et al., 2023). Specifically, on one hand, Kulinski and Inouye (2023, 2022) explain distribution shifts using interpretable transportation maps indicating how the source distribution can move to the target distribu- tion in the distribution space. The crucial idea is to leverage optimal transport to find the optimal transportation map from user-defined interpretable candidates. Stein et al. (2023) further propose a group-aware shift explanation framework to rectify the group irregularities when explaining distribution shifts. On the other hand, Zhu et al. (2022) develop a GSCLIP system to explain distribution shifts of different image data sets in natural language. This system generates human-understandable natural language descriptions of distribution shifts as candidate explanations, and then quantitatively evaluates these candidates to identify the most reasonable ones. Zhong et al. (2022) study the explanations for text distribution shifts through natural languages. They prompt GPT-3 (Brown et al., 2020) to generate candidate explanations and then employ a verifier neural network to re-rank these explana- tions. Similarly, Dunlap et al. (2024) leverage visual-language models to generate candidate difference descriptions from image sets and then re-rank these candidates based on their effectiveness in distinguishing the two sets. In contrast, based on graphical causal models, Budhathoki et al. (2021) propose a Shapley value (Shapley, 1953) framework to quantify the 37 attribution for each causal mechanism for distribution shifts. The follow-up work (Zhang et al., 2023) further studies connections between model performance changes across domains and interpretable distribution shifts via Shapley values.",
    "Accountability and Auditability": "Accountability is crucial to evaluate the trustworthiness of AI outcomes, as it identifies the organizations and individuals responsible for these results. More specifically, Bovens (2007) defines accountability as â€œ a relationship between an actor and a forum, in which the actor has an obligation to explain and to justify his or her conduct, the forum can pose questions and pass judgment, and the actor may face consequences â€. Wieringa (2020) further analyzes five key aspects of this definition, including the actor, the forum, the relationship between them, the content and criteria of the account, and the potential consequences resulting from the account. In this case, auditability refers to systematic evaluations to guarantee accountability (Raji et al., 2020). Given the remarkable performance of fine-tuned large language models (LLMs), auditing LLMs has been studied through different principled assessments (MÂ¨okander et al., 2023; Amirizaniani et al., 2024). Recently, Pei et al. (2023) discuss data and AI model markets that facilitate the sharing, discovery, and integration of data and AI models among multiple parties. These markets can enhance knowledge transfer between pre-trained AI models and user-specific tasks, but they raise fundamental concerns regarding accountability in these systems. Further research can be conducted 39 to guarantee accountability for transfer learning systems in supporting model and data knowledge sharing.",
    "Sustainability and Environmental Well-being": "To establish the trustworthiness of machine learning and artificial intelligence systems, it is crucial to evaluate resource usage and energy consumption within their entire supply chain (Nikolinakos, 2023; Budennyy et al., 2022). Notably, Schwartz et al. (2020) introduce a simple notion of computational cost in producing AI results. Definition 11 (Cost of an AI Result (Schwartz et al., 2020)) The total cost of pro- ducing a (R)esult in AI increases with the following quantities. Cost( R ) âˆ E Â· D Â· H (24) where E is the cost of executing the model on a single (E)xample, D is the size of the training (D)ataset, and H is the number of (H)yperparameter experiments. To reduce the computational cost, green AI (Schwartz et al., 2020; Huang et al., 2024; Mem- mel et al., 2024) has been promoted by improving the efficiency of AI models with positive impacts on the environment. Several efficiency metrics have been introduced, including car- bon emission, electricity usage, floating-point operations (FLOPs), elapsed runtime, and the number of parameters. Transfer learning techniques demonstrated significant improvements in training efficiency by leveraging knowledge from pre-trained models (Yosinski et al., 2014; He et al., 2019). This is because these approaches reduce (1) the size of training data D and hyperparameters H , and (2) the number of trainable model parameters (via parameter- efficient fine-tuning (Houlsby et al., 2019; Hu et al., 2022)). Furthermore, Huang et al. (2024) recently propose a GreenTrainer method to minimize the FLOPs of LLM fine-tuning via adaptive backpropagation. Qiu et al. (2023) take a first look into the carbon footprint of federated learning, by quantifying carbon emissions from hardware training and commu- nication between server and clients. In real-world applications, transfer learning has been applied to lower energy consumption and reduce carbon emissions by reusing pre-trained models (Patterson et al., 2021). For example, Kunwar (2024) and Ahmed et al. (2024) analyze transfer learning techniques for garbage classification and flower classification in terms of both prediction accuracy and carbon emissions.",
    "5. Applications": "Trustworthy transfer learning has been widely applied to artificial intelligence and machine learning fields, including computer vision (Neyshabur et al., 2020), natural language pro- cessing (Ding et al., 2023b), and graph learning (Ruiz et al., 2020). In addition, this section highlights real-world applications of trustworthy transfer learning in scientific discovery.",
    "Agriculture": "Transfer learning techniques have been applied to various precision agriculture applica- tions (Ma et al., 2024; Adve et al., 2024). Specifically, to improve the management of agri- cultural stakeholders, Zhang et al. (2021) and Wang et al. (2023b) propose process-guided 40 machine learning frameworks, which transfer knowledge from simulated data generated by soil-vegetation radiative transfer modeling to real-world field data for precise monitoring of cover crop traits. Wan et al. (2022) analyze the transferability of support vector regression models for estimating leaf nitrogen concentration across different plant species. Besides, pre-trained vision models have been fine-tuned for crop mapping (Jo et al., 2022), crop pest classification (Thenmozhi & Reddy, 2019), and plant phenotyping (Sama et al., 2023).",
    "Bioinformatics": "Notably, Theodoris et al. (2023) have introduced an attention-based foundation model Gene- former pre-trained on over 30 million single-cell transcriptomes to capture network dynamics (e.g., gene interactions). They also demonstrate the effectiveness of Geneformer in various downstream tasks with limited data through fine-tuning. Later, Hou and Ji (2024) illus- trate the efficacy of the pre-trained large language model GPT-4 in cell type annotation of single-cell RNA-seq data. Besides, Hu et al. (2020) and Aminzadeh et al. (2024) develop unified transfer learning frameworks for open-world single-cell annotation across different species and tissues, as well as for batch correction and multi-omics integration. Similarly, Mieth et al. (2019) study the clustering of single-cell RNA-seq data on the small disease- or tissue-specific data sets by leveraging prior knowledge from large reference data sets. Hetzel et al. (2022) and Lotfollahi et al. (2022) leverage architecture surgery based transfer learning techniques to understand cellular heterogeneity. In addition, recent efforts (Rao et al., 2019; Detlefsen et al., 2022; Heinzinger et al., 2019) have been devoted to protein representation learning for downstream tasks using language models pre-trained on a large protein corpus. Typically, Rao et al. (2019) introduce a protein transfer learning benchmark TAPE for learning transferable protein representation, while Detlefsen et al. (2022) further improve the quality of protein representation by considering the geometry of representation space. Dieckhaus et al. (2024) also exploit the pre-trained ProteinMPNN model (Dauparas et al., 2022) to extract embeddings of input proteins, which are then used to predict stability changes for protein point mutations.",
    "Healthcare": "Transfer learning advances the development of effective and efficient health care services (Ja- yaraman et al., 2020). For example, Chen et al. (2020) develop a federated transfer learn- ing framework for privacy-preserving wearable healthcare systems (e.g., Parkinsonâ€™s disease auxiliary diagnosis). Raghu et al. (2019) and Matsoukas et al. (2022) further understand the impact of the source domain/model on the downstream medical imaging tasks in the context of transfer learning. To enforce health equity across ethnic groups, Gao and Cui (2020), Toseef et al. (2022), and Lee et al. (2023a) propose transferring knowledge from majority groups with sufficient data to minority groups with limited data. In addition, transfer learning techniques have been applied to drug discovery (Chenjing et al., 2020). Specifically, Yao et al. (2021) propose a functional rationalized meta-learning algorithm to enable knowledge transfer across assays for virtual screening and ADMET prediction. Goh et al. (2018) and DalkÄ±ran et al. (2023) adopt pre-training and fine-tuning strategies for molecular property prediction and drug-target interaction prediction, respectively. 41",
    "Education": "Transfer learning has been studied in Educational Data Mining (EDM) for predicting stu- dent performance in higher education (Hunt et al., 2017). Over the past decade, Massive Open Online Courses (MOOCs) have supported millions of learners around the world. Early predictions of student performance are crucial for enabling timely interventions in these courses. Transfer learning has been explored to predict student performance in on- going courses by leveraging knowledge from previous courses. To be specific, Boyer and Veeramachaneni (2015) leverage knowledge from both previous courses and previous weeks of the same course to make real-time predictions for learners in MOOCs. Instead of relying on handcrafted features, Ding et al. (2019) aim to learn domain-invariant representation by using an auto-encoder and correlation alignment (Sun & Saenko, 2016) between source and target courses. Similarly, Swamy et al. (2022) study the transferability of early success pre- diction models across MOOCs from different domains and topics. Besides, Schmucker and Mitchell (2022) explore the transferability of student performance in addressing the cold- start problem for new courses in intelligent tutoring systems. More recently, large language models such as GPT-4 and ChatGPT have gained significant attention for improving in- structional efficiency and student engagement (e.g., by creating interactive homework with feedback and follow-up questions) (Vanzo et al., 2024; Kasneci et al., 2023).",
    "Robotics": "Sim-to-real transfer aims to transfer knowledge from simulation to real-world environments when training reinforcement learning models for robotic learning (Dai et al., 2024; Peng et al., 2018). Recently, this framework has been studied in various robotic learning tasks, including Rubikâ€™s cube (OpenAI et al., 2019), human pose estimation (Doersch & Zisserman, 2019), vision-and-language navigation (Anderson et al., 2020), biped locomotion (Yu et al., 2019), etc. Specifically, various strategies have been proposed to address the domain shift between simulation and real-world environments (Tzeng et al., 2020; Peng et al., 2018; Pinto et al., 2017; Rusu et al., 2017). One is to use distribution alignment regularization to learn domain-invariant representation (Tzeng et al., 2020; Tanwani, 2020). Another strategy is domain randomization (Andrychowicz et al., 2020; Tobin et al., 2017), which aims to train the model using a diverse set of randomized simulated environments, rather than relying on a single simulated environment. Chen et al. (2022) and Hu et al. (2023) further theoretically highlight the benefits of domain randomization for sim-to-real transfer.",
    "E-commerce": "Cross-domain recommendation aims to generate reliable recommendations in a target do- main by exploiting knowledge from source recommender systems. It has been studied from various perspectives, e.g., matrix factorization (Man et al., 2017; Samra et al., 2024), neural collaborative filtering (Hu et al., 2018; Kanagawa et al., 2019; Li & Tuzhilin, 2020), graph neural network (Wu et al., 2023; Zhao et al., 2019a), large language models (Petruzzelli et al., 2024), etc. In addition to prediction accuracy, recent works also investigate the trustworthiness properties of cross-domain recommender systems, such as adversarial vul- nerability (Chen & Li, 2019) and user privacy (Yang et al., 2024). 42",
    "6. Open Questions and Future Trends": "Despite the rapidly increasing research interest and applications of trustworthy transfer learning in both academia and industry, there remain many open questions, especially in the theoretical understanding of trustworthy transfer learning.",
    "Benchmarking Negative Transfer": "Negative transfer can roughly be defined as the phenomenon (Pan & Yang, 2010) where â€œ transferring knowledge from the source can have a negative impact on the target learner â€. Definition 12 (Negative Transfer (Pan & Yang, 2010; Wang et al., 2019)) Given a learning algorithm A tl , source data D S , and target data D T , negative transfer occurs if the following condition holds: E T \u0010 A tl ( D S , D T ) \u0011 > E T \u0010 A tl ( âˆ… , D T ) \u0011 (25) where E T ( A tl ( S )) represents the expected error on the target distribution P T when the learn- ing algorithm A tl is trained on data S . This definition reveals (Ben-David et al., 2010; Kuzborskij & Orabona, 2013; Wang et al., 2019) that given a learning algorithm, there are two major factors determining if negative transfer occurs: the distribution discrepancy between source and target domains and the size of the labeled target data . Negative transfer has been observed theoretically and empirically in various applications (Rosenstein et al., 2005; Ben-David et al., 2010; Zhao et al., 2019b; Wang et al., 2019). Recent work (Cohen-Wang et al., 2024) also explores identifying and characterizing the failure modes that pre-training can and cannot address. Despite the extensive work on transfer learning techniques, up until now, little effort (if any) has been devoted to rigorously understanding the boundary between positive and negative transfer given a learning algorithm. It remains an open question to determine when the negative transfer will occur given finite source and target samples (or a source model and finite target samples). Therefore, rather than focusing on performance improvement, more efforts can be dedicated to benchmarking the negative transfer of transfer learning models, e.g., the change from positive to negative transfer can be affected by the magnitude of distribution shifts and the number of target samples. This could provide valuable insights into when a transfer learning model can work well for real-world applications.",
    "Cross-modal Transferability": "Cross-modal transfer learning (Shen et al., 2023; Dinh et al., 2022; Socher et al., 2013) aims at understanding knowledge transferability when the source and target domains have differ- ent types of data modalities, e.g., transferring knowledge from a text-based source domain to an image-based target domain. This differs from multi-modal learning (Radford et al., 2021; Huang et al., 2021) which maps different data modalities in a unified latent feature space over pair-wise training samples. In contrast, cross-modal transfer learning focuses on investigating what knowledge can be transferred across data modalities. Although large language models (LLMs) have been applied to various scientific discovery tasks (Dinh et al., 43 2022), it is unclear what knowledge is being transferred in this process. Additionally, there is a lack of theoretical understanding regarding how LLMs generalize to downstream tasks with different data modalities.",
    "Physics-Informed Transfer Learning": "Physics-informed machine learning (Karniadakis et al., 2021; Raissi et al., 2019) aims to im- prove the training of machine learning models by incorporating physical domain knowledge as soft constraints on an empirical loss function. This alleviates the need for a large amount of high-quality data when training deep neural networks to solve scientific problems. Re- cent studies have introduced transfer learning to understand the knowledge transferability of physics-informed neural networks across tasks. For example, Desai et al. (2022) study the transferability of physics-informed neural networks across differential equations. Goswami et al. (2022) and Xu et al. (2023) study the transfer learning performance of deep operator networks across partial differential equations. Subramanian et al. (2023) further analyze the transfer behavior of neural operators pre-trained on a mixture of different physics problems. However, the theoretical explanation regarding the generalization performance of physics- informed neural networks under distribution shifts is unclear. Besides, it can be seen that in the context of transfer learning, the source knowledge can be provided from multiple aspects, including labeled source samples (Wiles et al., 2022) (e.g., Subsection 3.1.1), pre- trained source models (OpenAI, 2023) (e.g., Subsection 4.1.1), synthetic data generated by physics-based simulators (Andrychowicz et al., 2020) (e.g., Subsection 5.5), and fundamen- tal physical rules (Karniadakis et al., 2021). This can motivate a generic physics-informed transfer learning problem involving multi-faceted knowledge transfer from the source to the target domains.",
    "Trade-off between Transferability and Trustworthiness": "In standard machine learning, the trade-off between prediction accuracy and trustworthi- ness has been theoretically studied e.g., accuracy vs. group fairness (Zhao & Gordon, 2019; Dutta et al., 2020), accuracy vs. adversarial robustness (Tsipras et al., 2019; Zhang et al., 2019; Yang et al., 2020), accuracy vs. privacy (Bietti et al., 2022), accuracy vs. explain- ability (Zarlenga et al., 2022), etc. It has been observed that trustworthiness may be at odds with the prediction accuracy in a single domain. In contrast, recent works (Salman et al., 2020; Davchev et al., 2019) reveal that both trustworthiness (e.g., adversarial ro- bustness) and prediction accuracy can be improved in the target domain by leveraging relevant knowledge from source domains. This motivates us to re-think the fundamental trade-off between knowledge transferability and trustworthiness in the context of transfer learning. Specifically, there are several open questions: (1) Can source knowledge consis- tently enhance trustworthiness and transfer accuracy in the target domain under various distribution shifts and data modalities? (2) Is there an inherent trade-off between trust- worthiness and transfer accuracy in the target domain when discovering and transferring knowledge from the source data/model? These studies will significantly expand the appli- cation of transfer learning techniques by clarifying when trained models can be trusted and how well they perform. 44",
    "7. Conclusion": "In this survey, we provide a comprehensive review of trustworthy transfer learning from the perspective of knowledge transferability and trustworthiness. With different data and model assumptions, much effort has been devoted to understanding the generalization per- formance of trustworthy transfer learning and designing advanced techniques in quantifying and enhancing knowledge transfer in a variety of real-world applications. Besides, we also summarize several open questions for trustworthy transfer learning, including benchmark- ing positive and negative transfer, enabling unified knowledge transfer across different data modalities and physical rules, and achieving the inherent trade-off between transferabil- ity and trustworthiness. Ultimately, trustworthy transfer learning could lead to a unified machine learning and artificial intelligence framework that facilitates positive knowledge reuse and transfer in the presence of distribution shifts and across data modalities, while maintaining rigorous standards of trustworthiness.",
    "References": "Abdali, S., He, J., Barberan, C. J., & Anarfi, R. (2024). Can LLMs be fooled? investigating vulnerabilities in LLMs. CoRR , abs/2407.20529 . Achituve, I., Shamsian, A., Navon, A., Chechik, G., & Fetaya, E. (2021). Personalized fed- erated learning with gaussian processes. Advances in Neural Information Processing Systems , 34 , 8392â€“8406. Acuna, D., Zhang, G., Law, M. T., & Fidler, S. (2021). f -domain adversarial learning: Theory and algorithms. In International Conference on Machine Learning , Vol. 139, pp. 66â€“75. PMLR. Adve, V. S., Wedow, J. M., Ainsworth, E. A., Chowdhary, G., Green-Miller, A., & Tucker, C. (2024). AIFARMS: artificial intelligence for future agricultural resilience, manage- ment, and sustainability. AI Magazine , 45 (1), 83â€“88. Aghajanyan, A., Gupta, S., & Zettlemoyer, L. (2021). Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 7319â€“ 7328. Aghbalou, A., & Staerman, G. (2023). Hypothesis transfer learning with surrogate classi- fication losses: Generalization bounds through algorithmic stability. In International Conference on Machine Learning , pp. 280â€“303. PMLR. Agostinelli, A., PÂ´andy, M., Uijlings, J. R. R., Mensink, T., & Ferrari, V. (2022a). How stable are transferability metrics evaluations?. In European Conference on Computer Vision , pp. 303â€“321. Springer. Agostinelli, A., Uijlings, J. R. R., Mensink, T., & Ferrari, V. (2022b). Transferability metrics for selecting source model ensembles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 7926â€“7936. IEEE. 45 Ahmed, E., Darwish, A., & Hassanien, A. E. (2024). Flowers classification with low car- bon footprint using deep learning pretrained models. In Artificial Intelligence for Environmental Sustainability and Green Initiatives , pp. 51â€“69. Springer. Alhamoud, K., Hammoud, H. A. A. K., Alfarra, M., & Ghanem, B. (2023). Generalizability of adversarial robustness under distribution shifts. Transactions on Machine Learning Research , 2023 . Aminzadeh, F., Wu, J., He, J., Saberi, M., & Vafaee, F. (2024). Single-cell data integration and cell type annotation through contrastive adversarial open-set domain adaptation. bioRxiv , 2024â€“10 . Amirizaniani, M., Martin, E., Roosta, T., Chadha, A., & Shah, C. (2024). AuditLLM: a tool for auditing large language models using multiprobe approach. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management , pp. 5174â€“5179. An, B., Che, Z., Ding, M., & Huang, F. (2022). Transferring fairness under distribution shifts via fair consistency regularization. In Advances in Neural Information Processing Systems 35 . Anderson, P., Shrivastava, A., Truong, J., Majumdar, A., Parikh, D., Batra, D., & Lee, S. (2020). Sim-to-real transfer for vision-and-language navigation. In 4th Conference on Robot Learning , pp. 671â€“681. PMLR. Andrychowicz, M., Baker, B., Chociej, M., JÂ´ozefowicz, R., McGrew, B., Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., Schneider, J., Sidor, S., Tobin, J., Welinder, P., Weng, L., & Zaremba, W. (2020). Learning dexterous in-hand manipulation. The International Journal of Robotics Research , 39 (1). Angelopoulos, A. N., & Bates, S. (2021). A gentle introduction to conformal prediction and distribution-free uncertainty quantification. CoRR , abs/2107.07511 . Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. (2023). Palm 2 technical report. CoRR , abs/2305.10403 . Ansell, A., Ponti, E. M., Korhonen, A., & Vulic, I. (2022). Composable sparse fine-tuning for cross-lingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1778â€“1796. Arik, S. Â¨ O., Yoder, N. C., & Pfister, T. (2022). Self-adaptive forecasting for improved deep learning on non-stationary time-series. CoRR , abs/2202.02403 . Arivazhagan, M. G., Aggarwal, V., Singh, A. K., & Choudhary, S. (2019a). Federated learning with personalization layers. CoRR , abs/1912.00818 . Arivazhagan, N., Bapna, A., Firat, O., Aharoni, R., Johnson, M., & Macherey, W. (2019b). The missing ingredient in zero-shot neural machine translation. CoRR , abs/1903.07091 . Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein generative adversarial net- works. In International Conference on Machine Learning , pp. 214â€“223. PMLR. 46 Arora, S., Du, S. S., Hu, W., Li, Z., & Wang, R. (2019). Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning , pp. 322â€“332. PMLR. Awais, M., Zhou, F., Xu, H., Hong, L., Luo, P., Bae, S., & Li, Z. (2021). Adversarial robustness for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 8548â€“8557. IEEE. B., V. K., Bachu, S., Garg, T., Narasimhan, N. L., Konuru, R., & Balasubramanian, V. N. (2023). Building a winning team: Selecting source model ensembles using a submodu- lar transferability estimation approach. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 11575â€“11586. IEEE. Backhausz, Â´ A., & Szegedy, B. (2022). Action convergence of operators and graphs. Canadian Journal of Mathematics , 74 (1), 72â€“121. Bai, R., Bagchi, S., & Inouye, D. I. (2024). Benchmarking algorithms for federated domain generalization. In The Twelfth International Conference on Learning Representations . Bao, W., Wang, H., Wu, J., & He, J. (2023a). Optimizing the collaboration structure in cross-silo federated learning. In International Conference on Machine Learning , Vol. 202, pp. 1718â€“1736. PMLR. Bao, W., Wei, T., Wang, H., & He, J. (2023b). Adaptive test-time personalization for federated learning. Advances in Neural Information Processing Systems , 36 . Bao, Y., Li, Y., Huang, S., Zhang, L., Zheng, L., Zamir, A., & Guibas, L. J. (2019). An information-theoretic approach to transferability in task transfer learning. In 2019 IEEE International Conference on Image Processing , pp. 2309â€“2313. IEEE. Bapna, A., & Firat, O. (2019). Simple, scalable adaptation for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing , pp. 1538â€“1548. Barber, R. F., Cand`es, E. J., Ramdas, A., & Tibshirani, R. J. (2021). Predictive inference with the jackknife+. The Annals of Statistics , 49 (1), 486 â€“ 507. Begoli, E., Bhattacharya, T., & Kusnezov, D. (2019). The need for uncertainty quantifica- tion in machine-assisted medical decision making. Nature Machine Intelligence , 1 (1), 20â€“23. Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., & Vaughan, J. W. (2010). A theory of learning from different domains. Machine Learning , 79 (1-2), 151â€“175. Ben-David, S., Blitzer, J., Crammer, K., & Pereira, F. (2006). Analysis of representations for domain adaptation. Advances in Neural Information Processing Systems , 19 . Ben-David, S., Lu, T., Luu, T., & PÂ´al, D. (2010). Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , pp. 129â€“136. Bevilacqua, B., Zhou, Y., & Ribeiro, B. (2021). Size-invariant graph representations for graph classification extrapolations. In International Conference on Machine Learning , pp. 837â€“851. PMLR. 47 Bhatt, U., AntorÂ´an, J., Zhang, Y., Liao, Q. V., Sattigeri, P., Fogliato, R., MelanÂ¸con, G. G., Krishnan, R., Stanley, J., Tickoo, O., Nachman, L., Chunara, R., Srikumar, M., Weller, A., & Xiang, A. (2021). Uncertainty as a form of transparency: Measuring, commu- nicating, and using uncertainty. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , pp. 401â€“413. ACM. Bhattacharyya, A. (1946). On a measure of divergence between two multinomial popula- tions. SankhyÂ¯a: The Indian Journal of Statistics , 7 (4), 401â€“406. Bietti, A., Wei, C., DudÂ´Ä±k, M., Langford, J., & Wu, Z. S. (2022). Personalization improves privacy-accuracy tradeoffs in federated learning. In International Conference on Ma- chine Learning , pp. 1945â€“1962. PMLR. Biswas, A., & Mukherjee, S. (2021). Ensuring fairness under prior probability shifts. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , pp. 414â€“ 424. Blanchard, G., Lee, G., & Scott, C. (2011). Generalizing from several related classification tasks to a new unlabeled sample. In Advances in Neural Information Processing Systems 24 , pp. 2178â€“2186. Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., & Wortman, J. (2007). Learning bounds for domain adaptation. Advances in Neural Information Processing Systems , 20 . Bolya, D., Mittapalli, R., & Hoffman, J. (2021). Scalable diverse model selection for acces- sible transfer learning. In Advances in Neural Information Processing Systems 34 , pp. 19301â€“19312. Bonilla, E. V., Chai, K., & Williams, C. (2007). Multi-task gaussian process prediction. Advances in Neural Information Processing Systems , 20 . Boudiaf, M., Denton, T., Van MerriÂ¨enboer, B., Dumoulin, V., & Triantafillou, E. (2023). In search for a generalizable method for source free domain adaptation. In International Conference on Machine Learning , pp. 2914â€“2931. PMLR. Bovens, M. (2007). Analysing and assessing accountability: A conceptual framework. Eu- ropean Law Journal , 13 (4), 447â€“468. Bowen, D., Murphy, B., Cai, W., Khachaturov, D., Gleave, A., & Pelrine, K. (2024). Scaling laws for data poisoning in LLMs. CoRR , abs/2408.02946 . Boyer, S., & Veeramachaneni, K. (2015). Transfer learning for predictive models in massive open online courses. In Artificial Intelligence in Education , pp. 54â€“63. Springer. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 . Brunton, S. L., BudiË‡siÂ´c, M., Kaiser, E., & Kutz, J. N. (2022). Modern koopman theory for dynamical systems. SIAM Review , 64 (2), 229â€“340. 48 Budennyy, S. A., Lazarev, V. D., Zakharenko, N., Korovin, A. N., Plosskaya, O., Dimitrov, D., Arkhipkin, V., Oseledets, I. V., Barsola, I., Egorov, I., Kosterina, A., & Zhukov, L. (2022). Eco2AI: Carbon emissions tracking of machine learning models as the first step towards sustainable AI. In Doklady Mathematics , Vol. 106, pp. S118â€“S128. Springer. Budhathoki, K., Janzing, D., Bloebaum, P., & Ng, H. (2021). Why did the distribution change?. In International Conference on Artificial Intelligence and Statistics , pp. 1666â€“1674. PMLR. Bugliarello, E., Liu, F., Pfeiffer, J., Reddy, S., Elliott, D., Ponti, E. M., & Vulic, I. (2022). IGLUE: A benchmark for transfer learning across modalities, tasks, and languages. In International Conference on Machine Learning , pp. 2370â€“2392. PMLR. Cai, R., Chen, J., Li, Z., Chen, W., Zhang, K., Ye, J., Li, Z., Yang, X., & Zhang, Z. (2021a). Time series domain adaptation via sparse associative structure alignment. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35, pp. 6859â€“ 6867. Cai, T., Gao, R., Lee, J., & Lei, Q. (2021b). A theory of label propagation for subpopulation shift. In International Conference on Machine Learning , pp. 1170â€“1182. PMLR. Cai, X., Xu, H., Xu, S., Zhang, Y., & Yuan, X. (2022). BadPrompt: backdoor attacks on continuous prompts. In Advances in Neural Information Processing Systems 35 . Cao, B., Pan, S. J., Zhang, Y., Yeung, D.-Y., & Yang, Q. (2010). Adaptive transfer learning. In proceedings of the AAAI Conference on Artificial Intelligence , Vol. 24, pp. 407â€“412. Castelnovo, A., Crupi, R., Greco, G., Regoli, D., Penco, I. G., & Cosentini, A. C. (2022). A clarification of the nuances in the fairness metrics landscape. Scientific Reports , 12 (1), 4209. Cauchois, M., Gupta, S., Ali, A., & Duchi, J. C. (2024). Robust validation: Confident predic- tions even when distributions shift. Journal of the American Statistical Association , 0 (0), 1â€“66. CerviËœno, J., Ruiz, L., & Ribeiro, A. (2023). Learning by transference: Training graph neural networks on growing graphs. IEEE Transactions on Signal Processing , 71 , 233â€“247. Chan, A. J., Alaa, A. M., Qian, Z., & van der Schaar, M. (2020). Unlabelled data improves bayesian uncertainty calibration under covariate shift. In International Conference on Machine Learning , pp. 1392â€“1402. PMLR. Chatfield, C. (2004). The Analysis of Time Series: An Introduction . CRC Press. Chatterji, N., Neyshabur, B., & Sedghi, H. (2020). The intriguing role of module critical- ity in the generalization of deep networks. In International Conference on Learning Representations . Chen, D., Hu, H., Wang, Q., Li, Y., Wang, C., Shen, C., & Li, Q. (2021). CARTL: cooper- ative adversarially-robust transfer learning. In International Conference on Machine Learning , pp. 1640â€“1650. PMLR. 49 Chen, H., & Li, J. (2019). Data poisoning attacks on cross-domain recommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management , pp. 2177â€“2180. Chen, J., & Zhang, A. (2024). On disentanglement of asymmetrical knowledge transfer for modality-task agnostic federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38, pp. 11311â€“11319. Chen, T., Liu, S., Chang, S., Cheng, Y., Amini, L., & Wang, Z. (2020). Adversarial robustness: From self-supervised pre-training to fine-tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 696â€“705. Chen, X., Hu, J., Jin, C., Li, L., & Wang, L. (2022). Understanding domain randomization for sim-to-real transfer. In International Conference on Learning Representations . Chen, X., Wang, S., Wang, J., & Long, M. (2021). Representation subspace distance for domain adaptation regression. In International Conference on Machine Learning , pp. 1749â€“1759. PMLR. Chen, Y., Raab, R., Wang, J., & Liu, Y. (2022). Fairness transferability subject to bounded distribution shift. Advances in Neural Information Processing Systems , 35 , 11266â€“ 11278. Chen, Y., Huang, Y., Du, S. S., Jamieson, K. G., & Shi, G. (2023). Active representation learning for general task space with applications in robotics. Advances in Neural Information Processing Systems , 36 . Chen, Y., Jamieson, K., & Du, S. (2022). Active multi-task representation learning. In International Conference on Machine Learning , pp. 3271â€“3298. PMLR. Chen, Y., Qin, X., Wang, J., Yu, C., & Gao, W. (2020). Fedhealth: A federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems , 35 (4), 83â€“93. Chenjing, C., Wang, S., Xu, Y., Zhang, W., Tang, K., Ouyang, Q., Lai, L., & Pei, J. (2020). Transfer learning for drug discovery. Journal of Medicinal Chemistry , 63 (16), 8683â€“ 8694. Chi, H., Liu, F., Yang, W., Lan, L., Liu, T., Han, B., Cheung, W. K., & Kwok, J. T. (2021). TOHAN: A one-step approach towards few-shot hypothesis adaptation. In Advances in Neural Information Processing Systems 34 , pp. 20970â€“20982. Chin, T., Zhang, C., & Marculescu, D. (2021). Renofeation: A simple transfer learning method for improved adversarial robustness. In IEEE Conference on Computer Vision and Pattern Recognition Workshops , pp. 3243â€“3252. Cho, Y. J., Jhunjhunwala, D., Li, T., Smith, V., & Joshi, G. (2022). To federate or not to federate: Incentivizing client participation in federated learning. In Workshop on Fed- erated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022) . Choi, Y., Bae, S., Ban, S., Jeong, M., Zhang, C., Song, L., Zhao, L., Bian, J., & Kim, K.-E. (2024). Hard prompts made interpretable: Sparse entropy regularization for prompt tuning with rl. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 8252â€“8271. 50 Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., & Bengio, Y. (2015). A recurrent latent variable model for sequential data. In Advances in Neural Information Processing Systems 28 , pp. 2980â€“2988. Cohen, J., Rosenfeld, E., & Kolter, J. Z. (2019). Certified adversarial robustness via random- ized smoothing. In International Conference on Machine Learning , pp. 1310â€“1320. PMLR. Cohen-Wang, B., Vendrow, J., & Madry, A. (2024). Ask your distribution shift if pre- training is right for you. CoRR , abs/2403.00194 . Cole, F., Lu, Y., Oâ€™Neill, R., & Zhang, T. (2024). Provable in-context learning of linear systems and linear elliptic pdes with transformers. CoRR , abs/2409.12293 . Collins, L., Hassani, H., Mokhtari, A., & Shakkottai, S. (2021). Exploiting shared represen- tations for personalized federated learning. In International Conference on Machine Learning , pp. 2089â€“2099. PMLR. Commission, E., Directorate-General for Communications Networks, C., & Technology (2019). Ethics Guidelines for Trustworthy AI . Publications Office. Cortes, C., Mansour, Y., & Mohri, M. (2010). Learning bounds for importance weighting. Advances in neural information processing systems , 23 . Cortes, C., & Mohri, M. (2011). Domain adaptation in regression. In International Con- ference on Algorithmic Learning Theory , pp. 308â€“323. Springer. Cortes, C., Mohri, M., & Medina, A. M. (2015). Adaptation algorithm and theory based on generalized discrepancy. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 169â€“178. ACM. Cortes, C., Mohri, M., Riley, M., & Rostamizadeh, A. (2008). Sample selection bias correc- tion theory. In International Conference on Algorithmic Learning Theory , pp. 38â€“53. Springer. Coston, A., Ramamurthy, K. N., Wei, D., Varshney, K. R., Speakman, S., Mustahsan, Z., & Chakraborty, S. (2019). Fair transfer learning with missing protected attributes. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society , pp. 91â€“98. Courty, N., Flamary, R., Habrard, A., & Rakotomamonjy, A. (2017). Joint distribution optimal transportation for domain adaptation. Advances in Neural Information Pro- cessing Systems , 30 . Courty, N., Flamary, R., Tuia, D., & Rakotomamonjy, A. (2016). Optimal transport for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39 (9), 1853â€“1865. Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. In Burges, C. J. C., Bottou, L., Ghahramani, Z., & Weinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 26 , pp. 2292â€“2300. Dai, Q., Wu, X., Xiao, J., Shen, X., & Wang, D. (2023). Graph transfer learning via adver- sarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering , 35 (5), 4908â€“4922. 51 Dai, T., Wong, J., Jiang, Y., Wang, C., Gokmen, C., Zhang, R., Wu, J., & Fei-Fei, L. (2024). ACDC: Automated creation of digital cousins for robust policy learning. In 8th Annual Conference on Robot Learning . DalkÄ±ran, A., Atakan, A., RifaioË˜glu, A. S., Martin, M. J., Atalay, R. CÂ¸., Acar, A. C., DoË˜gan, T., & Atalay, V. (2023). Transfer learning for drug-target interaction prediction. Bioinformatics , 39 , i103â€“i110. Das, S. S. S., Zhang, H., Shi, P., Yin, W., & Zhang, R. (2023). Unified low-resource sequence labeling by sample-aware dynamic sparse finetuning. In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing , pp. 6998â€“7010. Dauparas, J., Anishchenko, I. V., Bennett, N. R., Bai, H., Ragotte, R. J., Milles, L. F., Wicky, B. I. M., Courbet, A., de Haas, R. J., Bethel, N. P., Leung, P. J. Y., Huddy, T. F., Pellock, S. J., Tischer, D. K., Chan, F., Koepnick, B., Nguyen, H., Kang, A., Sankaran, B., Bera, A. K., King, N. P., & Baker, D. (2022). Robust deep learningâ€“ based protein sequence design using ProteinMPNN. Science , 378 (6615), 49â€“56. Davchev, T., Korres, T., Fotiadis, S., Antonopoulos, N., & Ramamoorthy, S. (2019). An empirical evaluation of adversarial robustness under transfer learning. In ICML 2019 Workshop on Understanding and Improving Generalization in Deep Learning . Dayan, I., Roth, H. R., Zhong, A., Harouni, A., Gentili, A., Abidin, A. Z., Liu, A., Costa, A. B., Wood, B. J., Tsai, C.-S., et al. (2021). Federated learning for predicting clinical outcomes in patients with COVID-19. Nature Medicine , 27 (10), 1735â€“1743. Defferrard, M., Bresson, X., & Vandergheynst, P. (2016). Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems 29 , pp. 3837â€“3845. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: a large- scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pp. 248â€“255. IEEE. Deng, M., Wang, J., Hsieh, C., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E. P., & Hu, Z. (2022). RLPrompt: optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 3369â€“3391. Deng, Y., Kamani, M. M., & Mahdavi, M. (2020). Adaptive personalized federated learning. CoRR , abs/2003.13461 . Deng, Z., Zhang, L., Vodrahalli, K., Kawaguchi, K., & Zou, J. Y. (2021). Adversarial training helps transfer learning via better representations. In Advances in Neural Information Processing Systems 34 , pp. 25179â€“25191. Desai, S., Mattheakis, M., Joy, H., Protopapas, P., & Roberts, S. J. (2022). One-shot transfer learning of physics-informed neural networks. In ICML 2022 2nd AI for Science Workshop . Deshpande, A., Achille, A., Ravichandran, A., Li, H., Zancato, L., Fowlkes, C. C., Bhotika, R., Soatto, S., & Perona, P. (2021). A linearized framework and a new benchmark for model selection for fine-tuning. CoRR , abs/2102.00084 . 52 Detlefsen, N. S., Hauberg, S., & Boomsma, W. (2022). Learning meaningful representations of protein sequences. Nature Communications , 13 (1), 1914. Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: pre-training of deep bidirec- tional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies , pp. 4171â€“4186. Dieckhaus, H., Brocidiacono, M., Randolph, N. Z., & Kuhlman, B. (2024). Transfer learn- ing to leverage larger datasets for improved prediction of protein stability changes. Proceedings of the National Academy of Sciences , 121 (6), e2314853121. Ding, M., Wang, Y., Hemberg, E., & Oâ€™reilly, U.-M. (2019). Transfer learning using represen- tation learning in massive open online courses. In Proceedings of the 9th International Conference on Learning Analytics & Knowledge , pp. 145â€“154. Ding, N., Chen, X., Levinboim, T., Changpinyo, S., & Soricut, R. (2022). PACTran: pac- bayesian metrics for estimating the transferability of pretrained models to classifica- tion tasks. In European Conference on Computer Vision , pp. 252â€“268. Springer. Ding, N., Lv, X., Wang, Q., Chen, Y., Zhou, B., Liu, Z., & Sun, M. (2023a). Sparse low-rank adaptation of pre-trained language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pp. 4133â€“4145. Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen, Y., Chan, C., Chen, W., Yi, J., Zhao, W., Wang, X., Liu, Z., Zheng, H., Chen, J., Liu, Y., Tang, J., Li, J., & Sun, M. (2023b). Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence , 5 (3), 220â€“235. Dinh, C. T., Tran, N. H., & Nguyen, T. D. (2020). Personalized federated learning with moreau envelopes. In Advances in Neural Information Processing Systems 33 . Dinh, T., Zeng, Y., Zhang, R., Lin, Z., Gira, M., Rajput, S., Sohn, J., Papailiopoulos, D. S., & Lee, K. (2022). LIFT: language-interfaced fine-tuning for non-language machine learning tasks. In Advances in Neural Information Processing Systems 35 . Doersch, C., & Zisserman, A. (2019). Sim2real transfer learning for 3d human pose esti- mation: Motion to the rescue. In Advances in Neural Information Processing Systems 32 , pp. 12929â€“12941. Donahue, K., & Kleinberg, J. (2021). Model-sharing games: Analyzing federated learning under voluntary participation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35, pp. 5303â€“5311. Dong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., & Long, M. (2023a). SimMTM: A simple pre-training framework for masked time-series modeling. In Advances in Neural Information Processing Systems 36 . Dong, R., Liu, F., Chi, H., Liu, T., Gong, M., Niu, G., Sugiyama, M., & Han, B. (2023b). Diversity-enhancing generative network for few-shot hypothesis adaptation. In Inter- national Conference on Machine Learning , pp. 8260â€“8275. PMLR. Dong, X., Luu, A. T., Lin, M., Yan, S., & Zhang, H. (2021). How should pre-trained language models be fine-tuned towards adversarial robustness?. In Advances in Neural Information Processing Systems 34 , pp. 4356â€“4369. 53 Du, S. S., Koushik, J., Singh, A., & PÂ´oczos, B. (2017). Hypothesis transfer learning via transformation functions. In Advances in Neural Information Processing Systems 30 , pp. 574â€“584. Du, S. S., Hu, W., Kakade, S. M., Lee, J. D., & Lei, Q. (2021a). Few-shot learning via learning the representation, provably. In International Conference on Learning Rep- resentations . Du, Y., Wang, J., Feng, W., Pan, S. J., Qin, T., Xu, R., & Wang, C. (2021b). AdaRNN: adaptive learning and forecasting of time series. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management , pp. 402â€“411. ACM. Dudley, R. (2002). Real Analysis and Probability . Cambridge University Press. Dunlap, L., Zhang, Y., Wang, X., Zhong, R., Darrell, T., Steinhardt, J., Gonzalez, J. E., & Yeung-Levy, S. (2024). Describing differences in image sets with natural language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion , pp. 24199â€“24208. Dutt, R., Bohdal, O., Tsaftaris, S. A., & Hospedales, T. (2024). FairTune: optimizing parameter efficient fine tuning for fairness in medical image analysis. In International Conference on Learning Representations . Dutta, S., Wei, D., Yueksel, H., Chen, P., Liu, S., & Varshney, K. R. (2020). Is there a trade-off between fairness and accuracy? A perspective using mismatched hypothesis testing. In International Conference on Machine Learning , pp. 2803â€“2813. PMLR. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. S. (2012). Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference , pp. 214â€“226. ACM. Eshete, B. (2021). Making machine learning trustworthy. Science , 373 (6556), 743â€“744. Fallah, A., Mokhtari, A., & Ozdaglar, A. E. (2020). Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In Advances in Neural Information Processing Systems 33 . Fan, W., Wang, P., Wang, D., Wang, D., Zhou, Y., & Fu, Y. (2023a). Dish-TS: A general paradigm for alleviating distribution shift in time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37, pp. 7522â€“7529. Fan, Z., Ding, H., Deoras, A., & Hoang, T. N. (2023b). Personalized federated domain adaptation for item-to-item recommendation. In Uncertainty in Artificial Intelligence , pp. 560â€“570. PMLR. Fang, Z., Lu, J., Liu, F., Xuan, J., & Zhang, G. (2020). Open set domain adaptation: The- oretical bound and algorithm. IEEE Transactions on Neural Networks and Learning Systems , 32 (10), 4309â€“4322. Fannjiang, C., Bates, S., Angelopoulos, A. N., Listgarten, J., & Jordan, M. I. (2022). Con- formal prediction under feedback covariate shift for biomolecular design. Proceedings of the National Academy of Sciences , 119 (43), e2204569119. 54 Fatras, K., SÂ´ejournÂ´e, T., Flamary, R., & Courty, N. (2021). Unbalanced minibatch optimal transport; applications to domain adaptation. In International Conference on Machine Learning , pp. 3186â€“3197. PMLR. Fawaz, H. I., Forestier, G., Weber, J., Idoumghar, L., & Muller, P. (2018). Transfer learning for time series classification. In IEEE International Conference on Big Data , pp. 1367â€“ 1376. IEEE. Fei-Fei, L., Fergus, R., & Perona, P. (2006). One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence , 28 (4), 594â€“611. Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015). Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 259â€“268. ACM. Feng, H., You, Z., Chen, M., Zhang, T., Zhu, M., Wu, F., Wu, C., & Chen, W. (2021). KD3A: unsupervised multi-source decentralized domain adaptation via knowledge distillation. In International Conference on Machine Learning , pp. 3274â€“3283. PMLR. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., March, M., & Lempitsky, V. (2016). Domain-adversarial training of neural networks. Journal of Machine Learning Research , 17 (59), 1â€“35. Gao, Y., & Cui, Y. (2020). Deep transfer learning for reducing health care disparities arising from biomedical data inequality. Nature Communications , 11 (1), 5131. Germain, P., Bach, F. R., Lacoste, A., & Lacoste-Julien, S. (2016). Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems 29 , pp. 1876â€“1884. Gheini, M., Ren, X., & May, J. (2021). Cross-attention is all you need: Adapting pre- trained transformers for machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 1754â€“1765. Ghifary, M., Balduzzi, D., Kleijn, W. B., & Zhang, M. (2016). Scatter component anal- ysis: A unified framework for domain adaptation and domain generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39 (7), 1414â€“1430. Ghosh, A., Chung, J., Yin, D., & Ramchandran, K. (2020). An efficient framework for clustered federated learning. In Advances in Neural Information Processing Systems 33 . Gibbs, I., & Cand`es, E. J. (2021). Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems , 34 , 1660â€“1672. Gibbs, I., & Cand`es, E. J. (2024). Conformal inference for online prediction with arbitrary distribution shifts. Journal of Machine Learning Research , 25 (162), 1â€“36. Giguere, S., Metevier, B., Brun, Y., Thomas, P. S., Niekum, S., & da Silva, B. C. (2022). Fairness guarantees under demographic shift. In International Conference on Learning Representations . 55 Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural message passing for quantum chemistry. In International Conference on Machine Learning , pp. 1263â€“1272. PMLR. Goh, G. B., Siegel, C., Vishnu, A., & Hodas, N. (2018). Using rule-based labels for weak supervised learning: A chemnet for transferable chemical property prediction. In Pro- ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 302â€“310. Gong, T., Kim, Y., Lee, T., Chottananurak, S., & Lee, S.-J. (2023). SoTTA: Robust test- time adaptation on noisy data streams. Advances in Neural Information Processing Systems , 36 . Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). Explaining and harnessing adversarial examples. In 3rd International Conference on Learning Representations . Goodman, B., & Flaxman, S. (2017). European union regulations on algorithmic decision- making and a â€œright to explanationâ€. AI Magazine , 38 (3), 50â€“57. Goswami, S., Kontolati, K., Shields, M. D., & Karniadakis, G. E. (2022). Deep transfer operator learning for partial differential equations under conditional shift. Nature Machine Intelligence , 4 (12), 1155â€“1164. Gouk, H., Hospedales, T. M., & Pontil, M. (2021). Distance-based regularisation of deep networks for fine-tuning. In 9th International Conference on Learning Representa- tions . Goyal, S., Sun, M., Raghunathan, A., & Kolter, J. Z. (2022). Test time adaptation via conjugate pseudo-labels. In Advances in Neural Information Processing Systems 35 . Gretton, A., Borgwardt, K. M., Rasch, M. J., SchÂ¨olkopf, B., & Smola, A. J. (2012). A kernel two-sample test. Journal of Machine Learning Research , 13 , 723â€“773. Gu, T., Liu, K., Dolan-Gavitt, B., & Garg, S. (2019). BadNets: evaluating backdooring attacks on deep neural networks. IEEE Access , 7 , 47230â€“47244. Gu, Y., Han, X., Liu, Z., & Huang, M. (2022). PPT: pre-trained prompt tuning for few-shot learning. In Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers) , pp. 8410â€“8423. Gulrajani, I., & Lopez-Paz, D. (2021). In search of lost domain generalization. In Interna- tional Conference on Learning Representations . Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In International Conference on Machine Learning , pp. 1321â€“1330. PMLR. Guo, D., Rush, A. M., & Kim, Y. (2021). Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 4884â€“4896. Hambardzumyan, K., Khachatrian, H., & May, J. (2021). WARP: word-level adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing , pp. 4921â€“4933. 56 Hamilton, W. L., Ying, Z., & Leskovec, J. (2017). Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30 , pp. 1024â€“1034. Han, Z., Zhang, Z., Wang, F., He, R., Su, W., Xi, X., & Yin, Y. (2023). Discriminability and transferability estimation: A bayesian source importance estimation approach for multi-source-free domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37, pp. 7811â€“7820. Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S., Eichner, H., Kiddon, C., & Ramage, D. (2018). Federated learning for mobile keyboard prediction. CoRR , abs/1811.03604 . Harding, E. L., Vanto, J. J., Clark, R., Hannah Ji, L., & Ainsworth, S. C. (2019). Under- standing the scope and impact of the california consumer privacy act of 2018. Journal of Data Protection & Privacy , 2 (3), 234â€“253. Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems 29 , pp. 3315â€“3323. Havaldar, S., Chauhan, J., Shanmugam, K., Nandy, J., & Raghuveer, A. (2024). Fairness under covariate shift: Improving fairness-accuracy tradeoff with few unlabeled test samples. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38, pp. 12331â€“12339. Hayou, S., Ghosh, N., & Yu, B. (2024). LoRA+: Efficient low rank adaptation of large models. In International Conference on Machine Learning . He, H., Queen, O., Koker, T., Cuevas, C., Tsiligkaridis, T., & Zitnik, M. (2023). Domain adaptation for time series under feature and label shifts. In International Conference on Machine Learning , Vol. 202, pp. 12746â€“12774. PMLR. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2022). Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations . He, K., Girshick, R., & DollÂ´ar, P. (2019). Rethinking ImageNet pre-training. In Proceedings of the IEEE/CVF international conference on computer vision , pp. 4918â€“4927. Heinzinger, M., Elnaggar, A., Wang, Y., Dallago, C., Nechaev, D., Matthes, F., & Rost, B. (2019). Modeling aspects of the language of life through transfer-learning protein sequences. BMC Bioinformatics , 20 , 1â€“17. Hendrycks, D., Lee, K., & Mazeika, M. (2019). Using pre-training can improve model robustness and uncertainty. In International Conference on Machine Learning , pp. 2712â€“2721. PMLR. Hernandez, D., Kaplan, J., Henighan, T., & McCandlish, S. (2021). Scaling laws for transfer. CoRR , abs/2102.01293 . Hetzel, L., BÂ¨ohm, S., Kilbertus, N., GÂ¨unnemann, S., Lotfollahi, M., & Theis, F. J. (2022). Predicting cellular responses to novel drug perturbations at a single-cell resolution. Advances in Neural Information Processing Systems , 35 , 26711â€“26722. Hinton, G. E., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. CoRR , abs/1503.02531 . 57 Hoffman, J., Mohri, M., & Zhang, N. (2018a). Algorithms and theory for multiple-source adaptation. Advances in Neural Information Processing Systems , 31 . Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., & Darrell, T. (2018b). CyCADA: cycle-consistent adversarial domain adaptation. In International Conference on Machine Learning , pp. 1989â€“1998. PMLR. Hou, W., & Ji, Z. (2024). Assessing gpt-4 for cell type annotation in single-cell RNA-seq analysis. Nature Methods , 21 , 1â€“4. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., & Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning , pp. 2790â€“2799. PMLR. Howard, J., & Ruder, S. (2018). Universal language model fine-tuning for text classifica- tion. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , pp. 328â€“339. Hu, D., Liang, J., Wang, X., & Foo, C.-S. (2024). Pseudo-Calibration: Improving pre- dictive uncertainty estimation in unsupervised domain adaptation. In International Conference on Machine Learning . Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations . Hu, G., Zhang, Y., & Yang, Q. (2018). CoNet: collaborative cross networks for cross- domain recommendation. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management , pp. 667â€“676. ACM. Hu, J., Zhong, H., Jin, C., & Wang, L. (2023). Provable sim-to-real transfer in continuous domain with partial observations. In International Conference on Learning Represen- tations . Hu, J., Li, X., Hu, G., Lyu, Y., Susztak, K., & Li, M. (2020). Iterative transfer learning with neural network for clustering and cell type classification in single-cell RNA-seq analysis. Nature Machine Intelligence , 2 (10), 607â€“618. Hua, A., Gu, J., Xue, Z., Carlini, N., Wong, E., & Qin, Y. (2024). Initialization matters for adversarial transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 24831â€“24840. Huang, J., Gretton, A., Borgwardt, K., SchÂ¨olkopf, B., & Smola, A. (2006). Correcting sample selection bias by unlabeled data. Advances in Neural Information Processing Systems , 19 . Huang, K., Yin, H., Huang, H., & Gao, W. (2024). Towards green AI in fine-tuning large lan- guage models via adaptive backpropagation. In International Conference on Learning Representations . Huang, L., Huang, J., Rong, Y., Yang, Q., & Wei, Y. (2022). Frustratingly easy transfer- ability estimation. In International Conference on Machine Learning , pp. 9201â€“9225. PMLR. 58 Huang, S.-L., Makur, A., Wornell, G. W., Zheng, L., et al. (2024). Universal features for high-dimensional learning and inference. Foundations and Trends Â® in Communica- tions and Information Theory , 21 (1-2), 1â€“299. Huang, Y., Du, C., Xue, Z., Chen, X., Zhao, H., & Huang, L. (2021). What makes multi- modal learning better than single (provably). Advances in Neural Information Pro- cessing Systems , 34 , 10944â€“10956. Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler, D. M., Maxwell, T., Cheng, N., Jermyn, A. S., Askell, A., Radhakrishnan, A., Anil, C., Duvenaud, D., Ganguli, D., Barez, F., Clark, J., Ndousse, K., Sachan, K., Sellitto, M., Sharma, M., DasSarma, N., Grosse, R., Kravec, S., Bai, Y., Witten, Z., Favaro, M., Brauner, J., Karnofsky, H., Christiano, P. F., Bowman, S. R., Graham, L., Kaplan, J., Mindermann, S., Greenblatt, R., Shlegeris, B., Schiefer, N., & Perez, E. (2024). Sleeper agents: Training deceptive LLMs that persist through safety training. CoRR , abs/2401.05566 . HÂ¨ullermeier, E., & Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning , 110 (3), 457â€“ 506. Hunt, X. J., Kabul, I. K., & Silva, J. (2017). Transfer learning for education data. In Proceedings of the ACM SIGKDD Conference , Vol. 17. Hwang, U., Lee, J., Shin, J., & Yoon, S. (2024). SF(DA) 2 : Source-free domain adaptation through the lens of data augmentation. In The Twelfth International Conference on Learning Representations . Ibrahim, S., Ponomareva, N., & Mazumder, R. (2022). Newer is not always better: Rethink- ing transferability metrics, their peculiarities, stability and performance. In Joint Eu- ropean Conference on Machine Learning and Knowledge Discovery in Databases , pp. 693â€“709. Springer. Iwasawa, Y., & Matsuo, Y. (2021). Test-time classifier adjustment module for model- agnostic domain generalization. Advances in Neural Information Processing Systems , 34 , 2427â€“2440. Jacot, A., Hongler, C., & Gabriel, F. (2018). Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31 , pp. 8580â€“8589. Jang, U., Lee, J. D., & Ryu, E. K. (2024). LoRA training in the NTK regime has no spurious local minima. In International Conference on Machine Learning . Jayaraman, P. P., Forkan, A. R. M., Morshed, A., Haghighi, P. D., & Kang, Y.-B. (2020). Healthcare 4.0: A review of frontiers in digital health. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery , 10 (2), e1350. Jeddi, A., Shafiee, M. J., & Wong, A. (2020). A simple fine-tuning is all you need: Towards robust deep learning via adversarial fine-tuning. CoRR , abs/2012.13628 . Jeong, J., & Shin, J. (2020). Consistency regularization for certified robustness of smoothed classifiers. In Advances in Neural Information Processing Systems 33 . 59 Ji, Y., Zhang, X., Ji, S., Luo, X., & Wang, T. (2018). Model-reuse attacks on deep learning systems. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security , pp. 349â€“363. ACM. Ji, Z., & Telgarsky, M. (2019). Gradient descent aligns the layers of deep linear networks. In 7th International Conference on Learning Representations . Jiang, E., Zhang, Y. J., & Koyejo, S. (2024). Principled federated domain adaptation: Gradient projection and auto-weighting. In The Twelfth International Conference on Learning Representations . Jiang, S., Kadhe, S., Zhou, Y., Cai, L., & Baracaldo, N. (2023). Forcing generative models to degenerate ones: The power of data poisoning attacks. In NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly . Jin, X., Park, Y., Maddix, D. C., Wang, H., & Wang, Y. (2022). Domain adaptation for time series forecasting via attention sharing. In International Conference on Machine Learning , Vol. 162, pp. 10280â€“10297. PMLR. Jo, H.-W., Koukos, A. M., Sitokonstantinou, V., Lee, W.-K., & Kontoes, C. (2022). Towards global crop maps with transfer learning. In NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning . Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of ai ethics guidelines. Nature Machine Intelligence , 1 (9), 389â€“399. Johansson, F. D., Sontag, D. A., & Ranganath, R. (2019). Support and invertibility in domain-invariant representations. In The 22nd International Conference on Artificial Intelligence and Statistics , pp. 527â€“536. PMLR. Ju, H., Li, D., & Zhang, H. R. (2022). Robust fine-tuning of deep neural networks with hessian-based generalization guarantees. In International Conference on Machine Learning , pp. 10431â€“10461. PMLR. Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated learning. Foundations and Trends Â® in Machine Learning , 14 (1â€“2), 1â€“210. Kanagawa, H., Kobayashi, H., Shimizu, N., Tagami, Y., & Suzuki, T. (2019). Cross-domain recommendation via deep domain adaptation. In European Conference on Information Retrieval , pp. 20â€“29. Springer. Kang, J., He, J., Maciejewski, R., & Tong, H. (2020). InFoRM: individual fairness on graph mining. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 379â€“389. ACM. Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., & Yang, L. (2021). Physics-informed machine learning. Nature Reviews Physics , 3 (6), 422â€“440. Kashiparekh, K., Narwariya, J., Malhotra, P., Vig, L., & Shroff, G. (2019). ConvTimeNet: A pre-trained deep convolutional neural network for time series classification. In International Joint Conference on Neural Networks , pp. 1â€“8. IEEE. 60 Kashyap, A. R., Hazarika, D., Kan, M., & Zimmermann, R. (2021). Domain divergences: A survey and empirical analysis. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 1830â€“1849. Kasneci, E., SeÃŸler, K., KÂ¨uchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G. L., GÂ¨unnemann, S., HÂ¨ullermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., Stadler, M., Weller, J., Kuhn, J., & Kasneci, G. (2023). Chatgpt for good? on opportunities and challenges of large language models for education. Learning and Individual Differences , 103 . Kaur, D., Uslu, S., Rittichier, K. J., & Durresi, A. (2023). Trustworthy artificial intelligence: A review. ACM Computing Surveys , 55 (2), 39:1â€“39:38. Khashabi, D., Lyu, X., Min, S., Qin, L., Richardson, K., Welleck, S., Hajishirzi, H., Khot, T., Sabharwal, A., Singh, S., & Choi, Y. (2022). Prompt waywardness: The curious case of discretized interpretation of continuous prompts. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies , pp. 3631â€“3643. Kim, T., Kim, J., Tae, Y., Park, C., Choi, J., & Choo, J. (2022). Reversible instance normalization for accurate time-series forecasting against distribution shift. In The Tenth International Conference on Learning Representations . Kimura, M., & Bondell, H. (2024). Test-time augmentation meets variational bayes. CoRR , abs/2409.12587 . Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations . Koh, P. W., & Liang, P. (2017). Understanding black-box predictions via influence functions. In International Conference on Machine Learning , pp. 1885â€“1894. PMLR. Koopman, B. O. (1931). Hamiltonian systems and transformation in hilbert space. Pro- ceedings of the National Academy of Sciences , 17 (5), 315â€“318. Kornblith, S., Shlens, J., & Le, Q. V. (2019). Do better ImageNet models transfer bet- ter?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 2661â€“2671. Krishnagopal, S., & Ruiz, L. (2023). Graph neural tangent kernel: Convergence on large graphs. In International Conference on Machine Learning , Vol. 202, pp. 17827â€“17841. PMLR. Kulinski, S., & Inouye, D. I. (2022). Towards explaining image-based distribution shifts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion , pp. 4788â€“4792. Kulinski, S., & Inouye, D. I. (2023). Towards explaining distribution shifts. In International Conference on Machine Learning , pp. 17931â€“17952. PMLR. Kumar, A., Ma, T., & Liang, P. (2020). Understanding self-training for gradual domain adaptation. In International Conference on Machine Learning , pp. 5468â€“5479. PMLR. 61 Kundu, J. N., Kulkarni, A. R., Bhambri, S., Mehta, D., Kulkarni, S. A., Jampani, V., & Radhakrishnan, V. B. (2022). Balancing discriminability and transferability for source-free domain adaptation. In International Conference on Machine Learning , pp. 11710â€“11728. PMLR. Kundu, J. N., Venkat, N., V., R. M., & Babu, R. V. (2020). Universal source-free domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4543â€“4552. Kunwar, S. (2024). Managing household waste through transfer learning. CoRR , abs/2402.09437 . Kurita, K., Michel, P., & Neubig, G. (2020). Weight poisoning attacks on pretrained mod- els. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 2793â€“2806. Kuzborskij, I., & Orabona, F. (2013). Stability and hypothesis transfer learning. In Inter- national Conference on Machine Learning , pp. 942â€“950. Kuzborskij, I., & Orabona, F. (2017). Fast rates by transferring from auxiliary hypotheses. Machine Learning , 106 , 171â€“195. Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems 30 , pp. 6402â€“6413. Le, T., & Jegelka, S. (2023). Limits, approximation and size transferability for GNNs on sparse graphs via graphops. In Advances in Neural Information Processing Systems 36 . LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature , 521 (7553), 436â€“444. Lee, J. Y., Dernoncourt, F., & Szolovits, P. (2018). Transfer learning for named-entity recog- nition with neural networks. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation . Lee, J., Jung, D., Yim, J., & Yoon, S. (2022). Confidence score for source-free unsupervised domain adaptation. In International Conference on Machine Learning , pp. 12365â€“ 12377. PMLR. Lee, T., Wollstein, G., Madu, C. T., Wronka, A., Zheng, L., Zambrano, R., Schuman, J. S., & Hu, J. (2023a). Reducing ophthalmic health disparities through transfer learning: A novel application to overcome data inequality. Translational Vision Science & Technology , 12 (12), 2â€“2. Lee, Y., Chen, A. S., Tajwar, F., Kumar, A., Yao, H., Liang, P., & Finn, C. (2023b). Surgical fine-tuning improves adaptation to distribution shifts. In The Eleventh International Conference on Learning Representations . Lei, J., Gâ€™Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association , 113 (523), 1094â€“1111. Lei, T., Bai, J., Brahma, S., Ainslie, J., Lee, K., Zhou, Y., Du, N., Zhao, V. Y., Wu, Y., Li, B., Zhang, Y., & Chang, M. (2023). Conditional adapters: Parameter-efficient transfer 62 learning with fast inference. In Advances in Neural Information Processing Systems 36 . Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Nat- ural Language Processing , pp. 3045â€“3059. Levie, R., Huang, W., Bucci, L., Bronstein, M., & Kutyniok, G. (2021). Transferability of spectral graph convolutional neural networks. Journal of Machine Learning Research , 22 (272), 1â€“59. Levie, R., Isufi, E., & Kutyniok, G. (2019). On the transferability of spectral graph filters. In 2019 13th International conference on Sampling Theory and Applications (SampTA) , pp. 1â€“5. IEEE. Li, C., Farkhoor, H., Liu, R., & Yosinski, J. (2018). Measuring the intrinsic dimension of objective landscapes. In 6th International Conference on Learning Representations . Li, D., & Zhang, H. R. (2021). Improved regularization and robustness for fine-tuning in neural networks. In Advances in Neural Information Processing Systems 34 , pp. 27249â€“27262. Li, L., Song, D., Li, X., Zeng, J., Ma, R., & Qiu, X. (2021). Backdoor attacks on pre- trained models by layerwise weight poisoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 3023â€“3032. Li, P., & Tuzhilin, A. (2020). DDTCDR: deep dual transfer cross domain recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining , pp. 331â€“339. Li, T., Hu, S., Beirami, A., & Smith, V. (2021). Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning , pp. 6357â€“ 6368. PMLR. Li, X. L., & Liang, P. (2021). Prefix-Tuning: optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing , pp. 4582â€“4597. Li, X., & Bilmes, J. (2007). A bayesian divergence prior for classifier adaptation. In Artificial Intelligence and Statistics , pp. 275â€“282. PMLR. Li, Y., Jia, X., Sang, R., Zhu, Y., Green, B., Wang, L., & Gong, B. (2021). Ranking neural checkpoints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 2663â€“2673. Li, Y., Li, T., Chen, K., Zhang, J., Liu, S., Wang, W., Zhang, T., & Liu, Y. (2024). BadEdit: backdooring large language models by model editing. In The Twelfth International Conference on Learning Representations . Li, Z., & Hoiem, D. (2017). Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence , 40 (12), 2935â€“2947. 63 Liang, J., Hu, D., & Feng, J. (2020a). Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning , pp. 6028â€“6039. PMLR. Liang, P. P., Liu, T., Ziyin, L., Allen, N. B., Auerbach, R. P., Brent, D., Salakhutdinov, R., & Morency, L.-P. (2020b). Think locally, act globally: Federated learning with local and global representations. CoRR , abs/2001.01523 . Lin, H., Huang, B., Ye, H., Chen, Q., Wang, Z., Li, S., Ma, J., Wan, X., Zou, J., & Liang, Y. (2024). Selecting large language model to fine-tune via rectified scaling law. In International Conference on Machine Learning . Lipton, Z., Wang, Y.-X., & Smola, A. (2018). Detecting and correcting for label shift with black box predictors. In International Conference on Machine Learning , pp. 3122â€“ 3130. PMLR. Liu, M., Fang, Z., Zhang, Z., Gu, M., Zhou, S., Wang, X., & Bu, J. (2024). Rethinking propagation for unsupervised graph domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38, pp. 13963â€“13971. Liu, Q., & Xue, H. (2021). Adversarial spectral kernel matching for unsupervised time series domain adaptation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence , pp. 2744â€“2750. Liu, S., Niles-Weed, J., Razavian, N., & Fernandez-Granda, C. (2020). Early-learning reg- ularization prevents memorization of noisy labels. Advances in Neural Information Processing Systems , 33 , 20331â€“20342. Liu, S., Li, T., Feng, Y., Tran, N., Zhao, H., Qiu, Q., & Li, P. (2023). Structural re-weighting improves graph domain adaptation. In International Conference on Machine Learning , pp. 21778â€“21793. PMLR. Liu, S., Zou, D., Zhao, H., & Li, P. (2024). Pairwise alignment improves graph domain adaptation. In International Conference on Machine Learning . Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., & Tang, J. (2022). P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pp. 61â€“68. Liu, Y., Kang, Y., Xing, C., Chen, T., & Yang, Q. (2020). A secure federated transfer learning framework. IEEE Intelligent Systems , 35 (4), 70â€“82. Liu, Y., Li, C., Wang, J., & Long, M. (2023). Koopa: Learning non-stationary time series dynamics with koopman predictors. In Advances in Neural Information Processing Systems 36 . Liu, Y., Wu, H., Wang, J., & Long, M. (2022). Non-stationary transformers: Exploring the stationarity in time series forecasting. In Advances in Neural Information Processing Systems 35 . Liu, Y., Kothari, P., van Delft, B., Bellot-Gurlet, B., Mordan, T., & Alahi, A. (2021). TTT++: when does self-supervised test-time training fail or thrive?. In Advances in Neural Information Processing Systems 34 , pp. 21808â€“21820. 64 Liu, Z., Cheng, M., Li, Z., Huang, Z., Liu, Q., Xie, Y., & Chen, E. (2023a). Adaptive normalization for non-stationary time series forecasting: A temporal slice perspective. In Advances in Neural Information Processing Systems 36 . Liu, Z., Xu, Y., Ji, X., & Chan, A. B. (2023b). TWINS: A fine-tuning framework for improved transferability of adversarial robustness and generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 16436â€“ 16446. IEEE. Long, M., Cao, Y., Wang, J., & Jordan, M. (2015). Learning transferable features with deep adaptation networks. In International Conference on Machine Learning , pp. 97â€“105. PMLR. Long, M., Cao, Z., Wang, J., & Jordan, M. I. (2018). Conditional adversarial domain adaptation. Advances in Neural Information Processing Systems , 31 . Lotfi, S., Finzi, M. A., Kuang, Y., Rudner, T. G. J., Goldblum, M., & Wilson, A. G. (2024). Non-vacuous generalization bounds for large language models. In International Conference on Machine Learning . Lotfollahi, M., Naghipourfar, M., Luecken, M. D., Khajavi, M., BÂ¨uttner, M., Wagenstetter, M., Avsec, Ë‡ Z., Gayoso, A., Yosef, N., Interlandi, M., et al. (2022). Mapping single-cell data to reference atlases by transfer learning. Nature Biotechnology , 40 (1), 121â€“130. LovÂ´asz, L. (2012). Large Networks and Graph Limits , Vol. 60. American Mathematical Soc. Lu, J., & Sun, S. (2024). CaudiTS: Causal disentangled domain adaptation of multivariate time series. In International Conference on Machine Learning . Ma, Y., Chen, S., Ermon, S., & Lobell, D. (2024). Transfer learning in environmental remote sensing. Remote Sensing of Environment , 301 , 113924. Maddox, W., Tang, S., Moreno, P., Wilson, A. G., & Damianou, A. (2021). Fast adaptation with linearized neural networks. In International Conference on Artificial Intelligence and Statistics , pp. 2737â€“2745. PMLR. Madras, D., Creager, E., Pitassi, T., & Zemel, R. S. (2018). Learning adversarially fair and transferable representations. In International Conference on Machine Learning , pp. 3381â€“3390. PMLR. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2018). Towards deep learn- ing models resistant to adversarial attacks. In International Conference on Learning Representations . Mahabadi, R. K., Henderson, J., & Ruder, S. (2021). Compacter: Efficient low-rank hyper- complex adapter layers. In Advances in Neural Information Processing Systems 34 , pp. 1022â€“1035. Mahamadou, A. J. D., Goetz, L., & Altman, R. (2024). Individual fairness through reweight- ing and tuning. CoRR , abs/2405.01711 . Malhotra, P., TV, V., Vig, L., Agarwal, P., & Shroff, G. (2017). TimeNet: pre-trained deep recurrent neural network for time series classification. In 25th European Symposium on Artificial Neural Networks . 65 Malladi, S., Wettig, A., Yu, D., Chen, D., & Arora, S. (2023). A kernel-based view of language model fine-tuning. In International Conference on Machine Learning , pp. 23610â€“23641. PMLR. Man, T., Shen, H., Jin, X., & Cheng, X. (2017). Cross-domain recommendation: An embed- ding and mapping approach. In Proceedings of the 26th International Joint Conference on Artificial Intelligence , pp. 2464â€“2470. Mandal, D., Deng, S., Jana, S., Wing, J., & Hsu, D. J. (2020). Ensuring fairness beyond the training data. Advances in Neural Information Processing Systems , 33 , 18445â€“18456. Mansour, Y., Mohri, M., Ro, J., & Suresh, A. T. (2020). Three approaches for personaliza- tion with applications to federated learning. CoRR , abs/2002.10619 . Mansour, Y., Mohri, M., Ro, J., Suresh, A. T., & Wu, K. (2021). A theory of multiple-source adaptation with limited target labeled data. In International Conference on Artificial Intelligence and Statistics , pp. 2332â€“2340. PMLR. Mansour, Y., Mohri, M., & Rostamizadeh, A. (2009a). Domain adaptation: Learning bounds and algorithms. In The 22nd Conference on Learning Theory . Mansour, Y., Mohri, M., & Rostamizadeh, A. (2009b). Multiple source adaptation and the rÂ´enyi divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence , pp. 367â€“374. Maskey, S., Levie, R., & Kutyniok, G. (2023). Transferability of graph neural networks: An extended graphon approach. Applied and Computational Harmonic Analysis , 63 , 48â€“83. Matsoukas, C., Haslum, J. F., Sorkhei, M., SÂ¨oderberg, M., & Smith, K. (2022). What makes transfer learning work for medical images: Feature reuse & other factors. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 9225â€“9234. McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017). Communication-efficient learning of deep networks from decentralized data. In Pro- ceedings of the 20th International Conference on Artificial Intelligence and Statistics , pp. 1273â€“1282. PMLR. Mehra, A., Kailkhura, B., Chen, P., & Hamm, J. (2021). Understanding the limits of unsu- pervised domain adaptation via data poisoning. In Advances in Neural Information Processing Systems 34 , pp. 17347â€“17359. Memmel, E., Menzen, C., Schuurmans, J., Wesel, F., & kim batselier (2024). Position: Tensor networks are a valuable asset for green AI. In International Conference on Machine Learning . Michelmore, R., Wicker, M., Laurenti, L., Cardelli, L., Gal, Y., & Kwiatkowska, M. (2020). Uncertainty quantification with statistical guarantees in end-to-end autonomous driv- ing control. In 2020 IEEE International Conference on Robotics and Automation , pp. 7344â€“7350. IEEE. Mieth, B., Hockley, J., GÂ¨ornitz, N., HÂ¨ohne, M., MÂ¨uller, K.-R., Gutteridge, A., & Ziemek, D. (2019). Using transfer learning from prior reference knowledge to improve the clustering of single-cell rna-seq data. Scientific Reports , 9 , 20353. 66 Minami, S., Fukumizu, K., Hayashi, Y., & Yoshida, R. (2023). Transfer learning with affine model transformation. In Thirty-seventh Conference on Neural Information Processing Systems . Misra, I., Shrivastava, A., Gupta, A., & Hebert, M. (2016). Cross-stitch networks for multi- task learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3994â€“4003. Mitsuzumi, Y., Kimura, A., & Kashima, H. (2024). Understanding and improving source-free domain adaptation from a theoretical perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 28515â€“ 28524. Mohri, M., & MuËœnoz Medina, A. (2012). New analysis and algorithm for learning with drift- ing distributions. In 23rd International Conference on Algorithmic Learning Theory , pp. 124â€“138. Springer. Mohri, M., Sivek, G., & Suresh, A. T. (2019). Agnostic federated learning. In International Conference on Machine Learning , pp. 4615â€“4625. MÂ¨okander, J., Schuett, J., Kirk, H. R., & Floridi, L. (2023). Auditing large language models: a three-layered approach. CoRR , abs/2302.08500 . Mooij, J. M., Magliacane, S., & Claassen, T. (2020). Joint causal inference from multiple contexts. Journal of Machine Learning Research , 21 (99), 1â€“108. Muandet, K., Balduzzi, D., & SchÂ¨olkopf, B. (2013). Domain generalization via invariant feature representation. In International Conference on Machine Learning , pp. 10â€“18. Mukherjee, D., Petersen, F., Yurochkin, M., & Sun, Y. (2022). Domain adaptation meets individual fairness. and they get along. In Advances in Neural Information Processing Systems 35 . MÂ¨uller, A. (1997). Integral probability metrics and their generating classes of functions. Advances in Applied Probability , 29 (2), 429â€“443. Naeini, M. P., Cooper, G., & Hauskrecht, M. (2015). Obtaining well calibrated probabil- ities using bayesian binning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 29. Nern, L. F., Raj, H., Georgi, M. A., & Sharma, Y. (2023). On transfer of adversarial robustness from pretraining to downstream tasks. In Advances in Neural Information Processing Systems 36 . Neyshabur, B., Sedghi, H., & Zhang, C. (2020). What is being transferred in transfer learning?. In Advances in Neural Information Processing Systems 33 . Nguyen, A. T., Torr, P., & Lim, S. N. (2022a). FedSR: a simple and effective domain gener- alization method for federated learning. Advances in Neural Information Processing Systems , 35 , 38831â€“38843. Nguyen, A. T., Tran, T., Gal, Y., Torr, P., & Baydin, A. G. (2022b). KL guided domain adaptation. In International Conference on Learning Representations . 67 Nguyen, C. N., Tran, P., Ho, L. S. T., Dinh, V. C., Tran, A. T., Hassner, T., & Nguyen, C. V. (2023). Simple transferability estimation for regression tasks. In Uncertainty in Artificial Intelligence , pp. 1510â€“1521. PMLR. Nguyen, C. V., Hassner, T., Seeger, M. W., & Archambeau, C. (2020). LEEP: A new mea- sure to evaluate transferability of learned representations. In International Conference on Machine Learning , pp. 7294â€“7305. PMLR. Nguyen, X., Wainwright, M. J., & Jordan, M. I. (2010). Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Infor- mation Theory , 56 (11), 5847â€“5861. Nikolinakos, N. T. (2023). Ethical principles for trustworthy AI. In EU Policy and Legal Framework for Artificial Intelligence, Robotics and Related Technologies-The AI Act , pp. 101â€“166. Springer. Niloy, F. F., Ahmed, S. M., Raychaudhuri, D. S., Oymak, S., & Roy-Chowdhury, A. K. (2024). Effective restoration of source knowledge in continual test time adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pp. 2091â€“2100. Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., & Tan, M. (2022). Efficient test- time model adaptation without forgetting. In International Conference on Machine Learning , pp. 16888â€“16905. PMLR. Niu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao, P., & Tan, M. (2023a). Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations . Niu, Z., Anitescu, M., & Chen, J. (2023b). Graph neural network-inspired kernels for gaus- sian processes in semi-supervised learning. In The Eleventh International Conference on Learning Representations . Oneto, L., Donini, M., Luise, G., Ciliberto, C., Maurer, A., & Pontil, M. (2020a). Exploit- ing MMD and sinkhorn divergences for fair and transferable representation learning. Advances in Neural Information Processing Systems , 33 , 15360â€“15370. Oneto, L., Donini, M., Pontil, M., & Maurer, A. (2020b). Learning fair and transferable rep- resentations with theoretical guarantees. In 2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA) , pp. 30â€“39. IEEE. OpenAI (2023). GPT-4 technical report. CoRR , abs/2303.08774 . OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., & Zhang, L. (2019). Solving rubikâ€™s cube with a robot hand. CoRR , abs/1910.07113 . Ott, F., RÂ¨ugamer, D., Heublein, L., Bischl, B., & Mutschler, C. (2022). Domain adaptation for time-series classification to mitigate covariate shift. In Proceedings of the 30th ACM International Conference on Multimedia , pp. 5934â€“5943. ACM. Â¨Ozyurt, Y., Feuerriegel, S., & Zhang, C. (2023). Contrastive learning for unsupervised do- main adaptation of time series. In The Eleventh International Conference on Learning Representations . 68 Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering , 22 (10), 1345â€“1359. PÂ´andy, M., Agostinelli, A., Uijlings, J. R. R., Ferrari, V., & Mensink, T. (2022). Transferabil- ity estimation using bhattacharyya class separability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 9162â€“9172. IEEE. Park, S., Bastani, O., Weimer, J., & Lee, I. (2020). Calibrated prediction with covariate shift via unsupervised domain adaptation. In The 23rd International Conference on Artificial Intelligence and Statistics , pp. 3219â€“3229. PMLR. Park, S., Dobriban, E., Lee, I., & Bastani, O. (2022). PAC prediction sets under covariate shift. In International Conference on Learning Representations . Passalis, N., Tefas, A., Kanniainen, J., Gabbouj, M., & Iosifidis, A. (2020). Deep adap- tive input normalization for time series forecasting. IEEE Transactions on Neural Networks and Learning Systems , 31 (9), 3760â€“3765. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., KÂ¨opf, A., Yang, E. Z., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: an imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 , pp. 8024â€“8035. Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., & Dean, J. (2021). Carbon emissions and large neural network training. CoRR , abs/2104.10350 . Pei, J., Fernandez, R. C., & Yu, X. (2023). Data and AI model markets: Opportunities for data and model sharing, discovery, and integration. Proceedings of the VLDB Endowment , 16 (12), 3872â€“3873. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., & Wang, B. (2019). Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1406â€“1415. Peng, X., Huang, Z., Zhu, Y., & Saenko, K. (2020). Federated adversarial domain adapta- tion. In International Conference on Learning Representations . Peng, X. B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2018). Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation , pp. 3803â€“3810. IEEE. Pessach, D., & Shmueli, E. (2023). Algorithmic fairness. In Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook , pp. 867â€“886. Springer. Petruzzelli, A., Musto, C., Laraspata, L., Rinaldi, I., de Gemmis, M., Lops, P., & Semer- aro, G. (2024). Instructing and prompting large language models for explainable cross-domain recommendations. In Proceedings of the 18th ACM Conference on Rec- ommender Systems , pp. 298â€“308. Pfeiffer, J., Kamath, A., RÂ¨ucklÂ´e, A., Cho, K., & Gurevych, I. (2021). AdapterFusion: non-destructive task composition for transfer learning. In Proceedings of the 16th 69 Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 487â€“503. Pham, T.-H., Zhang, X., & Zhang, P. (2023). Fairness and accuracy under domain gener- alization. In International Conference on Learning Representations . Pinto, L., Davidson, J., Sukthankar, R., & Gupta, A. (2017). Robust adversarial reinforce- ment learning. In International Conference on Machine Learning , pp. 2817â€“2826. PMLR. Ploner, M., & Akbik, A. (2024). Parameter-efficient fine-tuning: Is there an optimal subset of parameters to tune?. In Findings of the Association for Computational Linguistics: EACL 2024 . Podkopaev, A., & Ramdas, A. (2021). Distribution-free uncertainty quantification for clas- sification under label shift. In Uncertainty in Artificial Intelligence , pp. 844â€“853. PMLR. Prasad, A., Hase, P., Zhou, X., & Bansal, M. (2023). GrIPS: gradient-free, edit-based instruction search for prompting large language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pp. 3827â€“3846. Principe, J. C. (2010). Information Theoretic Learning: Renyiâ€™s Entropy and Kernel Per- spectives . Springer Science & Business Media. Prinster, D., Liu, A., & Saria, S. (2022). JAWS: auditing predictive uncertainty under covariate shift. Advances in Neural Information Processing Systems , 35 , 35907â€“35920. Prinster, D., Saria, S., & Liu, A. (2023). Jaws-x: Addressing efficiency bottlenecks of con- formal prediction under standard and feedback covariate shift. In International Con- ference on Machine Learning , pp. 28167â€“28190. PMLR. Purushotham, S., Carvalho, W., Nilanon, T., & Liu, Y. (2017). Variational recurrent ad- versarial deep domain adaptation. In International Conference on Learning Repre- sentations . Qi, F., Chen, Y., Li, M., Yao, Y., Liu, Z., & Sun, M. (2021). ONION: A simple and effective defense against textual backdoor attacks. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 9558â€“9566. Qi, W., Ruan, Y., Zuo, Y., & Li, T. (2022). Parameter-efficient tuning on layer normalization for pre-trained language models. CoRR , abs/2211.08682 . Qin, G., & Eisner, J. (2021). Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT) , pp. 5203â€“5212. Qiu, X., Parcollet, T., Fernandez-Marques, J., Gusmao, P. P., Gao, Y., Beutel, D. J., Topal, T., Mathur, A., & Lane, N. D. (2023). A first look into the carbon footprint of federated learning. Journal of Machine Learning Research , 24 (129), 1â€“23. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning transferable 70 visual models from natural language supervision. In International Conference on Machine Learning , pp. 8748â€“8763. PMLR. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research , 21 , 140:1â€“140:67. Ragab, M., Eldele, E., Tan, W. L., Foo, C.-S., Chen, Z., Wu, M., Kwoh, C.-K., & Li, X. (2023). ADATIME: A benchmarking suite for domain adaptation on time series data. ACM Transactions on Knowledge Discovery from Data , 17 (8), 1â€“18. Raghu, M., Zhang, C., Kleinberg, J., & Bengio, S. (2019). Transfusion: Understanding transfer learning for medical imaging. Advances in Neural Information Processing Systems , 32 . Raina, R., Battle, A. J., Lee, H., Packer, B., & Ng, A. Y. (2007). Self-taught learning: Trans- fer learning from unlabeled data. In International Conference on Machine Learning , pp. 759â€“766. ACM. Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics , 378 , 686â€“707. Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pp. 33â€“44. Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, P., Canny, J., Abbeel, P., & Song, Y. (2019). Evaluating protein transfer learning with TAPE. Advances in Neural Information Processing Systems , 32 . Razdaibiedina, A., Mao, Y., Khabsa, M., Lewis, M., Hou, R., Ba, J., & Almahairi, A. (2023). Residual prompt tuning: Improving prompt tuning with residual reparameterization. In Findings of the Association for Computational Linguistics: ACL 2023 , pp. 6740â€“ 6757. Rebuffi, S., Bilen, H., & Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems 30 , pp. 506â€“516. Redko, I., Habrard, A., Morvant, E., Sebban, M., & Bennani, Y. (2019). Advances in Domain Adaption Theory . Redko, I., Habrard, A., & Sebban, M. (2017). Theoretical analysis of domain adaptation with optimal transport. In Machine Learning and Knowledge Discovery in Databases: European Conference , pp. 737â€“753. Springer. RÂ´enyi, A. (1961). On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contribu- tions to the Theory of Statistics , Vol. 4, pp. 547â€“562. University of California Press. Rezaei, A., Liu, A., Memarrast, O., & Ziebart, B. D. (2021). Robust fairness under covariate shift. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35, pp. 9419â€“9427. 71 Rezaei, S., & Liu, X. (2020). A target-agnostic attack on deep models: Exploiting security vulnerabilities of transfer learning. In International Conference on Learning Repre- sentations . Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). â€why should I trust you?â€: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 1135â€“1144. ACM. Roh, Y., Lee, K., Whang, S. E., & Suh, C. (2023). Improving fair training under correlation shifts. In International Conference on Machine Learning , pp. 29179â€“29209. PMLR. Romano, Y., Sesia, M., & Candes, E. (2020). Classification with valid and adaptive coverage. Advances in Neural Information Processing Systems , 33 , 3581â€“3591. Rosenstein, M. T., Marx, Z., Kaelbling, L. P., & Dietterich, T. G. (2005). To transfer or not to transfer. In NIPS 2005 Workshop on Transfer Learning , Vol. 898, p. 4. RÂ¨ucklÂ´e, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., & Gurevych, I. (2021). AdapterDrop: on the efficiency of adapters in transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 7930â€“7946. Ruiz, L., Chamon, L. F. O., & Ribeiro, A. (2020). Graphon neural networks and the trans- ferability of graph neural networks. In Advances in Neural Information Processing Systems 33 . Ruiz, L., Gama, F., & Ribeiro, A. (2021). Graph neural networks: Architectures, stability, and transferability. Proceedings of the IEEE , 109 (5), 660â€“682. Ruoss, A., Balunovic, M., Fischer, M., & Vechev, M. (2020). Learning certified individually fair representations. Advances in Neural Information Processing Systems , 33 , 7584â€“ 7596. Rusu, A. A., VeË‡cerÂ´Ä±k, M., RothÂ¨orl, T., Heess, N., Pascanu, R., & Hadsell, R. (2017). Sim-to- real robot learning from pixels with progressive nets. In Conference on Robot Learning , pp. 262â€“270. PMLR. Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2018). Maximum classifier discrep- ancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3723â€“3732. Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., & Madry, A. (2020). Do adversarially ro- bust imagenet models transfer better?. In Advances in Neural Information Processing Systems 33 . Sama, N., David, E., Rossetti, S., Antona, A., Franchetti, B., & Pirri, F. (2023). A new large dataset and a transfer learning methodology for plant phenotyping in vertical farms. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 540â€“551. Samra, A., Frolov, E., Vasilev, A., Grigorevskiy, A., & Vakhrushev, A. (2024). Cross- domain latent factors sharing via implicit matrix factorization. In Proceedings of the 18th ACM Conference on Recommender Systems , pp. 309â€“317. 72 Sattler, F., MÂ¨uller, K.-R., & Samek, W. (2020). Clustered federated learning: Model- agnostic distributed multitask optimization under privacy constraints. IEEE Trans- actions on Neural Networks and Learning Systems , 32 (8), 3710â€“3722. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2009). The graph neural network model. IEEE Transactions on Neural Networks , 20 (1), 61â€“80. Schmucker, R., & Mitchell, T. M. (2022). Transferable student performance modeling for intelligent tutoring systems. In Proceedings of the 30th International Conference on Computers in Education , pp. 13â€“23. Schrouff, J., Harris, N., Koyejo, S., Alabdulmohsin, I. M., Schnider, E., Opsahl-Ong, K., Brown, A., Roy, S., Mincu, D., Chen, C., Dieng, A., Liu, Y., Natarajan, V., Karthike- salingam, A., Heller, K. A., Chiappa, S., & Dâ€™Amour, A. (2022). Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. In Advances in Neural Information Processing Systems 35 . Schumann, C., Wang, X., Beutel, A., Chen, J., Qian, H., & Chi, E. H. (2019). Transfer of machine learning fairness across domains. CoRR , abs/1906.09688 . Schwartz, R., Dodge, J., Smith, N. A., & Etzioni, O. (2020). Green AI. Communications of the ACM , 63 (12), 54â€“63. Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-CAM: visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision , pp. 618â€“ 626. Shachaf, G., Brutzkus, A., & Globerson, A. (2021). A theoretical analysis of fine-tuning with linear teachers. In Advances in Neural Information Processing Systems 34 , pp. 15382â€“15394. Shafahi, A., Saadatpanah, P., Zhu, C., Ghiasi, A., Studer, C., Jacobs, D. W., & Goldstein, T. (2020). Adversarially robust transfer learning. In 8th International Conference on Learning Representations . Shao, W., Zhao, X., Ge, Y., Zhang, Z., Yang, L., Wang, X., Shan, Y., & Luo, P. (2022). Not all models are equal: Predicting model transferability in a self-challenging fisher space. In European Conference on Computer Vision , pp. 286â€“302. Springer. Shapley, L. S. (1953). A value for n-person games. Contribution to the Theory of Games , 2 . Shen, J., Qu, Y., Zhang, W., & Yu, Y. (2018). Wasserstein distance guided representation learning for domain adaptation. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence , pp. 4058â€“4065. AAAI Press. Shen, J., Li, L., Dery, L. M., Staten, C., Khodak, M., Neubig, G., & Talwalkar, A. (2023). Cross-modal fine-tuning: Align then refine. In International Conference on Machine Learning , pp. 31030â€“31056. PMLR. Shen, L., Ji, S., Zhang, X., Li, J., Chen, J., Shi, J., Fang, C., Yin, J., & Wang, T. (2021). Backdoor pre-trained models can transfer to all. In Proceedings of the 2021 ACM 73 SIGSAC Conference on Computer and Communications Security , pp. 3141â€“3158. ACM. Shen, M., Bu, Y., & Wornell, G. W. (2023). On balancing bias and variance in unsuper- vised multi-source-free domain adaptation. In International Conference on Machine Learning , pp. 30976â€“30991. PMLR. Shervashidze, N., Schweitzer, P., van Leeuwen, E. J., Mehlhorn, K., & Borgwardt, K. M. (2011). Weisfeiler-lehman graph kernels. Journal of Machine Learning Research , 12 , 2539â€“2561. Shi, W., Han, X., Gonen, H., Holtzman, A., Tsvetkov, Y., & Zettlemoyer, L. (2023). Toward human readable prompt tuning: Kubrickâ€™s the shining is a good movie, and a good prompt too?. In Findings of the Association for Computational Linguistics: EMNLP 2023 , pp. 10994â€“11005. Shimodaira, H. (2000). Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference , 90 (2), 227â€“244. Shin, T., Razeghi, Y., IV, R. L. L., Wallace, E., & Singh, S. (2020). AutoPrompt: eliciting knowledge from language models with automatically generated prompts. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing , pp. 4222â€“4235. Shu, M., Wang, J., Zhu, C., Geiping, J., Xiao, C., & Goldstein, T. (2023). On the exploitabil- ity of instruction tuning. In Advances in Neural Information Processing Systems 36 . Si, W., Park, S., Lee, I., Dobriban, E., & Bastani, O. (2024). PAC prediction sets under label shift. In International Conference on Learning Representations . Singh, H., Singh, R., Mhasawade, V., & Chunara, R. (2021). Fairness violations and mitiga- tion under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , pp. 3â€“13. Slack, D., Friedler, S. A., & Givental, E. (2020). Fairness warnings and Fair-MAML: Learn- ing fairly with minimal data. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pp. 200â€“209. Smith, V., Chiang, C., Sanjabi, M., & Talwalkar, A. (2017). Federated multi-task learning. In Advances in Neural Information Processing Systems 30 , pp. 4424â€“4434. Snoek, J., Ovadia, Y., Fertig, E., Lakshminarayanan, B., Nowozin, S., Sculley, D., Dillon, J. V., Ren, J., & Nado, Z. (2019). Can you trust your modelâ€™s uncertainty? evalu- ating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems 32 , pp. 13969â€“13980. Socher, R., Ganjoo, M., Manning, C. D., & Ng, A. (2013). Zero-shot learning through cross-modal transfer. Advances in Neural Information Processing Systems , 26 . Srebro, N., Sridharan, K., & Tewari, A. (2010). Optimistic rates for learning with a smooth loss. arXiv Preprint , arXiv:1009.3896 . Sriperumbudur, B. K., Gretton, A., Fukumizu, K., SchÂ¨olkopf, B., & Lanckriet, G. R. (2010). Hilbert space embeddings and metrics on probability measures. The Journal of Ma- chine Learning Research , 11 , 1517â€“1561. 74 Stein, A., Wu, Y., Wong, E., & Naik, M. (2023). Rectifying group irregularities in explana- tions for distribution shift. In XAI in Action: Past, Present, and Future Applications . Su, Y., Wang, X., Qin, Y., Chan, C., Lin, Y., Wang, H., Wen, K., Liu, Z., Li, P., Li, J., Hou, L., Sun, M., & Zhou, J. (2022). On transferability of prompt tuning for natural language processing. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 3949â€“3969. Subramanian, S., Harrington, P., Keutzer, K., Bhimji, W., Morozov, D., Mahoney, M. W., & Gholami, A. (2023). Towards foundation models for scientific machine learning: Char- acterizing scaling and transfer behavior. Advances in Neural Information Processing Systems , 36 . Sun, B., & Saenko, K. (2016). Deep CORAL: correlation alignment for deep domain adap- tation. In Computer Vision - ECCV 2016 Workshops - Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III , Vol. 9915, pp. 443â€“450. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., & Hardt, M. (2020). Test-time training with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning , pp. 9229â€“9248. PMLR. Sung, Y., Nair, V., & Raffel, C. (2021). Training neural networks with fixed sparse masks. In Advances in Neural Information Processing Systems 34 , pp. 24193â€“24205. Swamy, V., Marras, M., & KÂ¨aser, T. (2022). Meta transfer learning for early success pre- diction in moocs. In Proceedings of the Ninth ACM Conference on Learning @ Scale , pp. 121â€“132. ACM. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., & Fergus, R. (2014). Intriguing properties of neural networks. In 2nd International Conference on Learning Representations . Tanwani, A. K. (2020). DIRL: domain-invariant representation learning for sim-to-real transfer. In 4th Conference on Robot Learning , pp. 1558â€“1571. PMLR. Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., & Metzler, D. (2022). Scale efficiently: Insights from pretraining and finetuning transformers. In International Conference on Learning Representations . Terzi, M., Achille, A., Maggipinto, M., & Susto, G. A. (2021). Adversarial training reduces information and improves transferability. In Proceedings of the AAAI Conference on Artificial Intelligence , pp. 2674â€“2682. Thenmozhi, K., & Reddy, U. S. (2019). Crop pest classification based on deep convolutional neural network and transfer learning. Computers and Electronics in Agriculture , 164 , 104906. Theodoris, C. V., Xiao, L., Chopra, A., Chaffin, M. D., Sayed, Z. R. A., Hill, M. C., Manti- neo, H., Brydon, E. M., Zeng, Z., Liu, X. S., & Ellinor, P. T. (2023). Transfer learning enables predictions in network biology. Nature , 618 , 616â€“624. Tibshirani, R. J., Foygel Barber, R., Candes, E., & Ramdas, A. (2019). Conformal prediction under covariate shift. Advances in Neural Information Processing Systems , 32 . 75 Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., & Abbeel, P. (2017). Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems , pp. 23â€“30. IEEE. Toseef, M., Li, X., & Wong, K.-C. (2022). Reducing healthcare disparities using multi- ple multiethnic data distributions with fine-tuning of transfer learning. Briefings in Bioinformatics , 23 (3), bbac078. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lam- ple, G. (2023). Llama: Open and efficient foundation language models. CoRR , abs/2302.13971 . Tran, A. T., Nguyen, C. V., & Hassner, T. (2019). Transferability and hardness of supervised classification tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1395â€“1405. IEEE. Tripuraneni, N., Jin, C., & Jordan, M. (2021). Provable meta-learning of linear representa- tions. In International Conference on Machine Learning , pp. 10434â€“10443. PMLR. Tripuraneni, N., Jordan, M. I., & Jin, C. (2020). On the theory of transfer learning: The importance of task diversity. In Advances in Neural Information Processing Systems 33 . Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., & Madry, A. (2019). Robustness may be at odds with accuracy. In 7th International Conference on Learning Representa- tions . Tzeng, E., Devin, C., Hoffman, J., Finn, C., Abbeel, P., Levine, S., Saenko, K., & Darrell, T. (2020). Adapting deep visuomotor representations with weak pairwise constraints. In Algorithmic Foundations of Robotics XII: Proceedings of the Twelfth Workshop on the Algorithmic Foundations of Robotics , pp. 688â€“703. Springer. Tzeng, E., Hoffman, J., Saenko, K., & Darrell, T. (2017). Adversarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 7167â€“7176. Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., & Darrell, T. (2014). Deep domain confusion: Maximizing for domain invariance. CoRR , abs/1412.3474 . Ulyanov, D., Vedaldi, A., & Lempitsky, V. S. (2016). Instance normalization: The missing ingredient for fast stylization. CoRR , abs/1607.08022 . Utrera, F., Kravitz, E., Erichson, N. B., Khanna, R., & Mahoney, M. W. (2021). Adversarially-trained deep nets transfer better: Illustration on image classification. In 9th International Conference on Learning Representations . Vaishnavi, P., Eykholt, K., & Rahmati, A. (2022). Transferring adversarial robustness through robust representation matching. In 31st USENIX Security Symposium , pp. 2083â€“2098. USENIX Association. Vaishnavi, P., Eykholt, K., & Rahmati, A. (2024). A study of the effects of transfer learning on adversarial robustness. Transactions on Machine Learning Research , 2024 . 76 Valipour, M., Rezagholizadeh, M., Kobyzev, I., & Ghodsi, A. (2023). DyLoRA: parameter- efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pp. 3266â€“3279. Vanzo, A., Chowdhury, S. P., & Sachan, M. (2024). GPT-4 as a homework tutor can improve student engagement and learning outcomes. CoRR , abs/2409.15981 . Varshney, K. R. (2022). Trustworthy Machine Learning . Independently Published, Chap- paqua, NY, USA. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems 30 , pp. 5998â€“6008. Vu, T., Lester, B., Constant, N., Al-Rfouâ€™, R., & Cer, D. (2022). SPoT: better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5039â€“5059. Wan, A., Wallace, E., Shen, S., & Klein, D. (2023). Poisoning language models during instruction tuning. In International Conference on Machine Learning , pp. 35413â€“ 35425. PMLR. Wan, L., Zhou, W., He, Y., Wanger, T. C., & Cen, H. (2022). Combining transfer learning and hyperspectral reflectance analysis to assess leaf nitrogen concentration across different plant species datasets. Remote Sensing of Environment , 269 , 112826. Wang, B., Yao, Y., Viswanath, B., Zheng, H., & Zhao, B. Y. (2018). With great training comes great vulnerability: Practical attacks against transfer learning. In 27th USENIX Security Symposium (USENIX Security 18) , pp. 1281â€“1297. Wang, D., Shelhamer, E., Liu, S., Olshausen, B. A., & Darrell, T. (2021). Tent: Fully test-time adaptation by entropy minimization. In 9th International Conference on Learning Representations . Wang, H., Mao, Y., Yan, Y., Yang, Y., Sun, J., Choi, K., Veeramani, B., Hu, A., Bowen, E., Cody, T., & Zhou, D. (2024). EvoluNet: advancing dynamic non-IID transfer learning on graphs. In International Conference on Machine Learning . Wang, J.-K., & Wibisono, A. (2023). Towards understanding GD with hard and conjugate pseudo-labels for test-time adaptation. In The Eleventh International Conference on Learning Representations . Wang, Q., Fink, O., Van Gool, L., & Dai, D. (2022). Continual test-time domain adapta- tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 7201â€“7211. Wang, R., Dong, Y., Arik, S. Â¨ O., & Yu, R. (2023a). Koopman neural operator forecaster for time-series with temporal distributional shifts. In The Eleventh International Conference on Learning Representations . Wang, S., Guan, K., Zhang, C., Jiang, C., Zhou, Q., Li, K., Qin, Z., Ainsworth, E., He, J., Wu, J., Schaefer, D., Gentry, L., Margenot, A., & Herzberger, L. (2023b). Air- 77 borne hyperspectral imaging of cover crops through radiative transfer process-guided machine learning. Remote Sensing of Environment , 285 , 113386. Wang, X., Long, M., Wang, J., & Jordan, M. I. (2020). Transferable calibration with lower bias and variance in domain adaptation. In Advances in Neural Information Processing Systems 33 . Wang, Y., Chauhan, J., Wang, W., & Hsieh, C. (2023a). Universality and limitations of prompt tuning. In Advances in Neural Information Processing Systems 36 . Wang, Y., Chen, Y., Jamieson, K., & Du, S. S. (2023b). Improved active multi-task rep- resentation learning via lasso. In International Conference on Machine Learning , pp. 35548â€“35578. PMLR. Wang, Y., & Arora, R. (2024). Adversarially robust hypothesis transfer learning. In Inter- national Conference on Machine Learning . Wang, Z., Panda, R., Karlinsky, L., Feris, R., Sun, H., & Kim, Y. (2023). Multitask prompt tuning enables parameter-efficient transfer learning. In The Eleventh International Conference on Learning Representations . Wang, Z., Dai, Z., PÂ´oczos, B., & Carbonell, J. G. (2019). Characterizing and avoiding negative transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11293â€“11302. Watkins, A., Ullah, E., Nguyen-Tang, T., & Arora, R. (2023). Optimistic rates for multi- task representation learning. In Advances in Neural Information Processing Systems 36 . Wei, C., Shen, K., Chen, Y., & Ma, T. (2021a). Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations . Wei, C., Xie, S. M., & Ma, T. (2021b). Why do pretrained language models help in down- stream tasks? an analysis of head and prompt tuning. In Advances in Neural Infor- mation Processing Systems 34 , pp. 16158â€“16170. Weisfeiler, B., & Lehman, A. (1968). A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia , 2 (9), 12â€“16. Wen, Y., Jain, N., Kirchenbauer, J., Goldblum, M., Geiping, J., & Goldstein, T. (2023). Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. In Advances in Neural Information Processing Systems 36 . Wicker, M., Piratla, V., & Weller, A. (2023). Certification of distributional individual fairness. In Advances in Neural Information Processing Systems 36 . Wieringa, M. (2020). What to account for when accounting for algorithms: A systematic literature review on algorithmic accountability. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pp. 1â€“18. Wiese, G., Weissenborn, D., & Neves, M. L. (2017). Neural domain adaptation for biomed- ical question answering. In Proceedings of the 21st Conference on Computational Natural Language Learning , pp. 281â€“289. 78 Wiles, O., Gowal, S., Stimberg, F., Rebuffi, S.-A., Ktena, I., Dvijotham, K. D., & Cemgil, A. T. (2022). A fine-grained analysis on distribution shift. In International Conference on Learning Representations . Wilson, G., Doppa, J. R., & Cook, D. J. (2020). Multi-source deep domain adaptation with weak supervision for time-series sensor data. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 1768â€“1778. ACM. Wilson, G., Doppa, J. R., & Cook, D. J. (2023). CALDA: improving multi-source time series domain adaptation with contrastive adversarial learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 45 (12), 14208â€“14221. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38â€“45. Wu, J., Ainsworth, L., Leakey, A., Wang, H., & He, J. (2023a). Graph-structured gaus- sian processes for transferable graph learning. In Advances in Neural Information Processing Systems 36 . Wu, J., Bao, W., Ainsworth, E. A., & He, J. (2023b). Personalized federated learning with parameter propagation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 2594â€“2605. ACM. Wu, J., & He, J. (2021). Indirect invisible poisoning attacks on domain adaptation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , pp. 1852â€“1862. ACM. Wu, J., & He, J. (2022). Dynamic transfer learning with progressive meta-task scheduler. Frontiers Big Data , 5 . Wu, J., & He, J. (2023a). Trustworthy transfer learning: Transferability and trustworthiness. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 5829â€“5830. ACM. Wu, J., & He, J. (2023b). A unified framework for adversarial attacks on multi-source domain adaptation. IEEE Transactions on Knowledge and Data Engineering , 35 (11), 11039â€“11050. Wu, J., He, J., & Ainsworth, E. A. (2023). Non-iid transfer learning on graphs. In Proceed- ings of the AAAI Conference on Artificial Intelligence , Vol. 37, pp. 10342â€“10350. Wu, J., He, J., & Tong, H. (2024). Distributional network of networks for modeling data heterogeneity. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 3379â€“3390. Wu, J., He, J., Wang, S., Guan, K., & Ainsworth, E. A. (2022). Distribution-informed neural networks for domain adaptation regression. In Advances in Neural Information Processing Systems 35 . 79 Wu, M., Pan, S., Zhou, C., Chang, X., & Zhu, X. (2020). Unsupervised domain adaptive graph convolutional networks. In Proceedings of the Web Conference 2020 , pp. 1457â€“ 1467. Wu, Y., Winston, E., Kaushik, D., & Lipton, Z. C. (2019). Domain adaptation with asymmetrically-relaxed distribution alignment. In International Conference on Ma- chine Learning , pp. 6872â€“6881. PMLR. Wu, Z., Wu, Y., & Mou, L. (2024). Zero-shot continuous prompt transfer: Generalizing task semantics across language models. In The Twelfth International Conference on Learning Representations . Xi, Z., Du, T., Li, C., Pang, R., Ji, S., Chen, J., Ma, F., & Wang, T. (2023). Defending pre- trained language models as few-shot learners against backdoor attacks. In Advances in Neural Information Processing Systems 36 . Xu, J., Ma, M., Wang, F., Xiao, C., & Chen, M. (2024). Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. In Proceedings of the",
    "Conference of the North American Chapter of the Association for Computational": "Linguistics: Human Language Technologies , pp. 30â€“34. Zou, Y., Deng, W., & Zheng, L. (2023). Adaptive calibrator ensemble: Navigating test set difficulty in out-of-distribution scenarios. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision , pp. 19333â€“19342. 84"
  },
  "tables": [
    {
      "page": 2,
      "table_index": 0,
      "content": [
        [
          "Transfer Learning Model"
        ],
        [
          ""
        ]
      ]
    },
    {
      "page": 4,
      "table_index": 0,
      "content": [
        [
          "Distribution Discrepancy\n(Data-level)\nğ‘ƒğ‘‡\nğ‘ƒğ‘†",
          "Task Diversity\n(Task-level)\n+ â†’",
          "Transferability Estim\n(Model-level)"
        ]
      ]
    },
    {
      "page": 4,
      "table_index": 1,
      "content": [
        [
          "Graph Transferability",
          "Text Transferability",
          "Time-Series Transfera"
        ]
      ]
    },
    {
      "page": 4,
      "table_index": 2,
      "content": [
        [
          "Hypothesis Transfer\nTransfer",
          "Federated Transfer\nServer"
        ]
      ]
    },
    {
      "page": 4,
      "table_index": 3,
      "content": [
        [
          "Adversarial Attack\nTransfer",
          "Adversarial Defense\nTransfer"
        ]
      ]
    },
    {
      "page": 4,
      "table_index": 4,
      "content": [
        [
          "Group Fairness\nTransfer",
          "Individual Fairness\nâ‰ˆ â‰ˆ\nTransfer\nğ‘“ â‰ˆğ‘“ ğ‘“ â‰ˆ"
        ]
      ]
    },
    {
      "page": 14,
      "table_index": 0,
      "content": [
        [
          "ecruoS",
          "",
          "",
          "",
          ""
        ],
        [
          "tegraT",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "(a) ğ‘ƒ ğ‘‹ changes",
          "(b) ğ‘ƒ ğº changes",
          "(c) ğ‘ƒ ğ‘‹,ğº changes",
          "(d) ğ‘ƒ ğ‘Œ changes"
        ]
      ]
    },
    {
      "page": 18,
      "table_index": 0,
      "content": [
        [
          "ğ‘§Æ¸â†ğ‘§+ğ‘Šğ‘¢ğ‘ğœğ‘Šğ‘‘ğ‘œğ‘¤ğ‘›ğ‘§ for input ğ‘§âˆˆâ„ğ‘‘",
          null,
          null
        ],
        [
          "ğ‘Šğ‘‘ğ‘œğ‘¤ğ‘›âˆˆâ„ğ‘ŸÃ—ğ‘‘, ğ‘Šğ‘¢ğ‘âˆˆâ„ğ‘‘Ã—ğ‘Ÿ",
          "",
          "where ğ‘Ÿâ‰ªğ‘‘"
        ]
      ]
    },
    {
      "page": 18,
      "table_index": 1,
      "content": [
        [
          "",
          "ğ‘Š ğ‘„ ğ‘ ğ‘„ ğ‘„\nA\nnett\nğ‘Š ğ‘ ğ¾\nğ¾ ğ¾ oit\nn\nğ‘Š ğ‘ ğ‘‰\nğ‘‰ ğ‘‰"
        ]
      ]
    },
    {
      "page": 18,
      "table_index": 2,
      "content": [
        [
          "",
          "ğ‘Š ğ‘„ ğ‘ ğ‘„ ğ‘„\nA\nnett\nğ‘Š ğ‘ ğ¾\nğ¾ ğ¾ oit\nn\nğ‘Š ğ‘ ğ‘‰\nğ‘‰ ğ‘‰"
        ]
      ]
    },
    {
      "page": 18,
      "table_index": 3,
      "content": [
        [
          ""
        ],
        [
          "(c) Additive parameter updates (added low-rank Î”W)"
        ]
      ]
    },
    {
      "page": 34,
      "table_index": 0,
      "content": [
        [
          "High Salary Low Salary\nSource ğ‘Œ=0 ğ‘Œ=1\nMale ğ´=0\nFemale ğ´=1\nTransfer\nTarget",
          "Low Salary\nğ‘Œ=1"
        ],
        [
          "(c) Fairness transfer with only ğ‘‹\nğ‘‡",
          null
        ]
      ]
    },
    {
      "page": 34,
      "table_index": 1,
      "content": [
        [
          "(a) Fairness transfer with ğ‘‹ ,ğ´ ,ğ‘Œ\nğ‘‡ ğ‘‡ ğ‘‡",
          "(b) Fairness transfer with ğ‘‹ ,ğ´\nğ‘‡ ğ‘‡"
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 0,
      "content": [
        [
          "ayer 3Layer 4 Layer N FC Output\nâ‹¯",
          null,
          null
        ],
        [
          "",
          null,
          ""
        ],
        [
          null,
          "",
          null
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 1,
      "content": [
        [
          "",
          null,
          ""
        ],
        [
          null,
          "",
          null
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 2,
      "content": [
        [
          "",
          null,
          ""
        ],
        [
          null,
          "",
          null
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 3,
      "content": [
        [
          "Layer 1Layer 2Layer 3",
          null,
          null,
          null,
          null,
          null,
          null,
          "Layer 4 Layer N\nFine-tuned\nLayers\nâ‹¯",
          null,
          null,
          null
        ],
        [
          null,
          "",
          null,
          "",
          null,
          "",
          null,
          null,
          "",
          "",
          null
        ],
        [
          null,
          null,
          "",
          null,
          "",
          null,
          "",
          "",
          null,
          null,
          ""
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 4,
      "content": [
        [
          "FC Output",
          null
        ],
        [
          null,
          ""
        ],
        [
          "",
          null
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 5,
      "content": [
        [
          "Layer 1Layer 2Layer 3Layer 4 Layer N\nâ‹¯",
          null,
          null
        ],
        [
          null,
          "",
          null
        ],
        [
          null,
          null,
          ""
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 6,
      "content": [
        [
          "FC Output\nFi",
          null
        ],
        [
          null,
          ""
        ],
        [
          "",
          null
        ]
      ]
    },
    {
      "page": 37,
      "table_index": 7,
      "content": [
        [
          "",
          null,
          "",
          null,
          "",
          null,
          ""
        ],
        [
          null,
          "",
          null,
          "",
          null,
          "",
          null
        ]
      ]
    }
  ],
  "images": [
    "processed/images/2412.14116v1_page2_img0.png",
    "processed/images/2412.14116v1_page2_img1.png",
    "processed/images/2412.14116v1_page2_img2.png",
    "processed/images/2412.14116v1_page2_img3.png",
    "processed/images/2412.14116v1_page2_img4.png",
    "processed/images/2412.14116v1_page2_img5.png",
    "processed/images/2412.14116v1_page2_img6.png",
    "processed/images/2412.14116v1_page2_img7.png",
    "processed/images/2412.14116v1_page2_img8.png",
    "processed/images/2412.14116v1_page2_img9.png",
    "processed/images/2412.14116v1_page2_img10.png",
    "processed/images/2412.14116v1_page2_img11.png",
    "processed/images/2412.14116v1_page2_img12.png",
    "processed/images/2412.14116v1_page2_img13.png",
    "processed/images/2412.14116v1_page2_img14.png",
    "processed/images/2412.14116v1_page2_img15.png",
    "processed/images/2412.14116v1_page2_img16.png",
    "processed/images/2412.14116v1_page2_img17.png",
    "processed/images/2412.14116v1_page2_img18.png",
    "processed/images/2412.14116v1_page2_img19.png",
    "processed/images/2412.14116v1_page2_img20.png",
    "processed/images/2412.14116v1_page2_img21.png",
    "processed/images/2412.14116v1_page2_img22.png",
    "processed/images/2412.14116v1_page2_img23.png",
    "processed/images/2412.14116v1_page2_img24.png",
    "processed/images/2412.14116v1_page2_img25.png",
    "processed/images/2412.14116v1_page2_img26.png",
    "processed/images/2412.14116v1_page2_img27.png",
    "processed/images/2412.14116v1_page2_img28.png",
    "processed/images/2412.14116v1_page4_img0.png",
    "processed/images/2412.14116v1_page4_img1.png",
    "processed/images/2412.14116v1_page4_img2.png",
    "processed/images/2412.14116v1_page4_img3.png",
    "processed/images/2412.14116v1_page4_img4.png",
    "processed/images/2412.14116v1_page4_img5.png",
    "processed/images/2412.14116v1_page4_img6.png",
    "processed/images/2412.14116v1_page4_img7.png",
    "processed/images/2412.14116v1_page4_img8.png",
    "processed/images/2412.14116v1_page4_img9.png",
    "processed/images/2412.14116v1_page18_img0.png",
    "processed/images/2412.14116v1_page18_img1.png",
    "processed/images/2412.14116v1_page21_img0.png",
    "processed/images/2412.14116v1_page29_img0.png",
    "processed/images/2412.14116v1_page37_img0.png",
    "processed/images/2412.14116v1_page37_img1.png",
    "processed/images/2412.14116v1_page37_img2.png",
    "processed/images/2412.14116v1_page37_img3.png",
    "processed/images/2412.14116v1_page37_img4.png",
    "processed/images/2412.14116v1_page37_img5.png",
    "processed/images/2412.14116v1_page37_img6.png",
    "processed/images/2412.14116v1_page37_img7.png"
  ],
  "status": "completed"
}