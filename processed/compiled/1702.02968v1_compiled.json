{
  "metadata": {
    "title": "Comparative benchmarking of cloud computing vendors with High   Performance Linpack",
    "authors": [
      "Mohammad Mohammadi",
      "Timur Bazhirov"
    ],
    "abstract": "We present a comparative analysis of the maximum performance achieved by the Linpack benchmark on compute intensive hardware publicly available from multiple cloud providers. We study both performance within a single compute node, and speedup for distributed memory calculations with up to 32 nodes or at least 512 computing cores. We distinguish between hyper-threaded and non-hyper-threaded scenarios and estimate the performance per single computing core. We also compare results with a traditional supercomputing system for reference. Our findings provide a way to rank the cloud providers and demonstrate the viability of the cloud for high performance computing applications.",
    "published": "",
    "arxiv_id": "1702.02968v1",
    "categories": [
      "cs.PF",
      "cs.DC"
    ],
    "pdf_url": "http://arxiv.org/pdf/1702.02968v1",
    "pdf_file": "data/pdfs/1702.02968v1.pdf",
    "pdf_filename": "1702.02968v1.pdf"
  },
  "processing_info": {
    "processed_at": "2025-11-11T10:30:02.551912",
    "is_large_pdf": false,
    "sections_found": 15,
    "tables_found": 2,
    "images_found": 0
  },
  "sections_text": {
    "Abstract": "arXiv:1702.02968v1 [cs.PF] Feb Comparative benchmarking of cloud computing vendors with High Performance Linpack",
    "Mohammad Mohammadi, Timur Bazhirov": "Exabyte Inc., San Francisco, California 94103, USA Abstract —We present a comparative analysis of the maximum performance achieved by the Linpack benchmark on compute intensive hardware publicly available from multiple cloud providers. We study both performance within a single compute node, and speedup for distributed memory calculations with up to nodes or at least computing cores. We distinguish between hyper-threaded and non-hyper-threaded scenarios and estimate the performance per single computing core. We also compare results with a traditional supercomputing system for reference. Our ﬁndings provide a way to rank the cloud providers and demonstrate the viability of the cloud for high performance computing applications. Index Terms —Cloud Computing, High Performance Computing, Linpack, Benchmarking.",
    "I Ntroduction": "During the last decade cloud computing established itself as a viable alternative to on-premises hardware for missioncritical applications in multiple areas , , . For high performance computing (HPC) workloads that traditionally required large and cost-intensive hardware procurement, however, the feasibility and advantages of cloud computing are still debated. In particular, it is often questioned whether software applications that require distributed memory can be efﬁciently run on ”commodity” compute infrastructure publicly available from cloud computing vendors . Several studies reported on the poor applicability of cloud-based environments for scientiﬁc computing. Multiple research groups ran both standard benchmark suites such as Linpack and NAS , , , and network performance tests . The cost of solving a system of linear equations was found to increase exponentially with the problem size, illustrating that cloud was not mature enough for such workloads in . A study of the impact of virtualization on network performance reported signiﬁcant throughput instability and abnormal delay variations . An empirical",
    "Experiments": "publicly available cloud computing systems are capable of delivering comparable, if not better, performance than the top-tier traditional high performance computing systems. This fact conﬁrms that cloud computing is already a viable and cost-effective alternative to traditional costintensive supercomputing procurement. We believe that with further advancements in virtualization, such as lowoverhead container technology, and future improvements in cloud datacenter hardware we may experience a large-scale migration from on-premises to cloud-based usage for high performance applications, similar to what happened with less compute-intensive workloads.",
    "M Ethodology": "Benchmarking presented in this article is done through High Performance Linpack (HPL). The program solves a random system of linear equations, represented by a dense matrix, in double precision ( bits) arithmetic on distributed-memory computers. It does so through a two-dimensional blockcyclic data distribution, and right-looking variant of the LU factorization with row partial pivoting. It is a portable and freely available software package. HPL provides testing and timing means to quantify the accuracy of the obtained solution as well as the time-to-completion. The best performance achievable depends on a variety of factors, and the algorithm is scalable such that its parallel efﬁciency is kept constant with respect to per processor memory usage. information: , , , . process grid dimensions. These parameters are changed choosing the exact input parameters could be demonstrated Q slightly larger than P.",
    "R Esults": "We present the cloud server instance types and hardware speciﬁcation for all studied cases inside Table 1. We choose Table 1: Hardware speciﬁcation for the compute nodes used during benchmarking. Core count for physical computing cores and processor frequency, in GHz, are given together with Memory (RAM) size, in gigabytes, and network bandwidth in gigabit-per-second , , , . Provider Nodes Cores Freq. RAM Net AWS-* c4.8xlarage 2. Azure-AZ Standard F16s 2. Azure-IB-A 2. Azure-IB-H H16 3. SoftLayer Virtual 2. Rackspace Compute12. NERSC Edison 2. Table 2: [AWS] Results for Amazon Web Services c4.8xlarge instances with hyperthreading enabled (default scenario). Core count is given for virtual (hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). It can be seen that the ratio of absolute speedup to the number of nodes falls rapidly as the number of nodes is increased. Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 1. 1. 0. 3. 1. 1. 6. 2. 2. 13. 5. 5. 26. 9. 8. 52. 16. the highest performing servers available in an on-demand fashion. Most of the compute servers have physical cores and all have at least 2GB per or random access memory per core. The network options differ quite a bit, from to gigabit per second in bandwidth. We also provide metrics for the traditional supercomputing system used as reference . 3.",
    "Amazon Web Services": "For Amazon Web Services (AWS) we study different scenarios: the default hyper-threaded, non-hyper-threaded and non-hyper-threaded mode with placement group option enabled. The c4.8xlarge instance types are used. 3.1. Hyper-threaded regime Table shows the results for AWS instances with hyperthreading enabled (default regime). It can be seen that the ratio of absolute speedup to the number of nodes rapidly decreases as the node count increases. 3.1. Non-hyper-threaded regime Table shows the results for AWS with Hyper-Threading disabled. Thus only out of cores were used to run the benchmark, and each core was able to boost into the turbo-frequency . It can be seen that the ratio of absolute speedup to the number of nodes still rapidly degrades with increased node count. Table [AWS-NHT] Results for Amazon Web Services c4.8xlarge instances with hyper-threading disabled. Core count is given for physical (non-hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 6. 5. 5. 13. 8. 10. 26. 16.",
    "Microsoft Azure": "3.2. F-series Table shows the HPL benchmark results running on Azure Standard F16 instances. Although the overall performance degradation with increased node count is evident, it appears to be less severe than for AWS. The bare performance is worse however. 3.2. A-series Table shows the HPL benchmark results running on",
    "Azure Standard": "A9 instances using Inﬁniband interconnect network. The low-latency network interconnect deﬁnitely affects the scaling, increasing the speed-up ratio from 0. to 0. for compute nodes. The bare performance ﬁgures, however are still better for AWS due to the higher processor speed. Table 5: [AZ-F] Results for Azure F-series instances. Core count is given for physical (non-hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 0. 1. 0. 1. 1. 1. 2. 3. 3. 4. 6. 5. 9. 11. 10. 19. 22. Table 6: [AZ-A] Results for Azure A-series instances with Inﬁniband interconnect network. Core count is given for physical (non-hyper-threaded) computing cores. Numbers of computing nodes (Nodes) and total computing cores (Cores) are given together with the maximum achieved (Rmax) and peak (Rpeak) performance indicators, and the absolute achieved speedup (Speedup). Nodes Cores Rmax (TFLOPS) Rpeak (TFLOPS) Speedup 0. 0. 1. 0. 1. 1. 1. 2. 3. 2. 5. 7. 4. 10. 14. 8. 20. 28. 3.2. H-series Table shows the HPL benchmark results running on Azure Standard H16r instances using Inﬁniband interconnect network. The low-latency network interconnect enables the best scaling pattern, with sustained ratio above 0. in the 1node count (1computing cores) range. The bare performance ﬁgures are best of all cases studied, even when compared with the traditional supercomputing system of reference. 3. Rackspace Table shows the HPL benchmark results running on",
    "D Iscussion": "In Fig. we present a comparison of the speedup ratios for the scenarios described the previous part. As it can be seen, Microsoft Azure outperforms other cloud providers because of the low latency interconnect network that facilitates efﬁcient scaling. SoftLayer has the least favorable speedup ratio at scale, likely because of the interconnect network again. AWS and Rackspace show a signiﬁcant degree of parallel performance degradation, such that at nodes the measured performance is about one-half of the peak value.",
    "Figure 1:": "Speedup ratios (the ratios of maximum speedup Rmax to peak speedup Rpeak) against the number of nodes for all benchmarked cases. Speedup ratio for 1,2,4,8, and nodes are investigated and given by points. Lines are drawn to guide the eye. The legend is as follows: AWS Amazon Web Services in the default hyper-threaded regime; AWS-NHT same, with hyperthreading disabled; AWS-NHTPG same, with placement group option enabled; AZ Microsoft Azure standard F16 instances; AZ-IB-A same provider, A9 instances; AZ-IB-H same provider, H16 instances; RS Rackspace compute1instances; SL IBM/Softlayer virtual servers; NERSC Edison computing facility of the National Energy Research Scientiﬁc Computing Center. Fig. shows a comparative plot of the performance per core in giga-FLOPS for the previously described scenarios. Microsoft Azure H-instances are the highest performing option in this view as well (AZ-IB-H). One interesting fact is that although Microsoft Azure A-instances (AZ-IB-A) show better overall scaling in Fig. 1, AWS c4.8xlarge instances deliver better performance per core for up to nodes. This is likely because of faster processors speed. NERSC Edison supercomputer delivers a rather low performance per core metric, likely due to the type of processors used.",
    "Figure 2:": "Performance per core in giga-FLOPS against the number of nodes for all benchmarked cases. Performance per core is obtained by dividing the maximum performance by the total number of computing cores. The legend is the same as in Fig. 1. Lines are given to guide the eye.",
    "C Onclusion": "We benchmarked the performance of the best available computing hardware from public cloud providers with high performance Linpack. We optimized the benchmark for each computing environment and evaluated the relative performance for distributed memory calculations. We found Microsoft Azure to deliver the best results, and demonstrated that the performance per single computing core on public cloud to be comparable to modern traditional supercomputing systems. Based on our ﬁndings we suggest that the concept of high performance computing in the cloud is ready for a widespread adoption and can provide a viable and cost-efﬁcient alternative to capital-intensive onpremises hardware deployments.",
    "A Cknowledgement": "Authors would like to thank Michael G. Haverty for reading the manuscript, and acknowledge support from the National Energy Research Scientiﬁc Computing Center in a form of a startup allocation.",
    "R Eferences": "Dongarra, Luszczek, and Petitet, “The LINPACK benchmark: past, present and future,” Concurrency and Computation: Practice and Experience , vol. 15, no. 9, pp. 803–820, 2003. [Online]. Available: http://dx.doi.org/10.1002/cpe. P. Mell and T. Grance, “The nist deﬁnition of cloud computing,” National Institute of Standards and Technology , vol. 53, no. 6, p. 50, 2009. Armbrust, Fox, Grifﬁth, Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, “A view of cloud computing,” Journal Emerging Trends Computing and Information Sciences , vol. 53, no. 4, pp. 50–58, 2010. [Online]. Available: J. W. Rittinghouse and J. F. Ransome, Cloud computing: implementation, management, and security . CRC press, 2016. K. Jackson, L. Ramakrishnan, K. Muriki, S. Canon, S. Cholia, Shalf, Wasserman, and Wright, “Performance analysis high performance computing applications the amazon web services cloud,” Proceedings the IEEE Second International Conference on Cloud Computing Technology and Science (CloudCom 2010) , pp. 159–168, 2010. [Online]. Available: R. Masud, “High performance computing with clouds,” Technical Report, University of Oregon , 2010. S. Ostermann, A. Iosup, N. Yigitbasi, R. Prodan, T. Fahringer, and D. Epema, “An early performance analysis of cloud computing services for scientiﬁc computing,” Delft University of Technology, Tech. Rep , 2008. E. Walker, “Benchmarking amazon ec2 for high-performance scientiﬁc computing,” USENIX Login , vol. 33, no. 5, pp. 18–23, 2008. G. Wang and T. E. Ng, “The impact of virtualization on network performance of amazon ec2 data center,” Proceedings of IEEE INFOCOM , 2010. J. Napper and P. Bientinesi, “Can cloud computing reach the top500?” Proceedings of the combined workshops on UnConventional high performance computing workshop plus memory access workshop. ACM , vol. 2008, pp. 17–20, 2009. A. Iosup, S. Ostermann, N. Yigitbasi, R. Prodan, T. Fahringer, and D. Epema, “Performance analysis of cloud computing services for many-tasks scientiﬁc computing,” IEEE Transactions on Parallel and Distributed Systems , vol. 22, no. 6, pp. 931–945, 2011. K. Yelick, “The magellan report cloud computing for science,” U.S. Department Energy Ofﬁce Science , 2011. [Online]. Available: S. Hazelhurst, “Scientiﬁc computing using virtual highperformance computing: a case study using the amazon elastic computing cloud,” Proceedings of the annual research conference of the South African Institute of Computer Scientists and Information Technologists on IT research in developing countries: riding the wave of technology. ACM , pp. 94–103, 2008. E. Deelman, G. Singh, M. Livny, B. Berriman, and J. Good, “The cost of doing science on the cloud: the montage example,” Proceedings of the ACM/IEEE conference on Supercomputing. IEEE Press , pp. 1–12, 2008. C. Evangelinos and C. Hill, “Cloud computing for parallel scientiﬁc hpc applications: Feasibility of running coupled atmosphereocean climate models on amazons ec2,” ratio , vol. 2, no. 2.40, pp. 2–34, 2008. K. Keahey, T. Freeman, J. Lauret, and D. Olson, “Virtual workspaces for scientiﬁc applications,” Journal of Physics: Conference Series , vol. 78, 2007. K. Keahey, R. Figueiredo, J. Fortes, T. Freeman, and M. Tsugawa, “Science clouds: Early experiences in cloud computing for scientiﬁc applications,” Cloud Computing and Applications , vol. 2008, 2008. K. Keahey, “Cloud computing for science,” Proceedings of the 21st International Conference on Scientiﬁc and Statistical Database Management. Springer-Verlag , vol. 2008, p. 478, 2009. J. Li, D. Agarwal, M. Humphrey, C. van Ingen, K. Jackson, and Y. Ryu, “escience in the cloud: A modis satellite data reprojection and reduction pipeline in the windows azure platform,” Proceedings of the 24th IEEE International Parallel and Distributed Processing Symposium , 2010. L. Ramakrishnan, K. R. Jackson, S. Canon, S. Cholia, and J. Shalf, “Deﬁning future platform requirements for e-science clouds,” Proceedings of the ACM Symposium on Cloud Computing , 2010. J. Rehr, F. Vila, J. Gardner, L. Svec, and M. Prange, “Scientiﬁc computing in the cloud,” Computing in Science and Engineering , vol. 99, 2010. S. P. Ahuja and S. Mani, “The state of high performance computing in the cloud,” Journal of Emerging Trends in Computing and Information Sciences , vol. 3, no. 2, 2012. T. Bazhirov, M. Mohammadi, K. Ding, and S. Barabash, “Largescale high-throughput computer-aided discovery of advanced materials using cloud computing,” Proceedings of the American Physical Society March Meeting , 2017. [Online]. Available: “Linpack, top500.org webpage.” [Online]. Available: “Edison supercomputer, top500.org ranking.” [Online]. Available: [Online]. Available: http://www.netlib.org/benchmark/hpl J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart, “Linpack users guide,” 1979. “Amazon ec2 instance types.” [Online]. Available: “Sizes for linux virtual machines in azure.” [Online]. Available: “Rackspace virtual cloud server ﬂavors.” [Online]. Available: “Softlayer virtual servers.” [Online]. Available: “Turbo boost technology.” [Online]. Available: “Amazon elastic compute cloud: Placement group.” [Online]. Available: G. F. Pﬁster, “An introduction to the inﬁniband architecture,” High Performance Mass Storage and Parallel I/O , vol. 42, pp. 617–632, 2001."
  },
  "sections_summary": {
    "Abstract": "arXiv:1702.02968v1 [cs.PF] Feb Comparative benchmarking of cloud computing vendors with High Performance Linpack",
    "Mohammad Mohammadi, Timur Bazhirov": "This study compares the maximum performance achieved by the Linpack benchmark on compute-intensive hardware from multiple cloud providers. It analyzes single-node performance, distributed memory calculations with up to 16 nodes or cores, and estimates performance per core. The results rank the cloud providers and demonstrate their viability for high-performance computing applications.",
    "I Ntroduction": "Cloud computing's viability for high-performance computing (HPC) workloads is debated due to concerns over efficiency, scalability, and reliability. Several studies have shown that cloud-based environments struggle with large-scale computations, exhibiting poor applicability and significant performance issues, such as exponential cost increases and unstable network throughput.",
    "Experiments": "Publicly available cloud computing systems can deliver comparable or better performance than top-tier traditional high-performance computing systems. This confirms that cloud computing is a viable and cost-effective alternative to traditional supercomputing procurement, and a large-scale migration to cloud-based usage may occur for high-performance applications.",
    "M Ethodology": "Benchmarking is done using High Performance Linpack (HPL), which solves random linear equations on distributed-memory computers through LU factorization with row partial pivoting, in a two-dimensional blockcyclic data distribution. HPL provides accuracy and timing measures to evaluate performance. The best performance depends on various factors, including process grid dimensions, and is scalable with constant parallel efficiency per processor memory usage.",
    "R Esults": "Table 1 lists hardware specifications for compute nodes used during benchmarking, including:\n\n- Core count and processor frequency\n- Memory (RAM) size in gigabytes\n- Network bandwidth in gigabits per second\n\nTable 2 provides results for Amazon Web Services c4.8xlarge instances with hyperthreading enabled, showing:\n\n- Number of computing cores and nodes\n- Maximum achieved performance indicators (Rmax and Rpeak)\n- Peak performance (TFLOPS)\n- Absolute achieved speedup",
    "Amazon Web Services": "The study examines AWS instance types with and without hyper-threading enabled, comparing default and non-hyper-threaded modes. \n\nIn the hyper-threaded regime, the absolute speedup ratio decreases rapidly as the number of nodes increases.\n\nIn the non-hyper-threaded regime, the core count is given for physical cores only, leading to a similar rapid degradation of speedup with increased node count.\n\nPerformance indicators include maximum achieved (Rmax) and peak (Rpeak) performance, with results shown in tables.",
    "Microsoft Azure": "The F-series Table shows decreased overall performance with more nodes in Azure compared to AWS, while the A-series Table also displays lower performance compared to AWS.",
    "Azure Standard": "Azure F-series instances: \n- Increased speed-up ratio from 0. to 0.\n- Higher processor speeds still outperform A-series.\n\nAzure A-series instances:\n- Low-latency network interconnect enables best scaling pattern.\n- Sustained ratio above 1.5 in 1-node count range.\n\nH-series:\n- Best performance figures, even compared to traditional supercomputing systems.\n- Sustained ratio above 0.",
    "D Iscussion": "Microsoft Azure outperforms other cloud providers due to its low latency interconnect network, which facilitates efficient scaling. SoftLayer has the least favorable speedup ratio at scale, likely due to its interconnect network as well. AWS and Rackspace experience significant parallel performance degradation, with performance at nodes about half of the peak value.",
    "Figure 1:": "The text shows the results of benchmarking different cloud computing environments (AWS, Microsoft Azure, Rackspace, IBM/Softlayer, and NERSC) with various instance types. The plots show that:\n\n- Microsoft Azure H-instances perform the best in terms of overall performance per core.\n- AWS c4.8xlarge instances deliver better performance per core for up to 32 nodes due to faster processor speed.\n- NERSC Edison supercomputer has lower performance per core, likely due to its processors.\n- AZ Microsoft Azure standard and A9 instances show better overall scaling compared to other options.",
    "Figure 2:": "There is no text provided for me to summarize. You mentioned a figure or legend, but there's no actual text to work with. Please provide the relevant information, and I'll be happy to help!",
    "C Onclusion": "Benchmarking public cloud providers' best available computing hardware, optimized Linpack was used to evaluate relative performance. Microsoft Azure delivered the best results, with performance comparable to modern traditional supercomputing systems. This suggests widespread adoption of high-performance computing in the cloud as a viable and cost-efficient alternative to on-premises hardware deployments.",
    "A Cknowledgement": "The authors thank Michael G. Haverty for reviewing their manuscript and acknowledge support from the National Energy Research Scientific Computing Center through a startup allocation.",
    "R Eferences": "Here's a concise overall summary:\n\nResearch on using cloud computing for high-performance computing (HPC) has been ongoing since 2008. Key papers have explored various aspects, including benchmarking, performance analysis, and feasibility studies of cloud-based HPC applications. Studies have found that cloud computing offers advantages in scientific data management, materials science research, and high-performance computing, particularly with Linpack and the top500.org. Cloud service providers such as Amazon EC2, Azure, and Rackspace have also been evaluated for their virtual server configurations.\n\nThe research has led to the development of guidelines and benchmarks, including a Linpack users guide, which provide standards for scientific computing in the cloud. Overall, these findings suggest that cloud computing can be a viable option for HPC applications, offering benefits such as scalability, flexibility, and cost-effectiveness."
  },
  "tables": [
    {
      "page": 4,
      "table_index": 0,
      "content": [
        [
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          null
        ],
        [
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          null
        ],
        [
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          null
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          null
        ],
        [
          null,
          null,
          null,
          "",
          "",
          "",
          "",
          "",
          null,
          null
        ],
        [
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          null
        ],
        [
          "",
          "",
          null,
          "",
          "",
          "",
          "",
          "",
          "",
          null
        ]
      ]
    },
    {
      "page": 5,
      "table_index": 0,
      "content": [
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
        ]
      ]
    }
  ],
  "images": [],
  "status": "completed"
}