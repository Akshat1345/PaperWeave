{
  "metadata": {
    "title": "Uncovering the Connections Between Adversarial Transferability and   Knowledge Transferability",
    "authors": [
      "Kaizhao Liang",
      "Jacky Y. Zhang",
      "Boxin Wang",
      "Zhuolin Yang",
      "Oluwasanmi Koyejo",
      "Bo Li"
    ],
    "abstract": "Knowledge transferability, or transfer learning, has been widely adopted to allow a pre-trained model in the source domain to be effectively adapted to downstream tasks in the target domain. It is thus important to explore and understand the factors affecting knowledge transferability. In this paper, as the first work, we analyze and demonstrate the connections between knowledge transferability and another important phenomenon--adversarial transferability, \\emph{i.e.}, adversarial examples generated against one model can be transferred to attack other models. Our theoretical studies show that adversarial transferability indicates knowledge transferability and vice versa. Moreover, based on the theoretical insights, we propose two practical adversarial transferability metrics to characterize this process, serving as bidirectional indicators between adversarial and knowledge transferability. We conduct extensive experiments for different scenarios on diverse datasets, showing a positive correlation between adversarial transferability and knowledge transferability. Our findings will shed light on future research about effective knowledge transfer learning and adversarial transferability analyses.",
    "published": "",
    "arxiv_id": "2006.14512v4",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.14512v4",
    "pdf_file": "data/pdfs/2006.14512v4.pdf",
    "pdf_filename": "2006.14512v4.pdf"
  },
  "processing_info": {
    "processed_at": "2025-10-28T10:16:48.577697",
    "sections_found": 219,
    "tables_found": 4,
    "images_found": 6
  },
  "sections_text": {
    "Adversarial Transferability and Knowledge Transferability": "α 1 and α 2 (Deﬁnition 1 & 2 ) represent complementary aspects of the adversarial transferability: α 1 can be understood as how often the adversarial attack transfers, while α 2 encodes directional information of the output deviation caused by adversarial attacks. Recall that α 1 , α 2 ∈ [0 , 1] (higher values indicate better adversarial transferability). As we show in our theoretical results reveal that high α 1 alone is not enough, i.e. , both the proposed metrics are necessary to characterize adversarial transferability and the relation between adversarial and knowledge transferabilities. We provide a one-dimensional example showing that large α 1 only is not enough to indicate high knowledge transferability. Suppose the ground truth target function f T ( x ) = x 2 , and the source function f S ( x ) = sgn ( x ) · x 2 where sgn ( · ) denotes the sign function. Let the adversarial loss be the deviation in function output, and the data distribution be the uniform distribution on [ − 1 , 1] . As we can see, the direction that makes either f T or f S deviates the most is always the same, i.e. , in this example even with α 1 = 1 achieves its maximum and adversarial attacks always transfer, regardless of the choice of f 1 → f 2 or f 2 → f 1 . However, there does not exist an afﬁne function g ( i.e. , ﬁne-tuning) making g ◦ f S close to f T on [ − 1 , 1] . Indeed, one can verify that α 2 = 0 in this case (either f 1 → f 2 or f 2 → f 1 ), which contributes to the low knowledge transferability. However, if we move the data distribution to [0 , 2] , we can have α 1 = α 2 = 1 (either f 1 → f 2 or f 2 → f 1 ) indicating high adversarial transferability, and indeed it achieves f S = f T showing perfect knowledge transferability. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability B. Detailed Discussion About the Direction of Adversarial Transfer From f S → f T in",
    "Abstract": "Knowledge transferability, or transfer learning, has been widely adopted to allow a pre-trained model in the source domain to be effectively adapted to downstream tasks in the target do- main. It is thus important to explore and un- derstand the factors affecting knowledge trans- ferability. In this paper, as the ﬁrst work, we analyze and demonstrate the connections between knowledge transferability and another important phenomenon–adversarial transferability, i.e. , ad- versarial examples generated against one model can be transferred to attack other models. Our theoretical studies show that adversarial transfer- ability indicates knowledge transferability, and vice versa. Moreover, based on the theoretical insights, we propose two practical adversarial transferability metrics to characterize this process, serving as bidirectional indicators between adver- sarial and knowledge transferability. We conduct extensive experiments for different scenarios on diverse datasets, showing a positive correlation be- tween adversarial transferability and knowledge transferability. Our ﬁndings will shed light on fu- ture research about effective knowledge transfer learning and adversarial transferability analyses. All code and data are available here .",
    "1. Introduction": "Knowledge transfer is quickly becoming the standard ap- proach for fast learning adaptation across domains. Also known as transfer learning or learning transfer, knowledge transfer has been a critical technology for enabling several real-world applications, including object detection ( Zhang et al. , 2014 ), image segmentation ( Kendall et al. , 2018 ), * Equal contribution",
    "Department of Computer Science, the": "University of Illinois at Urbana-Champaign, Urbana, USA. Corre- spondence to: Oluwasanmi Koyejo < sanmi@illinois.edu > , Bo Li < lbo@illinois.edu > . Proceedings of the 38 th International Conference on Machine Learning , PMLR 139, 2021. Copyright 2021 by the author(s). multi-lingual machine translation ( Dong et al. , 2015 ), and language understanding evaluation ( Wang et al. , 2019a ), among others. For example, since the release of Ima- geNet ( Russakovsky et al. , 2015 ), pretrained ImageNet mod- els (e.g., on TensorFlow Hub or PyTorch-Hub) have become the default option for the knowledge transfer source due to its broad coverage of visual concepts and compatibility with various visual tasks ( Huh et al. , 2016 ). Motivated by its importance, many studies have explored the factors associ- ated with knowledge transferability. Most recently, Salman et al. ( 2020 ) showed that more robust pretrained ImageNet models transfer better to downstream tasks, which reveals that adversarial training helps to improve knowledge trans- ferability. In the meantime, adversarial transferability has been exten- sively studied—a phenomenon that an adversarial instance generated against one model has high probability attack an- other one without additional modiﬁcation ( Papernot et al. , 2016 ; Goodfellow et al. , 2014 ; Joon Oh et al. , 2017 ). Hence, adversarial transferability is widely exploited in black-box attacks ( Ilyas et al. , 2018 ; Liu et al. , 2016 ; Naseer et al. , 2019 ). A line of work has been conducted to bound the adversarial transferability based on model (gradient) simi- larity ( Tram ` er et al. , 2017b ). Given that both adversarial transferability and knowledge transferability are impacted by certain model similarity and adversarial ML properties, in this work, we aim to conduct the ﬁrst study to analyze the connections between them and ask, What is the fundamental connection between knowledge transferability and adversarial trans- ferability? Can we measure one and indicate the other? Technical Contributions. In this paper, we take the ﬁrst step towards exploring the fundamental relation between adversarial transferability and knowledge transferability. We make contributions on both theoretical and empirical fronts. • We formally deﬁne the adversarial transferability for the ﬁrst time by considering all potential adversarial pertur- bation vectors. We then conduct thorough and novel the- oretical analysis to characterize the precise connection between adversarial transferability and knowledge trans- ferability based on our deﬁnition.",
    "arXiv:2006.14512v4  [cs.LG]  8 Jul 2021": "Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability • In particular, we prove that high adversarial transferability will indicate high knowledge transferability, which can be represented as the distance in an inner product space deﬁned by the Hessian of the adversarial loss. In the meantime, we prove that high knowledge transferability will indicate high adversarial transferability. • Based on our theoretical insights, we propose two practi- cal adversarial transferability metrics that quantitatively measure the adversarial transferability in practice. We then provide simulational results to verify how these met- rics connect with the knowledge transferability in a bidi- rectional way. • Extensive experiments justify our theoretical insights and the proposed adversarial transferability metrics, leading to our discussion on potential applications and future re- search. Related Work There is a line of research studying different factors that affect knowledge transferability ( Yosinski et al. , 2014 ; Long et al. , 2015 ; Wang et al. , 2019b ; Xu et al. , 2019 ; Shinya et al. , 2019 ). Further, empirical observations show that the correlation between learning tasks ( Achille et al. , 2019 ; Zamir et al. , 2018 ), the similarity of model architec- tures, and data distribution are all correlated with different knowledge transfer abilities. Interestingly, recent empirical evidence suggests that adversarially-trained models transfer better than non-robust models ( Salman et al. , 2020 ; Utrera et al. , 2020 ), suggesting a connection between the adversar- ial properties and knowledge transferability. On the other hand, several approaches have been proposed to boost the adversarial transferability ( Zhou et al. , 2018 ; Demontis et al. , 2019 ; Dong et al. , 2019 ; Xie et al. , 2019 ). Beyond the above empirical studies, there are a few existing analyses of ad- versarial transferability, which explore different conditions that may enhance adversarial transferability ( Athalye et al. , 2018 ; Tram ` er et al. , 2017b ; Ma et al. , 2018 ; Demontis et al. , 2019 ). In this work, we aim to bridge the connection be- tween adversarial and knowledge transferability, both of which reveal interesting properties of ML model similarities from different perspectives.",
    "Transferability": "This section introduces the preliminaries and the formal deﬁnitions of the knowledge and adversarial transferability, and formally deﬁnes our problem of interest. Notation. Sets are denoted in blackboard bold, e.g., R , and the set of integers { 1 . . . n } is denoted as [ n ] . Distributions are denoted in calligraphy, e.g., D , and the support of a distribution D is denoted as supp ( D ) . Vectors are denoted as bold lower case letters, e.g., x ∈ R n , and matrices are denoted as bold uppercase letters, e.g., W . We denote the entry-wise product operator between vectors or matrices as ⊙ . The Moore–Penrose inverse of a matrix W is denoted as W † . We use ∥· ∥ 2 to denote Euclidean norm induced by Euclidean inner product ⟨· , ·⟩ . The standard inner prod- uct of two matrices is deﬁned as ⟨ W , M ⟩ = tr( W ⊤ M ) , where tr( · ) is the trace of a matrix. The Frobenius norm ∥·∥ F is induced by the standard matrix inner product. More- over, in the (semi-)inner product space deﬁned by a positive (semi-)deﬁnite matrix S , the (semi-)inner product of two vectors or matrices is deﬁned by ⟨ v 1 , v 2 ⟩ S = v ⊤",
    "Sv 2 or": "⟨ W , M ⟩ S = tr( W ⊤ SM ) , respectively. Given a vector v , we deﬁne its normalization as b v = v / ∥ v ∥ 2 . When using a denominator ∥· ∥ ∗ other than Euclidean norm, we denote",
    "the normalization as b v | ∗ .": "Given a (vector-valued) function f , we denote f ( x ) as its evaluated value at x , and f represents the function it- self in the corresponding Hilbert space. Composition of functions is denoted as g ◦ f ( x ) = g ( f ( x )) . We use ⟨· , ·⟩ D to denote the inner product induced by distribu- tion D and inherited from Euclidean inner product, i.e., ⟨ f 1 , f 2 ⟩ D = E x ∼D ⟨ f 1 ( x ) , f 2 ( x ) ⟩ . Accordingly, we use ∥· ∥ D to denote the norm induced by the inner product ⟨· , ·⟩ D , i.e., ∥ f ∥ D = p ⟨ f, f ⟩ D . When the inherited in- ner product is deﬁned by S , we denote ⟨ f 1 , f 2 ⟩ D , S = E x ∼D ⟨ f 1 ( x ) , f 2 ( x ) ⟩ S , and similarly for ∥ f ∥ D , S .",
    "Knowledge Transferability Given a pre-trained source": "model f S : R n → R m and a target domain x ∈ R n with data distribution x ∼D and target labels y ( x ) ∈ R d , knowledge transferability is deﬁned as the performance of ﬁne-tuning f S on D to predict y . Concretely, knowledge transferability can be represented as a loss L ( · , y, D ) after ﬁne-tuning by composing the ﬁxed source model with a trainable function g : R m → R d , typically from a small function class g ∈ G , i.e. , min g ∈ G L ( g ◦ f S , y, D ) , (1) where the loss function L measures the error between g ◦ f S and the ground truth y under the target data distribution D . For example, for neural networks it is usual to stack on and ﬁne-tune a linear layer; here G is the afﬁne function class. We will focus on the afﬁne setting in this paper. For our purposes, a more useful measure of transfer is to compare the quality of the ﬁne-tuned model to a model trained directly on the target domain f T : R n → R d . Thus, we study the following surrogate of knowledge transferabil- ity, where the ground truth target is replaced by a reference target model f T : min g ∈ G L ( g ◦ f S , f T , D ) . (2)",
    "Adversarial Attacks. For simplicity we consider untar-": "geted attacks that seeks to maximize the deviation of model Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability f S f T attack Attack generated against f T Attack generated against f S attack attack attack Output deviations α f S → f T 1",
    "( x ) =": "ℓ adv ( f S ( x ), f S ( x + δ f T , ϵ ( x ))) ℓ adv ( f S ( x ), f S ( x + δ f S , ϵ ( x ))) α f S → f T 2",
    "= ∥피 x ∼풟 [": "Δ f T → f T ( x ) Δ f T → f S ( x ) ⊤ ]∥ F Δ f S → f T ( x ) = f T ( x + δ f S , ϵ ( x )) − f T ( x ) Δ f T → f T ( x ) = f T ( x + δ f T , ϵ ( x )) − f T ( x ) Δ f S → f S ( x ) = f S ( x + δ f S , ϵ ( x )) − f S ( x ) Δ f T → f S ( x ) = f S ( x + δ f T , ϵ ( x )) − f S ( x )",
    "δ f S , ϵ ( x )": "(a) min g 2 G k f ? − g ◦ f ⇧ k D , H ? min g 2 G kr f > ? −r ( g ◦ f ⇧ ) > k D , H ? min g 2 G k y − g ◦ f S k D , H ?",
    "Generalized Adversarial Transferability:": "Gradient Matching Distance: Function Matching Distance:",
    "Knowledge Transfer Distance:": "↵ f ? ! f ⇧ 1 , ↵ f ? ! f ⇧ 2 Proposed Adversarial Transferability Metrics: Proved in section 3.1 Proved in section 3.2 Proved in section 3.3 Deﬁned in section 2 Bidirectional Indication Bidirectional Indication Bidirectional Indication Practical Representatives",
    "1 , A ?, ⇧": "2 (b) Figure 1. (a) An illustration of the two proposed adversarial transferability metrics α 1 , α 2 under different adversarial transferability settings, i.e. , α f S → f T 1 , α f T → f S 1 , α f S → f T 2 , and α f T → f S 2 . (b) An overview of the theoretical analysis framework, and its practical inspirations, where ⋆, ⋄∈{ T, S } and ⋆ ̸ = ⋄ . The three blue double-headed arrows are the bidirectional indication relationships proved in our theory section, and the dashed green arrow shows in practice how the two proposed adversarial transferability metrics are measured as representatives of the generalized adversarial transferability based on our theory. output as measured by a given adversarial loss function ℓ adv ( · , · ) . The targeted attack can be viewed as a special case. Without loss of generality, we assume the adversarial loss is non-negative . Given a datapoint x and model f , an adversarial example of magnitude ϵ is denoted by δ f,ϵ ( x ) , computed as:",
    "∥ δ ∥≤ ϵ": "ℓ adv ( f ( x ) , f ( x + δ )) , and we can see that by deﬁnition, 0 ≤ ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 1 ,ϵ ( x ))) ≤ ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 2 ,ϵ ( x ))) . Therefore, 0 ≤ ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 1 ,ϵ ( x ))) ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 2 ,ϵ ( x ))) ≤ 1 , where we deﬁne 0 / 0 = 0 if necessary. Hence, α f 1 → f 2 1 = E x ∼D [ α f 1 → f 2 1 ( x )] is also in [0 , 1] . Next, we use Proposition 2.1 to prove the same property for α f 1 → f 2 2 . Note that ( α f 1 → f 2 2 ) 2 = E x 1 , x 2 h ⟨ \\ ∆ f 1 → f 1 ( x 1 ) , \\ ∆ f 1 → f 1 ( x 2 ) ⟩· ⟨ \\ ∆ f 1 → f 2 ( x 1 ) , \\ ∆ f 1 → f 2 ( x 2 ) ⟩ i (23) is the expectation of the product of two inner products, where each inner product is of two unit-length vector. That is being said, ⟨ \\ ∆ f 1 → f 1 ( x 1 ) , \\ ∆ f 1 → f 1 ( x 2 ) ⟩∈ [ − 1 , 1] and ⟨ \\ ∆ f 1 → f 2 ( x 1 ) , \\ ∆ f 1 → f 2 ( x 2 ) ⟩∈ [ − 1 , 1] . Therefore, we know that",
    "Adversarial Transferability. The process of adversarial": "transfer involves applying the adversarial example gener- ated against a model f 1 to another model f 2 . Thus, adver- sarial transferability from f 1 to f 2 measures how well δ f 1 ,ϵ attacks f 2 . We propose two metrics, namely, α 1 and α 2 that characterize adversarial transferability from complementary perspectives. To provide a visual overview of our deﬁni- tions for the proposed adversarial transferability metrics, we present an illustration in Figure 1 (a).",
    "Deﬁnition 1 (The First Adversarial Transferability) . The": "ﬁrst adversarial transferability from f 1 to f 2 at data sample",
    "x ∼D , is deﬁned as": "α f 1 → f 2 1 ( x ) = ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 1 ,ϵ ( x ))) ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 2 ,ϵ ( x ))) . Taking the expectation, the ﬁrst adversarial transferability is deﬁned as α f 1 → f 2 1",
    "= E x ∼D": "\u0002 ∇ f ⋄ ( x ) ⊤ ∇ f ⋆ ( x ) T ⊤ \u0003",
    "Deﬁnition 2 (The Second Adversarial Transferability) . The": "second adversarial transferability from f 1 to f 2 , under data",
    "distribution x ∼D , is deﬁned as": "α f 1 → f 2 2",
    "= ∥ E x ∼D [": "\\ ∆ f ⋆ → f ⋆ ( x ) \\ ∆ f ⋆ → f ⋄ ( x ) ⊤ ] ∥ F . With these insights, next we will derive our ﬁrst theorem.",
    "where x 1 , x 2": "i.i.d. ∼D , and θ f 1 → f 1 ( x 1 , x 2 ) = ⟨ \\ ∆ f 1 → f 1 ( x 1 ) , \\ ∆ f 1 → f 1 ( x 2 ) ⟩ θ f 1 → f 2 ( x 1 , x 2 ) = ⟨ \\ ∆ f 1 → f 2 ( x 1 ) , \\ ∆ f 1 → f 2 ( x 2 ) ⟩ Proof. Recall that we want to show",
    "( x )": "\\ ∆ f 1 → f 1 ( x ) as a whole, we can show exactly the same as the Proposition 2.1 that (( α 1 ∗ α 2 ) f 1 → f 2 ) 2 = E x 1 , x 2 [ θ f 1 → f 1 ( x 1 , x 2 ) θ f 1 → f 2 ( x 1 , x 2 )] , (24) where θ f 1 → f 1 ( x 1 , x 2 ) = ⟨ α f 1 → f 2 1",
    "Proposition 2.2. The adversarial transferabililty metrics": "α f 1 → f 2 1 , α f 1 → f 2 2 and ( α 1 ∗ α 2 ) f 1 → f 2 are in [0 , 1] . So far, we have deﬁned knowledge transferability, and two adversarial trasferability metrics. We can now analyze their connections more precisely. Problem of Interest. Given a source model f S : R n → R m , the target data distribution x ∼D , the ground truth target y : R n → R d , and a target reference model f T : R n → R d , we aim to study how the adversarial trans- ferability between f S and f T , characterized by the two proposed adversarial transferability metrics, connects to the knowledge transfer loss min g ∈ G L ( g ◦ f S , y, D ) with afﬁne functions g ∈ G (equation 1 ).",
    "3. Theoretical Analysis": "In this section, we present the theoretical analysis on how the adversarial transferability and the knowledge transfer process are tied together. To simplify the discussion, as the objects studied in this section are speciﬁcally focused on the source domain S and the target domain T , we can use ⋆ or ⋄ as a placeholder for either S or T throughout this section.",
    "Theoretical Analysis Overview. In subsection 3.1 , we": "deﬁne the two generalized adversarial transferabilities , ( i.e. , A 1 , A 2 ), and present Theorem 3.1 showing that A 1 , A 2 together determine a gradient matching distance min g ∈ G ∥∇ f ⋆ −∇ g ◦ f ⋄ ∥ , between the Jacobian matrices of the source and target models in an inner product space de- ﬁned by the Hessian of the adversarial loss function. In the same subsection, we also show that α 1 and α 2 represent the most inﬂuential factors in A 1 and A 2 , respectively. Next, we explore the connection to knowledge transferability in subsection 3.2 via Theorem 3.2 which shows the gradient matching distance approximates the function matching dis- tance, i.e. , min g ∈ G ∥ f ⋆ − g ◦ f ⋄ ∥ , with a distribution shift up to a Wasserstein distance. Finally, in subsection 3.3 we complete the analysis by outlining the connection between the function matching distance and the knowledge transfer loss. A visual overview is shown in Figure 1 (b).",
    "Setting. As adversarial perturbations are constrained in a": "small ϵ -ball, it is reasonable to approximate the deviation of model outputs by its ﬁrst-order Taylor approximation. Speciﬁcally, in this section we consider the Euclidean ϵ -ball. Therefore, the output deviation of a function f at x given a small perturbation ∥ δ ϵ ∥ 2 ≤ ϵ can be approximated by f ( x + δ ϵ ) − f ( x ) ≈∇ f ( x ) ⊤ δ ϵ , where ∇ f ( x ) is the Jacobian matrix of f at x . We consider a convex and twice-differentiable adversarial loss function ℓ ⋆ adv ( · ) that measures the deviation of model output f ⋆ ( x + δ ϵ ) − f ⋆ ( x ) , with minimum ℓ ⋆",
    "adv ( 0 ) = 0 ,": "for ⋆ ∈{ S, T } . We note that we should treat the adversar- ial loss on f S and f T differently, as they may have differ- ent output dimensions. Accordingly, the adversarial attack (equation 3 ) can be written as δ f ⋆ ,ϵ ( x ) = arg max",
    "∥ δ ∥ 2 ≤ ϵ": "∥ T ∇ f ⋆ ( x ) ⊤ δ ∥ 2 2 . That is being said, the adversarial attack is the right singular vector corresponding to the largest singular value (in absolute value) of T ∇ f ⋆ ( x ) ⊤ . Similarly, we can see the singular values σ f ⋆ , H ⋆ ( x ) ∈ R n , deﬁned as the descending (in absolute value) singular values of the Jacobian ∇ f ⋆ ( x ) ⊤ ∈ R ·× n in the H ⋆ inner product space (equation 8 ), are the singular values of T ∇ f ⋆ ( x ) ⊤ . With this perspective, if we write down the singular value decomposition of T ∇ f ⋆ ( x ) ⊤ , i.e. , T ∇ f ⋆ ( x ) ⊤ = U ⋆ ( x )Σ ⋆ ( x ) V ⊤",
    "The Small- ϵ Regime.": "Recall that the adversarial loss ℓ ⋆ adv ( · ) studied in this section is convex, twice-differentiable, Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability and achieves its minimum at 0 , thus in the small ϵ regime: ℓ ⋆ adv ( ∇ f ⋆ ( x ) ⊤ δ ϵ ) = \u0000",
    "δ ⊤": "ϵ ∇ f ⋆ ( x ) H ⋆ ∇ f ⋆ ( x ) ⊤ δ ϵ \u0001 1 / 2 = ∥∇ f ⋆ ( x ) ⊤ δ ϵ ∥ H ⋆ , which is the norm of f ⋆ ’s output deviation in the inner product space deﬁned by the Hessian H ⋆ of the squared adversarial loss ( ℓ ⋆ adv ) 2 . Accordingly, the adversarial attacks ( 4 ) can be written as δ f ⋆ ,ϵ ( x ) = arg max",
    "Matching Distance, and Vice Versa": "We present an interesting ﬁnding in this subsection, i.e. , the generalized adversarial transferabilities A 1 , A 2 have a direct connection to the gradient matching distance between the source model f S : R n → R m and target model f T : R n → R d . The gradient matching distance is deﬁned as the smallest distance an afﬁne transformation can achieve between their Jacobians ∇ f T : R n → R n × d and ∇ f S : R n → R n × m in the inner product space deﬁned by H ⋆ and data sample distribution x ∼D , as shown below. min g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ D , H ⋆ , (7) where g ∈ G are afﬁne transformations. Note that g : R m → R d if ( ⋆, ⋄ ) = ( T, S ) , and g : R d → R m if ( ⋆, ⋄ ) = ( S, T ) . We defer the analysis of how the gradi- ent matching distance approximates the knowledge transfer loss, and focus on its connection to adversarial transfer. A Full Picture of Adversarial Transferability. A key ob- servation is that the adversarial attack (equation 5 ) is the singular vector corresponding to the largest singular value of the Jacobian ∇ f ⋆ ( x ) in the H ⋆ inner product space. Thus, information regarding other singular values that are not revealed by the adversarial attack. Therefore, we can consider other singular values, corresponding to smaller signals than the one revealed by adversarial attacks, to com- plete the analysis. We denote σ f ⋆ , H ⋆ ( x ) ∈ R n as the de- scending (in absolute value) singular values of the Jacobian ∇ f ⋆ ( x ) ⊤ ∈ R ·× n in the H ⋆ inner product space. In other words, we denote σ f ⋆ , H ⋆ ( x ) ∈ R n as the square root of the descending eigenvalues of ∇ f ⋆ ( x ) H ⋆ ∇ f ⋆ ( x ) ⊤ , i.e. , σ f ⋆ , H ⋆ ( x ) = [ σ (1) f ⋆ ( x ) , . . . , σ ( n )",
    "f ⋆ ( x )] ⊤ .": "(8) Note that the number of non-zero singular values may be less than n , in which case we ﬁll the rest with zeros such that vector is n -dimensional. Since the adversarial attack δ f ⋆ ,ϵ ( x ) corresponds to the largest singular value σ f ⋆ ( x ) (1) , we can also generalize the adversarial attack by including all the singular vectors. i.e. ,",
    "f ⋆ ( x )": "corresponds to σ ( i )",
    "f ⋆ ( x ) ,": "∀ i ∈ [ n ] . (9) Loosely speaking, one could think δ ( i )",
    "f ⋆ ( x ) as the adversarial": "attack of f ⋆ ( x ) in the subspace orthogonal to all the previous",
    "attacks, i.e. , δ ( j )": "f ⋆ ( x ) for ∀ j ∈ [ i − 1] . Accordingly, for ∀ i ∈ [ i ] we denote the output deviation as ∆ ( i ) f ⋆ → f ⋄ ( x ) = ∇ f ⋄ ( x ) ⊤ δ ( i )",
    "f ⋆ ( x ) .": "(10) As a consequence, we generalize the ﬁrst adversarial trans- ferability to be a n -dimensional vector A ⋆, ⋄",
    "1 ( x ) including": "the adversarial losses of all of the generalized adversarial attacks, where the i th element in the vector is",
    "1 ( x ) ( i ) =": "∥ ∆ ( i ) f ⋆ → f ⋄ ( x ) ∥ H ⋄ ∥∇ f ⋄ ( x ) ∥ H ⋄ . Moreover, to connect the magnitude of the output deviation to the generalized singular values (equation 9 ), we have ∥ ∆ ( i ) f ⋆ → f ⋆ ( x ) ∥ H ⋆ = ∥∇ f ⋆ ( x ) ⊤ δ ( i ) f ⋆ ( x ) ∥ H ∗ = σ ( i ) f ⋆ , H ⋆ ( x ) , and similarly, ∥∇ f ⋄ ( x ) ∥ H ⋄ = ∥∇ f ⋄ ( x ) δ (1) f ⋄ ( x ) ∥ H ⋄ = σ (1) f ⋄ , H ⋄ ( x ) . Therefore, we can ﬁnally rewrite the X 1 , X 2 in equation 30 as X 1 = σ ( i ) f ⋆ , H ⋆ ( x 1 ) σ ( j ) f ⋆ , H ⋆ ( x 2 ) · ⟨ \\ ∆ ( i ) f ⋆ → f ⋆ ( x 1 )",
    "transferability, i.e. , A ⋆, ⋄": "1 ( x ) (1) is the same as the α f ⋆ → f ⋄ 1",
    "H ⋄": "\u0003",
    "⋆ ∥ D , H ⋆ ,": "where the expectation is taken over x 1 , x 2 i.i.d. ∼D , and v ⋆, ⋄ ( x ) = σ (1) f ⋄ , H ⋄ ( x ) σ f ⋆ , H ⋆ ( x ) ⊙ A ⋆, ⋄",
    "1 ( x )": "J = E x ∼D [ ∇ f ⋄ ( x ) ⊤ ∇ f ⋄ ( x )] .",
    "Moreover, A ⋆, ⋄": "2 ( x 1 , x 2 ) is a matrix, and its element in the i th row and j th column is",
    "A ⋆, ⋄": "2 ( x 1 , x 2 ) ( i,j ) = ⟨ \\ ∆ ( i ) f ⋆ → f ⋆ ( x 1 )",
    "H ⋆ ,": "\\ ∆ ( j ) f ⋆ → f ⋆ ( x 2 )",
    "H ⋆ ⟩": "· ⟨ \\ ∆ ( i ) f ⋆ → f ⋄ ( x 1 )",
    "H ⋄ ,": "\\ ∆ ( j ) f ⋆ → f ⋄ ( x 2 )",
    "J † | H ⋄ .": "We can write X 1 X 2 = σ (1) f ⋄ , H ⋄ ( x 1 ) σ ( i ) f ⋆ , H ⋆ ( x 1 ) A ⋆, ⋄ 1 ( x 1 ) ( i ) · A ⋆, ⋄ 2 ( x 1 , x 2 ) ( i,j ) · σ (1) f ⋄ , H ⋄ ( x 2 ) σ ( j ) f ⋆ , H ⋆ ( x 2 ) A ⋆, ⋄ 1 ( x 2 ) ( j ) ∥ J † ∥ H ⋄ . Plugging the above into equation 30 , and rearranging the double summation, we have ( 30 ) = ∥∇ f ⊤ ⋆ ∥ 2",
    "⋆ ∥ D , H ⋆ outside repre-": "sents the overall magnitude of the loss. In the fraction, the ∥∇ f ⊤ ⋆ ∥ D , H ⋆ in the denominator normalizes the σ f ⋆ in the numerator. Similarly, though more complicated, the",
    "∥ J † ∥ − 1": "2 in the denominator corresponds to the σ (1) f ⋄ in the numerator. We note that these are properties of f ⋆ , f ⋄ . Next, observe that the components directly related to the adversarial transfer process are the generalized adversarial transferability A 1 and A 2 . Let us neglect the superscript ( i ) or ( j ) for now, so we can see that their interpretations are the same as we introduced for α 1 and α 2 in section 2 . That is, A 1 captures the magnitude of the deviation in model outputs caused by adversarial attacks, while A 2 captures the direction of the deviation. A minor difference between α 2 and A 2 is that the second inner product in the elements of A 2 is deﬁned by a positive semi-deﬁnite matrix c",
    "J † . For": "practical implementation, we choose to neglect this term, and use the standard Euclidean inner product in α 2 , which can be understood as a stretched version of the c",
    "J † inner": "product space. Moreover, as the singular vector σ f ⋆ has descending entries, we can see that in the vector A 1 and the matrix A 2 , the elements with superscript (1) have the most inﬂuence in the relations. In other words, the two proposed adversarial transferability metrics, α 1 and α 2 , are the most inﬂuential factors in equation 12 . We can also see that the combined metric ( α 1 ∗ α 2 ) also stems from here by only considering the components with the ﬁrst superscript. To interpret the relation between the gradient matching dis- tance and the adversarial transferabilities, we introduce the following proposition. This shows that, in general, A 1 and A 2 with their elements closer to 1 can serve as a bidirec- tional indicator of a smaller gradient matching distance.",
    "Proposition 3.1. In Theorem 3.1 ,": "0 ≤ E [ v ⋆, ⋄ ( x 1 ) ⊤ A ⋆, ⋄ 2 ( x 1 , x 2 ) v ⋆, ⋄ ( x 2 )] ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ · ∥ J † ∥ − 1",
    "Function Matching Distance, and Vice Versa": "To bridge the gradient matching distance to the knowledge transfer loss, an immediate step is to connect the gradient distance to the function distance which directly serves as a surrogate knowledge transfer loss as deﬁned in (equation 2 ). Speciﬁcally, in this subsection, we present a connection between the function matching distance, i.e. , min g ∈ G ∥ f ⋆ − g ◦ f ⋄ ∥ D , H ⋆ , (13) and the gradient matching distance, i.e. , min g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ D , H ⋆ , (14) where g ∈ G are afﬁne transformations. For intuition, consider a point x 0 in the input space R n , a path γ x : [0 , 1] → R n such that γ x (0) = x 0 and γ x (1) = x . Then, denoting γ as the function of x , we can write the difference between the two functions as f ⋆ − g ◦ f ⋄ = Z 1 0 ( ∇ f ⋆ ( γ ( t )) −∇ ( g ◦ f ⋄ ( γ ( t )))) ⊤ ˙ γ ( t ) d t + ( f ⋆ ( x 0 ) − g ◦ f ⋄ ( x 0 )) . Noting that the function difference is a path integral of the gradient difference, we should expect a distribution shift when characterizing their connection, i.e. , the integral path affects the original data distribution D . Accordingly, as the integral path may leave the support of D , it is necessary to assume the smoothness of the function, as shown below. Denoting the optimal g ∈ G in ( 13 ) as ˜ g , and one of the optimal g ∈ G in ( 14 ) as ˜ g ′ , we deﬁne h ⋆, ⋄ := f ⋆ − ˜ g ◦ f ⋄ and h ′ ⋆, ⋄ := f ⋆ − ˜ g ′ ◦ f ⋄ , (15) and we can see that the gradient matching distance and the function matching distance can be written as ( 13 ) = ∥ h ⋆, ⋄ ∥ D , H ⋆ and ( 14 ) = ∥∇ h ′ ⋆, ⋄",
    "⊤ ∥ D , H ⋆ .": "(34) The ﬁrst inequality. Then, we can prove the ﬁrst inequality using Lemma D.3 . Let x 0 ∈ R n be a free variable, and then set b = h ′ ⋆, ⋄ ( x 0 ) . Noting that ∥ h ⋆, ⋄ ∥ 2 D , H ⋆ by deﬁnition is the minimum of this function distance, we have ∥ h ⋆, ⋄ ∥ 2",
    "D , H ⋆ ≤∥∇ h ′⊤": "⋆, ⋄ ∥ 2 D 1 , H ⋆ + β 2 ( B − τ ) 2 + 1 3 n ∥∇ h ′⊤ ⋆, ⋄ ∥ 2",
    "D , H ⋆ ≤": "2 τ 2 ∥ h ⋆, ⋄ ∥ 2 D 2 , H ⋆ + β 2 τ 2 , where n is the dimension of x ∼D , and B = inf x 0 ∈ R n sup x ∈ supp ( D ) ∥ x − x 0 ∥ 2 is the radius of the supp ( D ) . The ( · ) + is an operator deﬁned by ∀ x ∈ R : ( x ) + = x if x ≥ 0 and ( x ) + = 0 otherwise. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Proof. Let us begin with recalling the deﬁnition of h ⋆, ⋄ and h ′ ⋆, ⋄ . The optimal afﬁne transformation g ∈ G in the function matching distance ( 13 ) is ˜ g , and one of the optimal g ∈ G in the gradient matching distance is ( 14 ) ˜ g ′ . Accordingly, we denote h ⋆, ⋄ := f ⋆ − ˜ g ◦ f ⋄ and h ′ ⋆, ⋄ := f ⋆ − ˜ g ′ ◦ f ⋄ , and we can see that the gradient matching distance and the function matching distance can be written as ( 13 ) = ∥ h ⋆, ⋄ ∥ D , H ⋆ and ( 14 ) = ∥∇ h ′ ⋆, ⋄",
    "where n is the dimension of x": "∼ D , and B = inf x 0 ∈ R n sup x ∈ supp ( D ) ∥ x − x 0 ∥ 2 is the radius of supp ( D ) . We note that the above theorem compromises some tightness in exchange for a cleaner presentation without losing its core message, which is discussed in the proof of the theorem. Interpretation of Theorem 3.2 . The theorem shows that under the smoothness assumption, the gradient matching distance indicates the function matching distance, and vice versa, with a distribution shift bounded in Wasserstein dis- tance. As the distribution shift is in general necessary, we conjecture that using different data distributions for adversar- ial transfer and knowledge transfer can also be applicable.",
    "Knowledge Transferability, and Vice Versa": "To complete the story, it remains to connect the function matching distance to knowledge transferability. As the ad- versarial transfer is symmetric ( i.e. , either from f S → f T or f T → f S ), we are able to use the placeholders ⋆, ⋄∈ { S, T } all the way through. However, as the knowledge transfer is asymmetric ( i.e. , f S → y to the target ground truth), we need to instantiate the direction of adversarial transfer to further our discussion. Adversarial Transfer from f T → f S . As we can see",
    "from the A ⋆, ⋄": "1 in Theorem 3.1 , this direction corresponds to ( ⋆, ⋄ ) = ( T, S ) . Accordingly, the function matching distance (equation 13 ) becomes min g ∈ G ∥ f T − g ◦ f S ∥ D , H T . (16) We can see that equation 16 directly translates to the surro- gate knowledge transfer loss that uses the “pseudo ground truth” from the target reference model f T . In other words, the function matching distance serves as an approximation of the knowledge transfer loss deﬁned as their distance in the inner product space of H T , i.e. , min g ∈ G ∥ y − g ◦ f S ∥ D , H T . (17) The accuracy of the approximation depends on the perfor- mance of f T , as shown in the following theorem. Theorem 3.3. The surrogate transfer loss ( 16 ) and the true transfer loss ( 17 ) are close, with an error of ∥ f T − y ∥ D , H T . −∥ f T − y ∥ D , H T ≤ ( 17 ) − ( 16 ) ≤∥ f T − y ∥ D , H T Adversarial Transfer from f S → f T . This direction cor- responds to ( ⋆, ⋄ ) = ( S, T ) . Accordingly, the function matching distance (equation 13 ) becomes min g ∈ G ∥ f S − g ◦ f T ∥ D , H S . (18) Since the afﬁne transformation g acts on the target reference model, it can not be directly viewed as a surrogate transfer loss. However, interesting interpretations can be found in this direction, depending on the output dimension of f S : R n → R m and f T : R n → R d . That is, when the direction of adversarial transfer is from f S → f T , the indicating relation between it and knowledge transferability would possibly be unidirectional, depend- ing on the dimensions. More discussion is included in the appendix section B due to space limitation.",
    "4. Synthetic Experiments": "The synthetic experiment aims to bridge the gap between theory and practice by verifying some of the theoretical insights that may be difﬁcult to compute for large-scale ex- periments. Speciﬁcally, the synthetic experiment aims to verify: ﬁrst, how inﬂuential are the two proposed adversarial transferability metrics α 1 , α 2 comparing to the other factors in the generalized adversarial attacks (equation 9 ); Second, how does the gradient matching distance track the knowl- edge transfer loss. The dataset ( N = 5000 ) is generated by a Gaussian mixture of 10 Gaussians. The ground truth target is set to be the sum of 100 radial basis functions. The dimension of x is 50 , and the dimension of the target is 10 . Details of the datasets are defer to appendix section F . Models Both the source model f S and target model f T are one-hidden-layer neural networks with sigmoid activation. Methods First, sample D = { ( x i , y i ) } N i =1 from the distri- bution, where x is 50 -dimensional, y is 10 -dimensional. Then we train a target model f T on D . To derive the source models, we ﬁrst train a target model on D with width m = 100 . Denoting the weights of a target model as W , we randomly sample a direction V where each entry of V is sampled from U ( − 0 . 5 , 0 . 5) , and choose a scale t ∈ [0 , 1] . Subsequently, we perturb the model weights of the clean source model as W ′ := W + t V , and deﬁne the source model f S to be a one-hidden-layer neural network with weights W ′ . Then, we compute each of the quantities we care about, including α 1 , α 2 from both f S → f T and f T → f S , the gradient matching distance (equation 7 ), and the actual knowledge transfer distance (equation 17 ). We use the standard ℓ 2 loss as the adversarial loss function. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Results We present two sets of experiment in Figure 2 . The indication relations between adversarial transferability and knowledge transferability can be observed. Moreover: 1. the metrics α 1 , α 2 are more meaningful if using the regular",
    "attacks δ (1)": "f ⋆ ; 2. the gradient matching distance tracks the actual knowledge transferability loss; 3. the directions of f T → f S and f S → f T are similar.",
    "(a) δ (1)": "f ⋆",
    "(b) δ (2)": "f ⋆ Figure 2. As deﬁned in equation 9 , (a) corresponds to the regu- lar adversarial attacks, while (b) the secondary adversarial attack. That is, (b) represents the other information in the adversarial transferring process compared with the ﬁrst. The x-axis shows the scale of perturbation t ∈ [0 , 1] that controls how much the source model deviates from the target model. There are in total 6 quantities reported. Speciﬁcally, α f T → f S 1 is black solid ; α f S → f T 1 is black dotted ; α f T → f S 2 is green solid ; α f S → f T 2",
    "is green dot-": "ted ; the gradient matching loss is red solid ; and the knowledge",
    "5. Experimental Evaluation": "We present the real-data experiments based on both image and natural language datasets in this section, and discuss the potential applications.",
    "Knowledge": "Transferability. In this experiment, we show how to use adversarial transferability to identify the optimal transfer learning candidates from a pool of models trained on the same source dataset. We ﬁrst train 5 different architectures (AlexNet, Fully connected network, LeNet, ResNet18, ResNet50) on cifar10 ( Krizhevsky et al. , 2009 ). Then we perform transfer learning to STL10 ( Coates et al. , 2011 ) to obtain the knowledge transferability of each, measured by accuracy. At the same time, we also train one ResNet18 on STL10 as the target model, which has poor accuracy because of the lack of data. To measure the adversarial transferability, we generate adversarial examples with PGD ( Madry et al. , 2017 ) on the target model and use the generated adversarial examples to attack each source model. The adversarial transferability is expressed in the form of α 1 and α 2 . Our results in Table 1 indicate that we can use adversarial tarnsferability to forecast knowledge transferability, where the only major computational overheads are training a naive model on the target domain and generating a few adversarial examples. In the end, We further evaluate the signiﬁcance of our results by Pearson score. More details about training and generation of adversarial examples can be found in the appendix G .",
    "Knowledge Trans.": "α 1 α 2 α 1 ∗ α 2 ρ = 0 . 0 73.91 0.394 0.239 0.103 ρ = 0 . 5 73.11 0.385 0.246 0.102 ρ = 1 . 0 72.47 0.371 0.244 0.100 ρ = 2 . 0 71.62 0.370 0.244 0.100 ρ = 5 . 0 72.16 0.378 0.240 0.098 source models f S to one target model f T with following training loss: L train = L CE ( f S ( x ) , y )) + ρ · L cos ( ∇ ˆ x ℓ f S , ∇ ˆ x ℓ f T ) where L CE refers to cross-entropy loss and L cos ( · , · ) the cosine similarity metric. x presents source domain instances while ˆ x presents target domain instances. We explore ρ ∈{ 0 . 0 , 0 . 5 , 1 . 0 , 2 . 0 , 5 . 0 } and ﬁnetune each source model for 50 epochs with learning rate as 0 . 01 . For knowledge transferability, we random initialize the last layer of each source model and ﬁnetune it on STL-10 for 10 epochs with learning rate as 0 . 01 . During the adversarial example generation, we utilize standard ℓ ∞ PGD attack with perturbation scale ϵ = 0 . 1 and 50 attack iterations with step size as ϵ/ 10 . Table 7 shows the relationship between knowledge transferability and adversarial transferability of different source model trained by different ρ . With the increasing of ρ , the adversarial transferabiltiy between source model and target model decreases ( α 1 , α 1 ∗ α 2 become smaller), and the knowledge transferability also decreases. We also plot the α 1 with its corresponding transfer loss on each instance, as shown in Figure 7 . The negative correlation between α 1 and transfer loss conﬁrms our theoretical insights. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability 0.2 0.4 0.6 0.8 1.0 1.2 1 0 1 2 3 4 5 6 transfer loss R:-0.303144 =0.0 =0.5 =1.0 =2.0 =5.0 Figure 7. Distribution of per-sample knowledge transfer loss and α 1 . The Pearson score shows negative correlation between α 1 and transfer loss. The higher the loss is, the lower the knowledge transferability is, the lower the α 1 should be.",
    "Adversarial": "Transferability. In addition, we are interested in the impact of knowledge transferability on adversarial transferability. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "6. Conclusion": "We theoretically analyze the relation between adversarial transferability and knowledge transferability. We provide empirical experimental justiﬁcations in pratical settings. Both our theoretical and empirical results show that adver- sarial transferability can indicate knowledge transferability and vice versa. We expect our work will inspire future work on further exploring other factors that impact knowledge transferability and adversarial transferability.",
    "Acknowledgments": "This work is partially supported by NSF IIS 1909577, NSF CCF 1934986, NSF CCF 1910100, NSF CNS 20-46726 CAR, Amazon Research Award, and the Intel RSA 2020. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "References": "Achille, A., Lam, M., Tewari, R., Ravichandran, A., Maji, S., Fowlkes, C. C., Soatto, S., and Perona, P. Task2vec: Task embedding for meta-learning. In Proceedings of the IEEE International Conference on Computer Vision , pp. 6430–6439, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra- dients give a false sense of security: Circumventing de- fenses to adversarial examples. In International Confer- ence on Machine Learning , pp. 274–283, 2018. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artiﬁcial intelligence and statistics , pp. 215–223, 2011. Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita-Rotaru, C., and Roli, F. Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks. In 28th { USENIX } Security Symposium ( { USENIX } Security 19) , pp. 321– 338, 2019. Dong, D., Wu, H., He, W., Yu, D., and Wang, H. Multi-task learning for multiple language translation. In Proceed- ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 1723–1732, 2015. Dong, Y., Pang, T., Su, H., and Zhu, J. Evading defenses to transferable adversarial examples by translation-invariant attacks. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 4312–4321, 2019. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 , 2014. Huh, M., Agrawal, P., and Efros, A. A. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614 , 2016. Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box adversarial attacks with limited queries and information. In International Conference on Machine Learning , pp. 2137–2146, 2018. Joon Oh, S., Fritz, M., and Schiele, B. Adversarial im- age perturbation for privacy protection–a game theory perspective. In Proceedings of the IEEE International Conference on Computer Vision , pp. 1482–1491, 2017. Kariyappa, S. and Qureshi, M. K. Improving adversarial robustness of ensembles with diversity training. arXiv preprint arXiv:1901.09981 , 2019. Kendall, A., Gal, Y., and Cipolla, R. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 7482–7491, 2018. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Liu, Y., Chen, X., Liu, C., and Song, D. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770 , 2016. Long, M., Cao, Y., Wang, J., and Jordan, M. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning , pp. 97– 105, 2015. Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S., Schoenebeck, G., Song, D., Houle, M. E., and Bailey, J. Characterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613 , 2018. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Miyato, T., Maeda, S.-i., Koyama, M., and Ishii, S. Virtual adversarial training: a regularization method for super- vised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence , 41(8):1979– 1993, 2018. Naseer, M. M., Khan, S. H., Khan, M. H., Khan, F. S., and Porikli, F. Cross-domain transferability of adversarial per- turbations. In Advances in Neural Information Processing Systems , pp. 12885–12895, 2019. Papernot, N., McDaniel, P., and Goodfellow, I. Transfer- ability in machine learning: from phenomena to black- box attacks using adversarial samples. arXiv preprint arXiv:1605.07277 , 2016. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition chal- lenge. International journal of computer vision , 115(3): 211–252, 2015. Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., and Madry, A. Do adversarially robust imagenet models transfer better? arXiv preprint arXiv:2007.08489 , 2020. Shinya, Y., Simo-Serra, E., and Suzuki, T. Understanding the effects of pre-training for object detectors via eigen- spectrum. In Proceedings of the IEEE International Con- ference on Computer Vision Workshops , pp. 0–0, 2019. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Tram ` er, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., and McDaniel, P. Ensemble adversar- ial training: Attacks and defenses. arXiv preprint arXiv:1705.07204 , 2017a. Tram ` er, F., Papernot, N., Goodfellow, I., Boneh, D., and Mc- Daniel, P. The space of transferable adversarial examples. arXiv preprint arXiv:1704.03453 , 2017b. Utrera, F., Kravitz, E., Erichson, N. B., Khanna, R., and Mahoney, M. W. Adversarially-trained deep nets transfer better. arXiv preprint arXiv:2007.05869 , 2020. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations , 2019a. URL https://openreview.net/forum? id=rJ4km2R5t7 . Wang, B., Pei, H., Pan, B., Chen, Q., Wang, S., and Li, B. T3: Tree-autoencoder constrained ad- versarial text generation for targeted attack. In Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP) , pp. 6134–6150, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.495. URL https://www.aclweb. org/anthology/2020.emnlp-main.495 . Wang, Z., Dai, Z., P ´ oczos, B., and Carbonell, J. Charac- terizing and avoiding negative transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 11293–11302, 2019b. Xie, C., Zhang, Z., Zhou, Y., Bai, S., Wang, J., Ren, Z., and Yuille, A. L. Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 2730–2739, 2019. Xu, R., Li, G., Yang, J., and Lin, L. Larger norm more transferable: An adaptive feature norm approach for un- supervised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision , pp. 1426–1435, 2019. Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Advances in neural information processing systems , pp. 3320–3328, 2014. Zamir, A. R., Sax, A., Shen, W., Guibas, L. J., Malik, J., and Savarese, S. Taskonomy: Disentangling task trans- fer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3712– 3722, 2018. Zhang, X., Zhao, J., and LeCun, Y. Character-level convo- lutional networks for text classiﬁcation. arXiv preprint arXiv:1509.01626 , 2015. Zhang, Z., Luo, P., Loy, C. C., and Tang, X. Facial land- mark detection by deep multi-task learning. In European conference on computer vision , pp. 94–108. Springer, 2014. Zhou, W., Hou, X., Chen, Y., Tang, M., Huang, X., Gan, X., and Yang, Y. Transferable adversarial perturbations. In Proceedings of the European Conference on Computer Vision (ECCV) , pp. 452–467, 2018.",
    "Contents Summary": "• Section A : An Example Illustrating the Necessity of both α 1 , α 2 in Characterizing the Relation Between Adversarial Transferability and Knowledge Transferability. • Section B : Detailed discussion about the direction of adversarial transfer from f S → f T in subsection 3.3 . • Section C : Proofs of the propositions in section 2 .",
    "– C.2 : Proof of Proposition 2.2": "• Section D : Proofs of the theorems and propositions in section 3 .",
    "– D.5 : Proof of Theorem B.1": "• Section E : Auxiliary lemmas. • Section F : Details and additional results of the synthetic experiments. • Section G : Details of model training and adversarial examples generations in the experiments section, and ablation study on controlling the adversarial transferability. A. An Example Illustrating the Necessity of both α 1 , α 2 in Characterizing the Relation Between",
    "Subsection 3.3": "In this section, we present a detailed discussion, in addition to subsection 3.3 , about the connection between function matching distance and knowledge transfer distance when the direction of adversarial transfer is from f S → f T . Recall that, to complete the story, it remains to connect the function matching distance to knowledge transferability. As the adversarial transfer is symmetric ( i.e. , either from f S → f T or f T → f S ), we are able to use the placeholders ⋆, ⋄∈{ S, T } all the way through. However, as the knowledge transfer is asymmetric ( i.e. , f S → y to the target ground truth), we need to instantiate the direction of adversarial transfer to further our discussion. We have discussed the direction of adversarial transfer from f T → f S in the main paper, where we show that the function matching distance of this direction, i.e. , min g ∈ G ∥ f T − g ◦ f S ∥ D , H T , ( 16 ) can both upper and lower bound the knowledge transfer distance, i.e. , min g ∈ G ∥ y − g ◦ f S ∥ D , H T . ( 17 ) The direction of adversarial transfer from f S → f T corresponds to ( ⋆, ⋄ ) = ( S, T ) . Accordingly, the function matching distance (equation 13 ) becomes min g ∈ G ∥ f S − g ◦ f T ∥ D , H S . ( 18 ) Since the afﬁne transformation g acts on the target reference model, it can not be directly viewed as a surrogate transfer loss. However, interesting interpretations can be found in this direction, depending on the output dimension of f S : R n → R m and f T : R n → R d . In this subsection in the appendix we provide detailed discussion on the connection between the function matching distance of the direction of adversarial transfer from f S → f T (equation 18 ) and the knowledge transfer distance (equation 17 ). We build this connection by providing the relationships between the two directions of function matching distance, i.e. , equation 16 and equation 18 . That is being said, since we know equation 17 and equation 16 are tied together, we only need to provide relationships between equation 16 and equation 18 to show the connection between equation 18 and equation 17 . Suppose g : R d → R m is full rank, and loosely speaking we can derive the following intuitions. • If d < m , then g is injective and there exists g − 1 : R m → R d such that g − 1 ◦ g is the identity function. That is, if g can map f T to closely track f S , then reversely g − 1 can map f S to f T , showing equation 18 upper bounds equation 16 in some sense. • If d > m , then g is surjective. By symmetry, equation 16 upper bounds equation 18 in some sense. • It is when m = d that equation 16 and equation 18 coincide. Formally, we have the following theorem. Theorem B.1. Denote ˜ g T,S : R m → R d as the optimal solution of equation 16 , and ˜ g S,T : R d → R m as the optimal solution of equation 18 . Suppose the two optimal afﬁne maps ˜ g T,S , ˜ g S,T are both full-rank. For v ∈ R m , denote the matrix representation of ˜ g T,S as ˜ g T,S ( v ) = ˜ W T,S v + ˜ b T,S . Similarly, for w ∈ R d , denote the matrix representation of ˜ g S,T as",
    "˜ g S,T ( w ) = ˜": "W S,T w + ˜ b S,T . We have the following statements. If d < m , then ˜ g S,T is injective, and we have: ∥ f T − ˜ g T,S ◦ f S ∥ D , H T ≤ q ∥ ( ˜",
    "W ⊤": "⋆, ⋄ ˜ W ⋆, ⋄ ) − 1 ∥ F · ∥ H ⋄ ∥ F · ∥ f ⋆ − ˜ g ⋆, ⋄ ◦ f ⋄ ∥ 2 D , where the second inequality is by invoking Lemma E.3 . Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Taking the square root of this claim, and applying ( ⋄ = T, ⋆ = S ) or ( ⋄ = S, ⋆ = T ) , we immediatly have the ﬁrst two statements about the case of d < m or d > m . Finally, noting that when m = d , both ˜ g S,T and ˜ g T,S are bijective and thus also injective, we can see that both ( 19 ) and ( 20 ) stand.",
    "C. Proofs in Section 2": "In this section, we present proofs for Proposition 2.1 and Proposition 2.2 .",
    "C.1. Proof of Proposition 2.1": "Proposition C.1 (Proposition 2.1 Restated) . The α f 1 → f 2 2 can be reformulated as ( α f 1 → f 2 2 ) 2 = E x 1 , x 2 [ θ f 1 → f 1 ( x 1 , x 2 ) θ f 1 → f 2 ( x 1 , x 2 )] ,",
    "E x [": "\\ ∆ f 1 → f 1 ( x ) \\ ∆ f 1 → f 2 ( x ) ⊤ ] 2 F . Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "E x 1 , x 2": "i.i.d. ∼D n X i,j =1 \u0010 ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) \u0011 · \u0010 ∆ ( i ) f ⋆ → f ⋄ ( x 1 )∆ ( j ) f ⋆ → f ⋄ ( x 2 ) ⊤ \u0011",
    "E x 1": "\u0014 \\ ∆ f 1 → f 1 ( x 1 ) \\ ∆ f 1 → f 2 ( x 1 ) ⊤ \u0015",
    "· E x 2": "\u0014 \\ ∆ f 1 → f 2 ( x 2 ) \\ ∆ f 1 → f 1 ( x 2 ) ⊤ \u0015\u0013 , (22) where the last equality is because that x 1 , x 2 are i.i.d. samples from the same distribution. Therefore, we can re-write the x 1 , x 2 to be the same x ∼D and realize that the two matrices are in fact the same one. ( 22 ) = tr \u0012",
    "E x": "\u0014 \\ ∆ f 1 → f 1 ( x ) \\ ∆ f 1 → f 2 ( x ) ⊤ \u0015",
    "· E x": "\u0014 \\ ∆ f 1 → f 2 ( x ) \\ ∆ f 1 → f 1 ( x ) ⊤ \u0015\u0013 =",
    "C.2. Proof of Proposition 2.2": "Proposition C.2 (Proposition 2.2 Restated) . The adversarial transferability metrics α f 1 → f 2 1 , α f 1 → f 2 2 and ( α 1 ∗ α 2 ) f 1 → f 2 are in [0 , 1] . Proof. Let us begin with α f 1 → f 2 1 ( x ) = ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 1 ,ϵ ( x ))) ℓ adv ( f 2 ( x ) , f 2 ( x + δ f 2 ,ϵ ( x ))) . Recall that ℓ adv ( · ) ≥ 0 , and the deﬁnition of adversarial attack:",
    "( x 1 )": "\\ ∆ f 1 → f 1 ( x 1 ) , α f 1 → f 2 1",
    "( x 2 )": "\\ ∆ f 1 → f 1 ( x 2 ) ⟩ θ f 1 → f 2 ( x 1 , x 2 ) = ⟨ \\ ∆ f 1 → f 2 ( x 1 ) , \\ ∆ f 1 → f 2 ( x 2 ) ⟩ . Similarly, as α f 1 → f 2 1 ( x ) ∈ [0 , 1] , we can see that θ f 1 → f 1 ( x 1 , x 2 ) θ f 1 → f 2 ( x 1 , x 2 ) ∈ [ − 1 , 1] , and hence E x 1 , x 2 [ θ f 1 → f 1 ( x 1 , x 2 ) θ f 1 → f 2 ( x 1 , x 2 )] ∈ [ − 1 , 1] . Noting that equation 24 is non-negative, we conclude that (( α 1 ∗ α 2 ) f 1 → f 2 ) 2 ∈ [0 , 1] . Since ( α 1 ∗ α 2 ) f 1 → f 2 itself is non-negative as well, we can see that ( α 1 ∗ α 2 ) f 1 → f 2 ∈ [0 , 1] . Therefore, the three adversarial transferability metrics are all within [0 , 1] . Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "D. Proofs in Section 3": "In this section, we prove the two theorems and the two propositions presented in section 3 , which are our main theories.",
    "D.1. Proof of Theorem 3.1": "We introduce two lemmas before proving Theorem 3.1 . Lemma D.1. The square of the gradient matching distance is min g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ 2",
    "D , H ⋆ = ∥∇ f ⊤": "⋆ ∥ 2 D , H ⋆ −⟨ P ⊤ H ⋆ P , J † ⟩ , where g ∈ G are afﬁne transformations, and",
    "P = E x ∼D": "\u0002 ∇ f ⋆ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003 ,",
    "J = E x ∼D": "\u0002 ∇ f ⋄ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003 . Proof. min g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ 2",
    "W": "E x ∈D ∥∇ f ⋆ ( x ) ⊤ − W ∇ f ⋄ ( x ) ⊤ ∥ 2",
    "⋆ − W ∇ f ⊤": "⋄ ∥ 2",
    "D , H ⋆": "E x ′ ∼D 1 ∥∇ h ( x ′ ) ⊤ ∥ 2",
    "where W is a matrix.": "We can see that ( 25 ) is a convex program, where the optimal solution exists in a closed-form form, as shown in the following. Denote l ( W ) = ∥∇ f ⋆ ( x ) ⊤ − W ∇ f ⋄ ( x ) ⊤ ∥ 2",
    "l ( W ) = E x ∼D": "\u0002 ∥∇ f ⋆ ( x ) ⊤ ∥ 2 H ⋆ + ∥ W ∇ f ⋄ ( x ) ⊤ ∥ 2 H ⋆ − 2 ⟨∇ f ⋆ ( x ) ⊤ , W ∇ f ⋄ ( x ) ⊤ ⟩ H ⋆ \u0003",
    "H ⋆ + tr": "\u0010",
    "∂ W = E x ∼D": "\u0002",
    "H ⋆": "! | {z } X + τ 2 β 2 . (38) Note that X can be reformulated to be the expectation of an induced distribution from x ∼D , since x ′ i is a pre-deﬁned function of x . Denote D 2 as the distribution induced by the following sampling process: ﬁrst, sample x ∼D ; then,",
    "∇ f ⋄ ( x )": "\u0003",
    "= 2 H ⋆ E x ∼D": "\u0002 W ∇ f ⋄ ( x ) ⊤ ∇ f ⋄ ( x ) −∇ f ⋆ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003",
    "= 2 H ⋆": "\u0000",
    "W E x ∼D": "\u0002 ∇ f ⋄ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003",
    "− E x ∼D": "\u0002 ∇ f ⋆ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003",
    "W such that": "∂l",
    "W is an optimal solution. Luckily,": "we can ﬁnd such solution easily by using pseudo inverse, i.e. , ˜",
    "W = E x ∼D": "\u0002 ∇ f ⋆ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003 \u0000",
    "E x ∼D": "\u0002",
    "= P J † ,": "(27)",
    "where we denote P = E x ∼D": "\u0002 ∇ f ⋆ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003",
    "and J = E x ∼D": "\u0002 ∇ f ⋄ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003 . We can verify that such ˜ W indeed make the partial derivative (equation 26 ) zero. In equation 26 , we have ˜",
    "= P J † J − P .": "(28) To continue, we can see from Lemma E.2 that ker( J ) ⊆ ker( P ) which means rowsp( P ) ⊆ rowsp( J ) , where ker( · ) denotes the kernel of a matrix, and rowsp( · ) denotes the row space of a matrix. Therefore, by deﬁnition of the pseudo-inverse, we can see that P J † J = P , i.e. , ( 28 ) = 0 , and hence ˜",
    "W is indeed the optimal solution.": "Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Plugging ( 27 ) into ( 25 ), we have the optimal value as ( 25 ) = l ( ˜",
    "H ⋆ ˜": "W ∇ f ⋄ ( x ) ⊤ ∇ f ⋆ ( x ) \u0011i = ∥∇ f ⊤ ⋆ ∥ 2",
    "W ⊤ \u0011": "− 2 tr \u0010",
    "D , H ⋆ + tr": "\u0000 H ⋆ P J † JJ † P ⊤ − 2 H ⋆ P J † P ⊤ \u0001 = ∥∇ f ⊤ ⋆ ∥ 2",
    "W P ⊤ \u0011": "= ∥∇ f ⊤ ⋆ ∥ 2",
    "D , H ⋆ − tr": "\u0000",
    "H ⋆ P J † P ⊤ \u0001": "= ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ −⟨ P ⊤ H ⋆ P , J † ⟩ . Next, we present another lemma to analyze the term P ⊤ H ⋆ P . Lemma D.2. In this lemma, we break down the matrix representation of P ⊤ H ⋆ P into pieces relating to the output deviation caused by the generalized adversarial attacks (deﬁned in equation 10 ) P ⊤ H ⋆ P = E x 1 , x 2 i.i.d. ∼D n X i,j =1 \u0010 ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) \u0011 · \u0010 ∆ ( i ) f ⋆ → f ⋄ ( x 1 )∆ ( j ) f ⋆ → f ⋄ ( x 2 ) ⊤ \u0011 . Proof. Denote a symmetric decomposition of the positive semi-deﬁnitive matrix H ⋆ as",
    "H ⋆ = T ⊤ T ,": "where T is of the same dimension of H ⋆ . We note that the choice of decomposition does not matter. Then, plugging in the deﬁnition of P , we can see that P ⊤ H ⋆ P = E x ∼D \u0002 ∇ f ⋄ ( x ) ⊤ ∇ f ⋆ ( x ) \u0003",
    "· T ⊤ T · E x ∼D": "\u0002 ∇ f ⋆ ( x ) ⊤ ∇ f ⋄ ( x ) \u0003",
    "· E x ∼D": "\" n X i =1",
    "δ f ⋆ ,ϵ ( x ) =": "arg max",
    "⋆ ( x ) ,": "we can observe that: 1. Σ ⋆ ( x ) is diagonalized singular values σ f ⋆ , H ⋆ ( x ) ; 2. The i th column of V ⋆ ( x ) is the i th generalized attack δ ( i ) f ⋆ ( x ) (deﬁned in equation 9 ); Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability 3. The i th column of U ⋆ ( x )Σ( x ) is T ∆ ( i ) f ⋆ → f ⋆ ( x ) where ∆ ( i ) f ⋆ → f ⋆ ( x ) is the output deviation (deﬁned in equation 10 ); 4. The i th column of ∇ f ⋄ ( x ) ⊤ V ⋆ ( x ) is the output deviation ∆ ( i ) f ⋆ → f ⋄ ( x ) (deﬁned in equation 10 ). With the four key observations, we can break down the Jacobian matrices as ∇ f ⋄ ( x ) ⊤ ∇ f ⋆ ( x ) T ⊤ = \u0010 ∆ (1) f ⋆ → f ⋄ ( x ) . . . ∆ ( n )",
    "f ⋆ → f ⋄ ( x )": "\u0011     ∆ (1) f ⋆ → f ⋆ ( x ) ⊤ T ⊤ ... ∆ ( n ) f ⋆ → f ⋆ ( x ) ⊤ T ⊤     = n X i =1 ∆ ( i ) f ⋆ → f ⋄ ( x )∆ ( i ) f ⋆ → f ⋆ ( x ) ⊤ T ⊤ . Therefore, plugging it into the equation 29 , we have",
    "( 29 ) = E x ∼D": "\" n X i =1 ∆ ( i ) f ⋆ → f ⋄ ( x )∆ ( i ) f ⋆ → f ⋆ ( x ) ⊤ T ⊤ #",
    "T ∆ ( i )": "f ⋆ → f ⋆ ( x )∆ ( i ) f ⋆ → f ⋄ ( x ) ⊤ #",
    "= E x 1 , x 2": "i.i.d. ∼D n X i,j =1 \u0010 ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) \u0011 · \u0010 ∆ ( i ) f ⋆ → f ⋄ ( x 1 )∆ ( j ) f ⋆ → f ⋄ ( x 2 ) ⊤ \u0011 , where the last equality is due to that ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) is a scalar value. Equipped with Lemma D.1 and Lemma D.2 , we are able to prove the Theorem 3.1 . Theorem D.1 (Theorem 3.1 Restated) . Given the target and source models f ⋆ , f ⋄ , where ( ⋆, ⋄ ) ∈{ ( S, T ) , ( T, S ) } , the gradient matching distance (equation 7 ) can be written as min g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ D , H ⋆ = 1 − E [ v ⋆, ⋄ ( x 1 ) ⊤ A ⋆, ⋄ 2 ( x 1 , x 2 ) v ⋆, ⋄ ( x 2 )] ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ · ∥ J † ∥ − 1",
    "T ∆ ( j )": "f ⋆ → f ⋆ ( x 2 )∆ ( j ) f ⋆ → f ⋄ ( x 2 ) ⊤ \u0011  ",
    "H ⋆ ⟩· ⟨": "\\ ∆ ( i ) f ⋆ → f ⋄ ( x 1 )",
    ", J †": "= ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ − E x 1 , x 2 i.i.d. ∼D n X i,j =1 \u0010 ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) \u0011 · ∆ ( i ) f ⋆ → f ⋄ ( x 1 )∆ ( j ) f ⋆ → f ⋄ ( x 2 ) ⊤ , J † = ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ − E x 1 , x 2 i.i.d. ∼D n X i,j =1 \u0010 ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) \u0011 · tr \u0010 ∆ ( i ) f ⋆ → f ⋄ ( x 1 )∆ ( j ) f ⋆ → f ⋄ ( x 2 ) ⊤ J † \u0011 = ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ − E x 1 , x 2 i.i.d. ∼D n X i,j =1 \u0010 ∆ ( i ) f ⋆ → f ⋆ ( x 1 ) ⊤ H ⋆ ∆ ( j ) f ⋆ → f ⋆ ( x 2 ) \u0011 | {z } X 1 · \u0010 ∆ ( i ) f ⋆ → f ⋄ ( x 1 ) ⊤ J † ∆ ( j ) f ⋆ → f ⋄ ( x 2 ) \u0011 | {z } X 2 . (30) As the generalized ﬁrst adversarial transferability A 1 is about the magnitude of the output deviation (deﬁned in equation 11 ), and we can separate the A 1 out from the above equation. Then, what left should be about the directions about the output deviation, which we will put into the matrix A 2 , i.e. , the generalized second adversarial transferability. Recall that the generalized the ﬁrst adversarial transferability is a n -dimensional vector A ⋆, ⋄",
    "1 ( x ) including the adversarial": "losses of all of the generalized adversarial attacks, where the i th element in the vector is",
    "X 2 = A ⋆, ⋄": "1 ( x 1 ) ( i ) A ⋆, ⋄ 1 ( x 2 ) ( j ) · ⟨ \\ ∆ ( i ) f ⋆ → f ⋄ ( x 1 )",
    "J † | H ⋄ · σ (1)": "f ⋄ , H ⋄ ( x 1 ) σ (1) f ⋄ , H ⋄ ( x 2 ) ∥ J † ∥ H ⋄ . Recall the ( i, j ) th entry of the matrix A 2 is",
    "− E x 1 , x 2": "i.i.d. ∼D h ( σ (1) f ⋄ , H ⋄ ( x 1 ) σ f ⋆ , H ⋆ ( x 1 ) ⊙ A ⋆, ⋄ 1 ( x 1 )) ⊤ A ⋆, ⋄ 2 ( x 1 , x 2 )( σ (1) f ⋄ , H ⋄ ( x 2 ) σ f ⋆ , H ⋆ ( x 2 ) ⊙ A ⋆, ⋄",
    "1 ( x 2 ))": "i",
    "∥ J † ∥ H ⋄ .": "(31) Denoting v ⋆, ⋄ ( x ) = σ (1) f ⋄ , H ⋄ ( x ) σ f ⋆ , H ⋆ ( x ) ⊙ A ⋆, ⋄",
    "1 ( x ) ,": "and rearranging equation 31 give us the Theorem 3.1 . Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "D.2. Proof of Proposition 3.1": "From the proof of Theorem 3.1 in the above subsection, we can see why this proposition holds. Proposition D.1 (Proposition 3.1 Restated) . In Theorem 3.1 , 0 ≤ E [ v ⋆, ⋄ ( x 1 ) ⊤ A ⋆, ⋄ 2 ( x 1 , x 2 ) v ⋆, ⋄ ( x 2 )] ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ · ∥ J † ∥ − 1",
    "⋆ ∥ D , H ⋆ .": "We can see that the ≤ 1 part stands, since min g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ D , H ⋆ is always non-negative. The ≥ 0 part can be proved by observing 1 − E [ v ⋆, ⋄ ( x 1 ) ⊤ A ⋆, ⋄ 2 ( x 1 , x 2 ) v ⋆, ⋄ ( x 2 )] ∥∇ f ⊤ ⋆ ∥ 2 D , H ⋆ · ∥ J † ∥ − 1",
    "⋆ ∥ D , H ⋆ = min": "g ∈ G ∥∇ f ⊤ ⋆ −∇ ( g ◦ f ⋄ ) ⊤ ∥ D , H ⋆ ≤∥∇ f ⊤ ⋆ −∇ (0 ◦ f ⋄ ) ⊤ ∥ D , H ⋆ = ∥∇ f ⊤",
    "D.3. Proof of Theorem 3.2": "We introduce two lemmas before proving Theorem 3.2 . Lemma D.3. Assume that function h ( · ) satisﬁes the β -smoothness under ∥· ∥ H ⋆ norm (Assumption 1 ), and assume there is a vector x 0 in the same space as x ∼D such that h ( x 0 ) = 0 . Given τ > 0 , there exists x ′ as a function of x such that ∥ x − x ′ ∥ 2 ≤ τ , and",
    "H ⋆ ≤ 2": "\u0010 ∥∇ h ( x ′ ) ⊤ ∥ 2 H ⋆ + β 2 ( ∥ x − x 0 ∥ 2 − τ ) 2 + \u0011",
    "· ∥ x − x 0 ∥ 2": "2 . (36) Note that we can have tighter but similar results if we keep the inf x 0 ∈ R n . However, by plugging in the radius B = inf",
    "+ ∥∇ h ( x − τ": "\\ ( x − x 0 )) ⊤ ∥ H ⋆ , where we deﬁne",
    "x ′ = x − τ": "\\",
    "( x − x 0 ) .": "By deﬁnition, in this case ∥ x ′ − x ∥ 2 ≤ τ as well. We then treat X : it can be bounded using β -smoothness, i.e. , X ≤ β ∥ x 0 + ξ ( x − x 0 ) − x + τ \\",
    "( x − x 0 )) ∥ 2": "= β ∥ τ \\ ( x − x 0 ) − (1 − ξ )( x − x 0 )) ∥ 2 = β τ − (1 − ξ ) · ∥ ( x − x 0 ) ∥ 2 = β ((1 − ξ ) · ∥ ( x − x 0 ) ∥ 2 − τ ) , where the last step is because we are exactly considering the case of (1 − ξ ) · ∥ ( x − x 0 ) ∥ 2 > τ . Therefore, combining the two cases together, we can write ∥∇ h ( x 0 + ξ ( x − x 0 )) ⊤ ∥ H ⋆ ≤ β ((1 − ξ ) · ∥ ( x − x 0 ) ∥ 2 − τ ) + + ∥∇ h ( x ′ ) ⊤ ∥ H ⋆ , where ∥ x − x ′ ∥≤ τ . Combining the above, we have ∥ h ( x ) ∥ H ⋆ ≤ \u0000 ∥∇ h ( x ′ ) ⊤ ∥ H ⋆ + β ( ∥ x − x 0 ∥ 2 − τ ) + \u0001 · ∥ x − x 0 ∥ 2 . Take the square on both sides, and apply the Cauchy-Schwarz inequality, we have the lemma proved.",
    "H ⋆ ≤": "2",
    "exists x ′": "i as a function of x for ∀ i ∈ [ n ] such that ∥ x − x ′ i ∥ 2 ≤ τ , and τ 2 · ∥∇ h ( x ) ⊤ ∥ 2",
    "H ⋆ ≤ 3": "n X i =1",
    "∥ h ( x ′": "i ) ∥ 2 H ⋆ + n ∥ h ( x ) ∥ 2",
    "H ⋆ + nτ 4 β 2": "! . Proof. Denote the dimension of x as n , and let U be an orthogonal matrix in R n × n , where we denote its column vectors as u i ∈ R n for i ∈ [ n ] . Applying the mean value theorem, there exists ξ i ∈ (0 , 1) such that h ( x + τ u i ) − h ( x ) = ∇ h ( x + τξ i u i ) ⊤ τ u i = τ \u0000 ∇ h ( x ) ⊤ u i + ( ∇ h ( x + τξ i u i ) ⊤ −∇ h ( x ) ⊤ ) u i \u0001 . Rearranging the equality, we have ∇ h ( x ) ⊤ u i = 1",
    "τ γ i ,": "Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability where we denote γ i = h ( x + τ u i ) − h ( x ) − τ ( ∇ h ( x + τξ i u i ) ⊤ −∇ h ( x ) ⊤ ) u i . Collecting each γ i for i ∈ [ n ] into a matrix Γ = [ γ 1 ... γ n ] , we can re-formulate the above equality as τ ∇ h ( x ) ⊤ U = Γ τ ∇ h ( x ) ⊤ = Γ U ⊤ , where the last equality is because that U is orthogonal. Taking the ∥· ∥ 2 H ⋆ on both sides, with some linear algebra manipulation we can derive τ 2 · ∥∇ h ( x ) ⊤ ∥ 2 H ⋆ = ∥ Γ U ⊤ ∥ 2",
    "= tr( H ⋆": "n X i =1",
    "γ i γ ⊤": "i ) = n X i =1",
    "tr( H ⋆ γ i γ ⊤": "i ) = n X i =1",
    "i H ⋆ γ i )": "= n X i =1",
    "H ⋆ .": "(40) Combining equation 38 , equation 39 and equation 40 , we have the second inequality proved. Hence, we have proved Theorem 3.2 .",
    "H ⋆ + τ 4 β 2 \u0001": "= 3 n X i =1 ∥ h ( x + τ u i ) ∥ 2 H ⋆ + 3 n ∥ h ( x ) ∥ 2 H ⋆ + 3 nτ 4 β 2 , where the inequality is done Cauchy-Schwarz inequality.",
    "Denoting x ′": "i = x + τ u i , we have the lemma proved. Theorem D.2 (Theorem 3.2 Restated) . Given a data distribution D and τ > 0 , there exist distributions D 1 , D 2 such that the type-1 Wasserstein distance W 1 ( D , D 1 ) ≤ τ and W 1 ( D , D 2 ) ≤ τ satisfying 1",
    "D , H ⋆ .": "(35) Denoting h := h ′ ⋆, ⋄ − b , we can see h ( x 0 ) = 0 . Therefore, h can be used to invoke Lemma D.3 . That is, there exists x ′ as a function of x such that ∥ x − x ′ ∥ 2 ≤ τ , and",
    "H ⋆ ≤ 2 E x ∼D": "\u0010 ∥∇ h ( x ′ ) ⊤ ∥ 2 H ⋆ + β 2 ( ∥ x − x 0 ∥ 2 − τ ) 2 + \u0011",
    "x 0 ∈ R n": "sup",
    "∥ x − x 0 ∥ 2": "we can make the presentation much more simpliﬁed without losing its core messages. That is, ( 36 ) ≤ 2 \u0010 E x ′ ∼D 1 ∥∇ h ( x ′ ) ⊤ ∥ 2 H ⋆ + β 2 ( B − τ ) 2 + \u0011 B 2 . Combining the above inequality and equation 35 , and noting that E x ∼D ∥ h ( x ) ∥ 2",
    "D 1 , H ⋆ ,": "we have ∥ h ⋆, ⋄ ∥ 2",
    "⋆, ⋄ − b ∥ 2": "D , H ⋆ = E x ∼D ∥ h ( x ) ∥ 2",
    "D , H ⋆ ≤ 2": "\u0010 ∥∇ h ′⊤ ⋆, ⋄ ∥ 2 D 1 , H ⋆ + β 2 ( B − τ ) 2 + \u0011 B 2 . Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability It remains to show the Wasserstein distance between D 1 and D . As x ′ is a function of the random variable x ∼D with ∥ x ′ − x ∥ 2 ≤ τ , and D 1 is the induced distribution of x ′ as a function of x , we can see that by the deﬁnition of type-1 Wasserstein distance between D and D 1 is bounded by τ . Denote J ( D , D ′ ) as the set of all joint distributions that have marginals D and D ′ , and recall the deﬁnition of type-1 Wasserstein distance is W 1 ( D , D 1 ) = inf J ∈ J ( D , D 1 ) Z ∥ x − x ′ ∥ 2 d J ( x , x ′ ) . Denote J 0 as the joint distribution such that in ( x , x ′ ) ∼J we always have x ′ being a function of x as how x ′ is deﬁned. We can see that W 1 ( D , D 1 ) = inf J ∈ J ( D , D 1 ) Z ∥ x − x ′ ∥ 2 d J ( x , x ′ ) ≤ Z ∥ x − x ′ ∥ 2 d J 0 ( x , x ′ ) ≤ Z τ d J 0 ( x , x ′ ) = τ. (37) Therefore, we have the ﬁrst inequality in the theorem proved . The second inequality. Invoking Lemma D.4 with h ⋆, ⋄ , and rearranging the inequality, we have 1 3 n ∥∇ h ⋆, ⋄ ( x ) ⊤ ∥ 2",
    "H ⋆ + 1": "2 ∥ h ⋆, ⋄ ( x ) ∥ 2",
    "τ 2 E x ∼D": "n X i =1 1 2 n ∥ h ⋆, ⋄ ( x ′ i ) ∥ 2",
    "x ′ = x": "with probability 1 2",
    "x ′ = x ′": "i with probability 1 2 n for ∀ i ∈ [ n ] . Therefore, we can write X as X = ∥ h ⋆, ⋄ ∥ 2",
    "D 2 , H ⋆ .": "(39) Similarly to equation 37 , it also holds that W 1 ( D , D 2 ) ≤ τ . To ﬁnally complete the proof, noting that ∥∇ h ′⊤ ⋆, ⋄ ∥ 2 D , H ⋆ is the minimum of this gradient distance (equation 34 ), we have ∥∇ h ′⊤ ⋆, ⋄ ∥ 2 D , H ⋆ ≤ E x ∼D ∥∇ h ⋆, ⋄ ( x ) ⊤ ∥ 2",
    "D.4. Proof of Theorem 3.3": "Theorem D.3 (Theorem 3.3 Restated) . The surrogate transfer loss ( 16 ) and the true transfer loss ( 17 ) are close, with an error of ∥ f T − y ∥ D , H T . −∥ f T − y ∥ D , H T ≤ ( 17 ) − ( 16 ) ≤∥ f T − y ∥ D , H T Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Proof. Let us begin by recall the deﬁnition of the surrogate transfer loss ( 16 ) and the true transfer loss ( 17 ). ( 16 ) := min g ∈ G ∥ f T − g ◦ f S ∥ D , H T ( 17 ) := min g ∈ G ∥ y − g ◦ f S ∥ D , H T . Denote ˜ g ′ := arg min g ∈ G ∥ f T − g ◦ f S ∥ D , H T ˜ g := arg min g ∈ G ∥ y − g ◦ f S ∥ D , H T . First, we show an upper bound for ( 16 ). ( 16 ) ≤∥ f T − ˜ g ◦ f S ∥ D , H T ≤∥ y − ˜ g ◦ f S ∥ D , H T + ∥ f T − y ∥ D , H T = ( 17 ) + ∥ f T − y ∥ D , H T , (41) where the last inequality is by triangle inequality. Similarly, we can derive its lower bound. ( 16 ) = ∥ f T − ˜ g ′ ◦ f S ∥ D , H T ≥∥ y − ˜ g ′ ◦ f S ∥ D , H T −∥ f T − y ∥ D , H T ≥ min g ∈ G ∥ y − g ◦ f S ∥ D , H T −∥ f T − y ∥ D , H T = ( 17 ) −∥ f T − y ∥ D , H T , (42) where the ﬁrst inequality is by triangle inequality. Combining equation 41 and equation 42 , we have the proposition proved.",
    "D.5. Proof of Theorem B.1": "Theorem D.4 (Theorem B.1 Restated) . Denote ˜ g T,S : R m → R d as the optimal solution of equation 16 , and ˜ g S,T : R d → R m as the optimal solution of equation 18 . Suppose the two optimal afﬁne maps ˜ g T,S , ˜ g S,T are both full-rank. For v ∈ R m , denote the matrix representation of ˜ g T,S as ˜ g T,S ( v ) = ˜ W T,S v + ˜ b T,S . Similarly, for w ∈ R d , denote the matrix representation of ˜ g S,T as ˜ g S,T ( w ) = ˜ W S,T w + ˜ b S,T . We have the following statements. If d < m , then ˜ g S,T is injective, and we have: ∥ f T − ˜ g T,S ◦ f S ∥ D , H T ≤ q ∥ ( ˜",
    "D , H ⋄ , we have": "∥ f ⋄ − ˜ g ⋄ ,⋆ ◦ f ⋆ ∥ 2 D , H ⋄ ≤∥ f ⋄ − ˜ g − 1 ⋆, ⋄ ◦ f ⋆ ∥ 2",
    "D , H ⋄ = E x ∼D": "\u0002 ∥ f ⋄ ( x ) − ˜ g − 1 ⋆, ⋄ ( f ⋆ ( x )) ∥ 2",
    "≤ E x ∼D": "h ∥ ( ˜",
    "E. Auxiliary Lemmas": "Lemma E.1 (Compatibility of ∥·∥ H and ∥·∥ 2 ) . Let H ∈ R m × m be a positive semi-deﬁnite matrix, and denote H = T ⊤ T as its symmetric decomposition with T ∈ R m × m . For W ∈ R m × n and v ∈ R n , we have ∥ W v ∥ H ≤∥ W ∥ H · ∥ v ∥ 2 . Proof.",
    "∥ W v ∥ 2": "H = v ⊤ W ⊤ T ⊤ T W v = ∥ T W v ∥ 2 2",
    "F · ∥ v ∥ 2": "2 , where ∥· ∥ F is the Frobenius norm. Then, we can continue as",
    "∥ T W ∥ 2": "F = tr( W ⊤ T ⊤ T W ) = tr( W ⊤ HW ) = ∥ W ∥ 2",
    "H .": "Combining the above two parts, we have the lemma proved. Lemma E.2 (Expectation Preserves the Inclusion Relationship Between Linear Spaces) . Given a distribution x ∼D in R n , we denote the associated probability measure as µ . Given linear maps M x : R n → R m and N x : R n → R d , noting that they are both functions of x , we have the following statement. ker \u0000",
    "x M x": "\u0001 . Lemma E.3 (Inverse an Injective Linear Map) . Given a full-rank injective afﬁne transformation g : R m → R d , we denote its matrix representation as g ( v ) = W v + b where v ∈ R m , W ∈ R d × m , b ∈ R d . The inverse of g is g − 1 : R d → R m deﬁned by g − 1 ( w ) := W † w − W † b for w ∈ R d , i.e. , g − 1 ◦ g is the identity function. Moreover, given a positive semi-deﬁnite matrix H , for ∀ v ∈ R m and ∀ w ∈ R d , we have q ∥ ( W ⊤ W ) − 1 ∥ F · ∥ H ∥ F · ∥ w − g ( v ) ∥ 2 ≥∥ v − g − 1 ( w ) ∥ H . Proof. First, let us verify that g − 1 ◦ g is the identity function. The conditions of g being full-rank and injective are equivalent to W being full-rank and d ≥ m . That is being said, W ⊤ W is invertible and W † = ( W ⊤ W ) − 1 W ⊤ . Therefore, for",
    ", we also have v ∈ ker": "\u0000",
    "Denote P := E x ∼D M ⊤": "x M x , and let v ∈ ker( P ) , we have",
    "P v = 0 .": "Noting that P is positive semi-deﬁnite, we have the following equivalent statements.",
    "v ∈ ker( P )": "⇐⇒",
    "v ⊤ P v = 0 ,": "where the ’ = ⇒ ’ direction is trivial, and the ’ ⇐ = ’ direction can be proved by decomposing P = T ⊤ T as two matrices and noting that",
    "v ⊤ T ⊤ T v = 0": "= ⇒",
    "∥ T v ∥ 2": "2 = 0 = ⇒",
    "T v = 0": "= ⇒",
    "= ⇒ P v = 0 .": "Therefore, we have",
    "v ⊤ P v = 0": "= ⇒",
    "x M x v": "\u0003 = 0 = ⇒",
    "∥ M x v ∥ 2": "2 d µ = 0 , which implies M x v = 0 almost everywhere w.r.t. µ . Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability Therefore, applying v to E x ∼D [ N ⊤",
    "v =": "Z",
    "x M x v d µ": "= Z a.e.",
    "which means v ∈ ker": "\u0000",
    "∀ v ∈ R m , we have": "g − 1 ◦ g ( v ) = W † ( W v + b ) − W † b = W † W v = ( W ⊤ W ) − 1 W ⊤ W v = v . That is, g − 1 ◦ g is indeed the identity function. Next, to prove the inequality, let us start from the right-hand-side of the inequality. ∥ v − g − 1 ( w ) ∥ H = ∥ g − 1 ◦ g ( v ) − g − 1 ( w ) ∥ H = ∥ W † ( g ( v ) − w ) ∥ H ≤∥ W † ∥ H · ∥ g ( v ) − w ∥ 2 , (43) where the inequality is done by applying Lemma E.1 . To complete the prove, we can see that",
    "∥ W † ∥ 2": "H = ∥ ( W ⊤ W ) − 1 W ⊤ ∥ 2 H = tr( W ( W ⊤ W ) − 1 H ( W ⊤ W ) − 1 W ⊤ ) = tr(( W ⊤ W ) − 1 H ( W ⊤ W ) − 1 W ⊤ W ) = tr(( W ⊤ W ) − 1 H ) = ⟨ ( W ⊤ W ) − 1 , H ⟩ ≤∥ ( W ⊤ W ) − 1 ∥ F · ∥ H ∥ F . (44) Plugging the square root of equation 44 into equation 43 , we have the lemma proved.",
    "F. Additional Details of Synthetic Experiments": "In this section, we complete the description of the settings and methods used in the synthetic experiments. Moreover, we report two additional sets of results in cross-architecture scenarios. In the main paper (section 4 ), the synthetic experiments are done on the setting where source models have the same architecture as the target model, i.e. , all the models are one-hidden-layer neural networks with width m = 100 . A natural question is what would the results be if using different architectures? That is, the architecture of the source models are different from the target model. To answer this question, we present two additional sets of synthetic experiments where the width of the source models is m = 50 or m = 200 , different from the target model (width m = 100 ). Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "(a) width= 50 , δ (1)": "f ⋆",
    "(b) width= 50 , δ (2)": "f ⋆",
    "(c) width= 200 , δ (1)": "f ⋆",
    "(d) width= 200 , δ (2)": "f ⋆ Figure 3. In this ﬁgure, ’width’ is the width of the source models (one-hidden-layer neural networks). As deﬁned in equation 9 , δ (1) f ⋆ corresponds to the regular adversarial attacks, while δ (2) f ⋆ the secondary adversarial attack. That is, δ (2) f ⋆ represents the other information in the adversarial transferring process compared with the ﬁrst. The x-axis shows the scale of perturbation t ∈ [0 , 1] that controls how much the source model deviates from its corresponding reference source model. There are in total 6 quantities reported. Speciﬁcally, α f T → f S 1 is black solid ; α f S → f T 1 is black dotted ; α f T → f S 2 is green solid ; α f S → f T 2 is green dotted ; the gradient matching loss is red solid ; and",
    "the knowledge transferability distance is blue solid .": "As we have presented in the main paper about the description of the methods and models used in this experiment, here we present the detailed description of the settings and the datasets being used. Settings. We follow the small- ϵ setting used in the theory, i.e. , the adversarial attack are constrained to a small magnitude, so that we can use its ﬁrst-order Talyor approximation. Dataset. Denote a radial basis function as φ i ( x ) = e −∥ x − µ i ∥ 2 2 / ( σ i ) 2 , and for each input data we form its corresponding M -dimensional feature vector as φ ( x ) = [ φ 1 ( x ) , . . . , φ M ( x )] ⊤ . We set the dimension of x to be 50 . For each radial basis function φ i ( x ) , i ∈ [ M ] , µ i is sampled from U ( − 0 . 5 , 0 . 5) 50 , and σ 2 i is sampled from U (0 , 100) . We use M = 100 radial basis functions so that the feature vector is 100 -dimensional. Then, we set the target ground truth to be y ( x ) = W φ ( x ) + b where W ∈ R 10 × 100 , b ∈ R 10 are sampled from U ( − 0 . 5 , 0 . 5) element-wise. We generate N = 5000 samples of x from a Gaussian mixture formed by 10 Gaussians with different centers but the same covariance matrix Σ = I . The centers are sampled randomly from U ( − 0 . 5 , 0 . 5) 50 . That is, the dataset D = { ( x i , y i ) } N i =1 consists of N = 5000 sample from the distribution, where x i is 50 -dimensional, y i is 10 -dimensional. The ground truth target y i are computed using the ground truth target function y ( x i ) . That is, we want our neural networks to approximate y ( · ) on the Gaussian mixture. Methods of Additional Experiments. Note that we have provided the detailed description of the methods used in the main paper synthetic experiments. Here, we present the methods for two additional sets of synthetic experiments, using the same dataset and settings, but different architectures. In the main paper, the source model and the target model are of the same architecture, and the source models are perturbed target model. Here, we use the same target model f T (width m = 100 ) trained on the dataset D , but two different architectures for source models. That is, the source models and the target model are of different width. To derive the source models, we ﬁrst train two reference source models on D with width m = 50 and m = 200 . For each of the reference models, denoting the weights of the model as W , we randomly sample a direction V where each entry of V is sampled from U ( − 0 . 5 , 0 . 5) , and choose a scale t ∈ [0 , 1] . Subsequently, we perturb the model weights of the clean source model as W ′ := W + t V , and deﬁne the source model f S to be a one-hidden-layer neural network with weights W ′ . Then, we compute each of the quantities we care about, including α 1 , α 2 from both f S → f T and f T → f S , the gradient matching distance (equation 7 ), and the actual knowledge transfer distance (equation 17 ). We use the standard ℓ 2 loss as the adversarial loss function. Results. We present four sets of result in Figure 3 . The indication relations between adversarial transferability and knowledge transferability can be observed in the cross-architecture setting. Moreover: 1. the metrics α 1 , α 2 are more meaningful if using the regular attacks; 2. the gradient matching distance tracks the actual knowledge transferability loss; 3. the directions of f T → f S and f S → f T are similar. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "G. Details of the Empirical Experiments": "All experiments are run on a single GTX2080Ti.",
    "G.1. Datasets": "G.1.1. I MAGE D ATASETS • CIFAR10 : 1 : it consists of 60000 32 × 32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. • STL10 : 2 : it consists of 13000 labeled 96 × 96 colour images in 10 classes, with 1300 images per class. There are 5000 training images and 8000 test images. 500 training images (10 pre-deﬁned folds), 800 test images per class. G.1.2. NLP D ATASETS • IMDB : 3 Document-level sentiment classiﬁcation on positive and negative movie reviews. We use this dataset to train the target model. • AG’s News (AG) : Sentence-level classiﬁcation with regard to four news topics: World, Sports, Business, and Sci- ence/Technology. Following Zhang et al. ( 2015 ), we concatenate the title and description ﬁelds for each news article. We use this dataset to train the source model. • Fake News Detection (Fake) : Document-level classiﬁcation on whether a news article is fake or not. The dataset comes from the Kaggle Fake News Challenge 4 . We concatenate the title and news body of each article. We use this dataset to train the source model. • Yelp : Document-level sentiment classiﬁcation on positive and negative reviews ( Zhang et al. , 2015 ). Reviews with a rating of 1 and 2 are labeled negative and 4 and 5 positive. We use this dataset to train the source model.",
    "G.2. Adversarial Trasnferability Indicating Knowledge Transferability": "G.2.1. I MAGE For all the models, both source and target, in the Cifar10 to STL10 experiment, we train them by SGD with momentumn and learning rate 0.1 for 100 epochs. For knowledge tranferability, we randomly reinitialize and train the source models’ last layer for 10 epochs on STL10. Then we generate adversarial examples with the target model on the validation set and measure the adversarial transferability by feeding these adversarial examples to the source models. We employ two adversarial attacks in this experiments and show that they achieve the same propose in practice: First, we generate adversarial examples by 50 steps of projected gradient descent and epsilon 0 . 1 (Results shown in Table 1 ). Then, we generate adversarial examples by the more efﬁcient FGSM with epsilon 0 . 1 (Results shown in Table 6 ) and show that we can efﬁciently identify candidate models without the expensive PGD attacks. To further visualize the averaged relation presented in Table 1 and 6 , we plot scatter plots Figure 5 and Figure 4 with per sample α 1 as x axis and per sample transfer loss as y axis. Transfer loss is the cross entropy loss predicted by the source model with last layer ﬁne-tuned on STL10. The Pearson score indicates strong correlation between adversarial transferability and knowledge transferability. We note that in the ﬁgures where we report per-sample α 1 , although ideally α 1 ∈ [0 , 1] , we can observe that for some samples they have α 1 > 1 due to the attacking algorithm is not ideal in practice. However, the introduced sample-level noise does not affect the overall results, e.g. , see the averaged results in our tables, or the overall correlation in these ﬁgures. G.2.2. NLP In the NLP experiments, to train source and target models, we ﬁnetune BERT-base models on different datasets for 3 epochs with learning rate equal to 5 e − 5 and warm-up steps equal to the 10% of the total training steps. For knowledge 1 https://www.cs.toronto.edu/˜kriz/cifar.html 2 https://cs.stanford.edu/˜acoates/stl10/ 3 https://datasets.imdbws.com/ 4 https://www.kaggle.com/c/fake-news/data Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1 2 0 2 4 6 8 10 transfer loss R:-0.51 Fully Connected LeNet AlexNet ResNet18 ResNet50 Figure 4. Distribution of per sample knowledge transfer loss and α 1 . The adversarial samples are generated by PGD. The Pearson score shows strong negative correlation between α 1 and the knowledge transfer loss. The higher the transfer loss is, the lower the knowledge transferability is, and the lower the α 1 is. 0.0 0.5 1.0 1.5 2.0 1 0 2 4 6 8 10 transfer loss R:-0.57 Fully Connected LeNet AlexNet ResNet18 ResNet50 Figure 5. Distribution of per-sample knowledge transfer loss and α 1 . The adversarial samples are generated by FGSM. The Pearson score shows negative strong correlation between α 1 and transfer loss. The higher the transfer loss is, the lower the knowledge transferability is, the lower the α 1 should be. tranferability, we random initialize the last layer of source models and ﬁne-tune all layers of BERT for 1 epoch on the targeted dataset (IMDB). Based on the test data from the target model, we generate 1 , 000 textual adversarial examples via the state-of-the-art adversarial attacks T3 ( Wang et al. , 2020 ) with adversarial learning rate equal to 0.2, maximum iteration steps equal to 100, and c = κ = 100 . G.2.3. A BLATION STUDIES ON CONTROLLING ADVERSARIAL TRANSFERABILITY We conduct series of experiments on controlling adversarial transferability between source models and target model by promoting their Loss Gradient Diversity. Demontis et al. ( 2019 ) shows that for two models f S and f T , the cosine similarity between their loss gradient vectors ∇ x ℓ f S and ∇ x ℓ f T could be a signiﬁcant indicator measuring two models’ adversarial transferability. Moreover, Kariyappa & Qureshi ( 2019 ) claims that adversarial transferability betwen two models could be well controlled by regularizing the cosine similairity between their loss gradient vectors. Inspired by this, we train several Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
    "G.3. Knowledge Trasnferability Indicating Adversarial Transferability": "G.3.1. I MAGE We follow the same setup in the previous image experiment for source model training, transfer learning as well as generation of adversarial examples. However, there is one key difference: Instead of generating adversarial examples on the target model and measuring adversarial transferability on source models, we generate adversarial examples on each source model and measure the adversarial transferability by feeding these adversarial examples to the target model. Similarly, we also visualize the results (Table 3 ) and compute the Pearson score. Due to the signiﬁcant noise introduced by per-sample calculation, the R score is not as signiﬁcant as ﬁgure 5 , but the trend is still correct and valid, which shows that higher knowledge transferability indicates higher adversarial transferability. 0.0 0.2 0.4 0.6 0.8 1.0 1 0 1 2 3 4 5 transfer loss R:-0.06 0% 25% 50% 75% 100% Figure 8. Distribution of per-sample knowledge transfer loss and α . The Pearson score shows negative strong correlation between α and transfer loss. The higher the loss is, the lower the knowledge transferability is, and the lower the α 1 is. Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability G.3.2. NLP We follow the same setup to train the models and generate textual adversarial examples as § G.2 in the NLP experiments. We note that to measure the adversarial transferability, we generate 1 , 000 adversarial examples on each source model based on the test data from the target model, and measure the adversarial transferability by feeding these adversarial examples to the target model. 0.00 0.25 0.50 0.75 1.00 1 0.75 0.80 0.85 0.90 0.95 1.00 transfer confidence R:0.267687 MR Yelp Fake AG Figure 9. Distribution of per-batch knowledge transfer conﬁdence and α 1 . The Pearson score shows positive correlation between α 1 and transfer conﬁdence. The higher the conﬁdence, the higher the knowledge transferability."
  },
  "tables": [
    {
      "page": 3,
      "table_index": 0,
      "content": [
        [
          "UncoveringtheConnectionsBetweenAdversarialTransferabilityandKnowledgeTransferabili\nα fS→fT(x)= ℓ a d v ( f T ( x ) , f T ( x + δ f S ,ϵ ( x ) ) ) α fS→fT=∥𝔼x∼𝒟[ΔfS→fS (x̂)ΔfS→fT (x̂)⊤]∥F Proposed Adversarial Transferability Metrics: ↵ 1 f?! f ⇧,\n1 ℓ a d v ( f T ( x ) , f T ( x + δ f T ,ϵ ( x ) ) ) 2 <latexit sha1_base64=\"Xi2meTwLj7m+xkt4UY182u0xvHg=\">AAACPXicdVDLSgMxFM34rPU16tJNsAgupMz4QJdFXbisYK3QqcOdTMYGk8yQZIQy9If8CX/BbV0K7sStW9M6C616IHA451xu7okyzrTxvKEzNT0zOzdfWaguLi2vrLpr61c6zRWhLZLyVF1HoClnkrYMM5xeZ4qCiDhtR3enI799T5Vmqbw0/Yx2BdxKljACxkqhexYAz3oQ+jdFEgbagMKBSbHlMQORyniwi8vI3r+R0K15dW8M/Jv4JamhEs3QfQnilOSCSkM4aN3xvcx0C1CGEU4H1SDXNANyB7e0Y6kEQXW3GF87wNtWiXGSKvukwWP1+0QBQuu+iGxSgOnpSW8k/uV1cpMcdwsms9xQSb4WJTnH9thRdThmihLD+5YAUcz+FZMeKCDGFvxjSyQGVVuKP1nBb3K1V/f364cXB7XGSVlPBW2iLbSDfHSEGugcNVELEfSAntAQPTuPzqvz5rx/RaeccmYD/YDz8Qk7w6+L</latexit>\nPractical Representatives Defined in se\nΔfS→fT (x)=f T(x+δfS,ϵ(x))−f T(x)\nAtta a c g k a g in e s n t e f r T ated Generalized Adversarial Transferability: A <latexit sha1_base64=\"U6PPhn+EobhyRxVG83nlmUX4j2E=\">AAACL3icbZDLSgMxFIYz9VbrbdSlm2ARXEiZqddlxY3LCvYCnbFkMmkbmmSGJCOUYd7El/AV3Ope3Ii48y1M21nY1gOBj/8/h3PyBzGjSjvOh1VYWl5ZXSuulzY2t7Z37N29pooSiUkDRyyS7QApwqggDU01I+1YEsQDRlrB8Gbstx6JVDQS93oUE5+jvqA9ipE2Ute+8AKeXmdd9yH1lEbyBHohRTwSYWZw6lUXva5ddirOpOAiuDmUQV71rv3thRFOOBEaM6RUx3Vi7adIaooZyUpeokiM8BD1ScegQJwoP538L4NHRglhL5LmCQ0n6t+JFHGlRjwwnRzpgZr3xuJ/XifRvSs/pSJONBF4uqiXMKgjOA4LhlQSrNnIAMKSmlshHiCJsDaRzmwJeFYyobjzESxCs1pxTyvnd2flWi2PpwgOwCE4Bi64BDVwC+qgATB4Ai/gFbxZz9a79Wl9TVsLVj6zD2bK+vkFTTuphw==</latexit> ? 1 , ⇧,A\nattack\nδfT,ϵ(x) f T ΔfT→fT (x)=f T(x+δfT,ϵ(x))−f T(x)\n<latexit sha1_base64=\"I8On65cxVfUwhGf4CHf1Kgn2JdQ=\">AAAB+nicbVA9TwJBEJ3DL8Qv1NJmIzGxIneK0ZJoY4mRrwQuZG/Zgw27e5fdPRNy8hNstbcztv4ZW3+JC1wh4EsmeXlvJjPzgpgzbVz328mtrW9sbuW3Czu7e/sHxcOjpo4SRWiDRDxS7QBrypmkDcMMp+1YUSwCTlvB6G7qt56o0iySdTOOqS/wQLKQEWys9Bj26r1iyS27M6BV4mWkBBlqveJPtx+RRFBpCMdadzw3Nn6KlWGE00mhm2gaYzLCA9qxVGJBtZ/OTp2gM6v0URgpW9Kgmfp3IsVC67EIbKfAZqiXvan4n9dJTHjjp0zGiaGSzBeFCUcmQtO/UZ8pSgwfW4KJYvZWRIZYYWJsOgtbAjEp2FC85QhWSfOi7F2Wrx4qpeptFk8eTuAUzsGDa6jCPdSgAQQG8AKv8OY8O+/Oh/M5b8052cwxLMD5+gVgEJRe</latexit> Bidirectional Indication Proved in sec\nattack\nattack Output deviations Gradient Matching Distance: m <latexit sha1_base64=\"tuX4tlWXWupehfwTrw6l630VpPI=\">AAACaXicbVFdaxQxFM2M9sP1a6svoi/BRaigy4xV7GNRwT5WcNvCZh1uMpltaD6GJCMs6fzNgq+++hd8MDM7gm29EDicc++5lxNaS+F8lv1I0lu3Nza3tu+M7t67/+DheOfRsTONZXzGjDT2lILjUmg+88JLflpbDopKfkLPP3b6yXdunTD6q1/VfKFgqUUlGPhIFeOaKKGLsCRCY6LAn1EaPrctJheYaKAScFUQ58F+I97Ur/+Su0vChGWdWApQRpcv+wZyUYTehoEMn9pXg2cVDtu1TVuMJ9k06wvfBPkAJmioo2L8k5SGNYprzyQ4N8+z2i8CWC+Y5O2INI7XwM5hyecRalDcLUKfTItfRKbElbHxaY979t+JAMq5laKxszvUXdc68n/avPHV/iIIXTeea7ZeVDUSe4O7mHEpLGderiIAZkW8FbMzsMB8/IwrW6hqRzGU/HoEN8Hxm2m+N3335e3k4MMQzzZ6hp6jXZSj9+gAHaIjNEMMXaLfyUaymfxKd9In6dN1a5oMM4/RlUonfwArA7wI</latexit> g 2 i G n kr f ?>\u0000r\nδfS,ϵ(x) f S ΔfS→fS (x)=f S(x+δfS,ϵ(x))−f S(x) Bidirectional Indication Proved in sec\nattack <latexit sha1_base64=\"poEHibOyk1t+2ITTavU2zPDYw8I=\">AAAB+nicbVA9TwJBEJ3DL8Qv1NJmIzGxIneK0ZJoY4lBPhK4kL1lDzbs7l1290zIyU+w1d7O2PpnbP0lLnCFgC+Z5OW9mczMC2LOtHHdbye3tr6xuZXfLuzs7u0fFA+PmjpKFKENEvFItQOsKWeSNgwznLZjRbEIOG0Fo7up33qiSrNIPppxTH2BB5KFjGBjpXrYq/eKJbfszoBWiZeREmSo9Yo/3X5EEkGlIRxr3fHc2PgpVoYRTieFbqJpjMkID2jHUokF1X46O3WCzqzSR2GkbEmDZurfiRQLrccisJ0Cm6Fe9qbif14nMeGNnzIZJ4ZKMl8UJhyZCE3/Rn2mKDF8bAkmitlbERlihYmx6SxsCcSkYEPxliNYJc2LsndZvnqolKq3WTx5OIFTOAcPrqEK91CDBhAYwAu8wpvz7Lw7H87nvDXnZDPHsADn6xdefJRd</latexit>\nAttack generated\nagainst fS Function Matching Distance: m i n f?\u0000 g\nΔfT→fS (x)=f S(x+δfT,ϵ(x))−f S(x) <latexit sha1_base64=\"QXPy59bBaNH7sAJ0AZjPPyEeEVM=\">AAACUHicbVDPaxNBFH6bWltjtbEevQwGoYcadrVFj6UK5hjBNIVMWN7OzqZDZ2aXmVkhTPdP6z/RW6961bs3nU0i2NYHw3x8P3iPL6uksC6Ob6LOxoPNh1vbj7qPd5483e092zu1ZW0YH7NSluYsQ8ul0HzshJP8rDIcVSb5JLv40OqTr9xYUeovblHxmcK5FoVg6AKV9iZUCZ36ORWaUIXuPMv8p6Yh9JIUKbUOzWtC5pQJw1oiF6hKndPL1C/dDKX/2Byso4UfNqtQk/b68SBeDrkPkjXow3pGae8bzUtWK64dk2jtNIkrN/NonGCSN11aW14hu8A5nwaoUXE788sCGvIqMDkpShOedmTJ/pvwqKxdqCw420PtXa0l/6dNa1e8n3mhq9pxzVaLiloSV5K2TZILw5mTiwCQGRFuJewcDTIXOr+1JVNNN5SS3K3gPjh9M0jeDo4+H/aPT9b1bMMLeAn7kMA7OIYhjGAMDK7gO/yAn9F19Cv63YlW1r8/PIdb0+n+AeR9tcs=</latexit> g 2 G k\nBidirectional Indication Proved in sec\nα 1 fT→fS(x)= ℓ ℓ a a d d v v ( ( f f S S ( ( x x ) ) , , f f S S ( ( x x + + δ δ f f T S , , ϵ ϵ ( ( x x ) ) ) ) ) ) α 2 fT→fS=∥𝔼x∼𝒟[ΔfT→fT (x̂)ΔfT→fS (x̂)⊤]∥F Knowledge Transfer Distance: m <latexit sha1_base64=\"mfENe/Ksp0evU51LZq0KnKmOZgE=\">AAACRHicbVDLSgMxFM34tr5GXboJFsGFlhkf6FJUUHCjaFVoypBJM21okhmSjFDifJQ/4S+IO3XtTtyKaZ2FrwOBwznncm9OnHGmTRA8ekPDI6Nj4xOTlanpmdk5f37hUqe5IrROUp6q6xhrypmkdcMMp9eZoljEnF7F3YO+f3VDlWapvDC9jDYFbkuWMIKNkyL/BAkmI9tGTEIksOnEsT0qCohuYQ+uQ9hGhCkCk+gc3UZ2kCCY28NirYwn9riIkDZYFZFfDWrBAPAvCUtSBSVOI/8FtVKSCyoN4VjrRhhkpmmxMoxwWlRQrmmGSRe3acNRiQXVTTv4dAFXnNKCSarckwYO1O8TFguteyJ2yf6h+rfXF//zGrlJdpuWySw3VJKvRUnOoUlhv0HYYooSw3uOYKKYuxWSDlaYGNfzjy2xKCqulPB3BX/J5UYt3Kxtn21V9/bLeibAElgGqyAEO2APHINTUAcE3IEH8ASevXvv1Xvz3r+iQ145swh+wPv4BJyhsio=</latexit> g 2 i G n k y \u0000 g\n(a)\n(b)",
          null
        ],
        [
          null,
          "]∥F Proposed Adversarial Transferability Metrics: ↵ 1 f?! f ⇧,\n<latexit sha1_base64=\"Xi2meTwLj7m+xkt4UY182u0xvHg=\">AAACPXicdVDLSgMxFM34rPU16tJNsAgupMz4QJdFXbisYK3QqcOdTMYGk8yQZIQy9If8CX/BbV0K7sStW9M6C616IHA451xu7okyzrTxvKEzNT0zOzdfWaguLi2vrLpr61c6zRWhLZLyVF1HoClnkrYMM5xeZ4qCiDhtR3enI799T5Vmqbw0/Yx2BdxKljACxkqhexYAz3oQ+jdFEgbagMKBSbHlMQORyniwi8vI3r+R0K15dW8M/Jv4JamhEs3QfQnilOSCSkM4aN3xvcx0C1CGEU4H1SDXNANyB7e0Y6kEQXW3GF87wNtWiXGSKvukwWP1+0QBQuu+iGxSgOnpSW8k/uV1cpMcdwsms9xQSb4WJTnH9thRdThmihLD+5YAUcz+FZMeKCDGFvxjSyQGVVuKP1nBb3K1V/f364cXB7XGSVlPBW2iLbSDfHSEGugcNVELEfSAntAQPTuPzqvz5rx/RaeccmYD/YDz8Qk7w6+L</latexit>\nPractical Representatives Defined in se\nGeneralized Adversarial Transferability: A? 1 , ⇧,A\n<latexit sha1_base64=\"U6PPhn+EobhyRxVG83nlmUX4j2E=\">AAACL3icbZDLSgMxFIYz9VbrbdSlm2ARXEiZqddlxY3LCvYCnbFkMmkbmmSGJCOUYd7El/AV3Ope3Ii48y1M21nY1gOBj/8/h3PyBzGjSjvOh1VYWl5ZXSuulzY2t7Z37N29pooSiUkDRyyS7QApwqggDU01I+1YEsQDRlrB8Gbstx6JVDQS93oUE5+jvqA9ipE2Ute+8AKeXmdd9yH1lEbyBHohRTwSYWZw6lUXva5ddirOpOAiuDmUQV71rv3thRFOOBEaM6RUx3Vi7adIaooZyUpeokiM8BD1ScegQJwoP538L4NHRglhL5LmCQ0n6t+JFHGlRjwwnRzpgZr3xuJ/XifRvSs/pSJONBF4uqiXMKgjOA4LhlQSrNnIAMKSmlshHiCJsDaRzmwJeFYyobjzESxCs1pxTyvnd2flWi2PpwgOwCE4Bi64BDVwC+qgATB4Ai/gFbxZz9a79Wl9TVsLVj6zD2bK+vkFTTuphw==</latexit>\nBidirectional Indication Proved in sec\nGradient Matching Distance: m <latexit sha1_base64=\"tuX4tlWXWupehfwTrw6l630VpPI=\">AAACaXicbVFdaxQxFM2M9sP1a6svoi/BRaigy4xV7GNRwT5WcNvCZh1uMpltaD6GJCMs6fzNgq+++hd8MDM7gm29EDicc++5lxNaS+F8lv1I0lu3Nza3tu+M7t67/+DheOfRsTONZXzGjDT2lILjUmg+88JLflpbDopKfkLPP3b6yXdunTD6q1/VfKFgqUUlGPhIFeOaKKGLsCRCY6LAn1EaPrctJheYaKAScFUQ58F+I97Ur/+Su0vChGWdWApQRpcv+wZyUYTehoEMn9pXg2cVDtu1TVuMJ9k06wvfBPkAJmioo2L8k5SGNYprzyQ4N8+z2i8CWC+Y5O2INI7XwM5hyecRalDcLUKfTItfRKbElbHxaY979t+JAMq5laKxszvUXdc68n/avPHV/iIIXTeea7ZeVDUSe4O7mHEpLGderiIAZkW8FbMzsMB8/IwrW6hqRzGU/HoEN8Hxm2m+N3335e3k4MMQzzZ6hp6jXZSj9+gAHaIjNEMMXaLfyUaymfxKd9In6dN1a5oMM4/RlUonfwArA7wI</latexit> g 2 i G n kr f ?>\u0000r\nBidirectional Indication Proved in sec\nFunction Matching Distance: m i n f?\u0000 g\n<latexit sha1_base64=\"QXPy59bBaNH7sAJ0AZjPPyEeEVM=\">AAACUHicbVDPaxNBFH6bWltjtbEevQwGoYcadrVFj6UK5hjBNIVMWN7OzqZDZ2aXmVkhTPdP6z/RW6961bs3nU0i2NYHw3x8P3iPL6uksC6Ob6LOxoPNh1vbj7qPd5483e092zu1ZW0YH7NSluYsQ8ul0HzshJP8rDIcVSb5JLv40OqTr9xYUeovblHxmcK5FoVg6AKV9iZUCZ36ORWaUIXuPMv8p6Yh9JIUKbUOzWtC5pQJw1oiF6hKndPL1C/dDKX/2Byso4UfNqtQk/b68SBeDrkPkjXow3pGae8bzUtWK64dk2jtNIkrN/NonGCSN11aW14hu8A5nwaoUXE788sCGvIqMDkpShOedmTJ/pvwqKxdqCw420PtXa0l/6dNa1e8n3mhq9pxzVaLiloSV5K2TZILw5mTiwCQGRFuJewcDTIXOr+1JVNNN5SS3K3gPjh9M0jeDo4+H/aPT9b1bMMLeAn7kMA7OIYhjGAMDK7gO/yAn9F19Cv63YlW1r8/PIdb0+n+AeR9tcs=</latexit> g 2 G k\nBidirectional Indication Proved in sec\n]∥F Knowledge Transfer Distance: min y g\ng 2Gk \u0000\n<latexit sha1_base64=\"mfENe/Ksp0evU51LZq0KnKmOZgE=\">AAACRHicbVDLSgMxFM34tr5GXboJFsGFlhkf6FJUUHCjaFVoypBJM21okhmSjFDifJQ/4S+IO3XtTtyKaZ2FrwOBwznncm9OnHGmTRA8ekPDI6Nj4xOTlanpmdk5f37hUqe5IrROUp6q6xhrypmkdcMMp9eZoljEnF7F3YO+f3VDlWapvDC9jDYFbkuWMIKNkyL/BAkmI9tGTEIksOnEsT0qCohuYQ+uQ9hGhCkCk+gc3UZ2kCCY28NirYwn9riIkDZYFZFfDWrBAPAvCUtSBSVOI/8FtVKSCyoN4VjrRhhkpmmxMoxwWlRQrmmGSRe3acNRiQXVTTv4dAFXnNKCSarckwYO1O8TFguteyJ2yf6h+rfXF//zGrlJdpuWySw3VJKvRUnOoUlhv0HYYooSw3uOYKKYuxWSDlaYGNfzjy2xKCqulPB3BX/J5UYt3Kxtn21V9/bLeibAElgGqyAEO2APHINTUAcE3IEH8ASevXvv1Xvz3r+iQ145swh+wPv4BJyhsio=</latexit>\n(b)"
        ]
      ]
    },
    {
      "page": 3,
      "table_index": 1,
      "content": [
        [
          "↵ f?! f ⇧,\n1",
          "↵ 2 f?! f ⇧"
        ]
      ]
    },
    {
      "page": 3,
      "table_index": 2,
      "content": [
        [
          "A? , ⇧,A\n1\n<latexit sha1_base64=\"U6PPhn+EobhyRxVG83nlmUX4j2E=\">AAACL3icbZDLSgMxFIYz9VbrbdSlm2ARXEiZqddlxY3LCvYCnbFkMmkbmmSGJCOUYd7El/AV3Ope3Ii48y1M21nY1gOBj/8/h3PyBzGjSjvOh1VYWl5ZXSuulzY2t7Z37N29pooSiUkDRyyS7QApwqggDU01I+1YEsQDRlrB8Gbstx6JVDQS93oUE5+jvqA9ipE2Ute+8AKeXmdd9yH1lEbyBHohRTwSYWZw6lUXva5ddirOpOAiuDmUQV71rv3thRFOOBEaM6RUx3Vi7adIaooZyUpeokiM8BD1ScegQJwoP538L4NHRglhL5LmCQ0n6t+JFHGlRjwwnRzpgZr3xuJ/XifRvSs/pSJONBF4uqiXMKgjOA4LhlQSrNnIAMKSmlshHiCJsDaRzmwJeFYyobjzESxCs1pxTyvnd2flWi2PpwgOwCE4Bi64BDVwC+qgATB4Ai/gFbxZz9a79Wl9TVsLVj6zD2bK+vkFTTuphw==</latexit>",
          "?,\n2⇧"
        ]
      ]
    },
    {
      "page": 3,
      "table_index": 3,
      "content": [
        [
          "m <latexit sha1_base64=\"tuX4tlWXWupehfwTrw6l630VpPI=\">AAACaXicbVFdaxQxFM2M9sP1a6svoi/BRaigy4xV7GNRwT5WcNvCZh1uMpltaD6GJCMs6fzNgq+++hd8MDM7gm29EDicc++5lxNaS+F8lv1I0lu3Nza3tu+M7t67/+DheOfRsTONZXzGjDT2lILjUmg+88JLflpbDopKfkLPP3b6yXdunTD6q1/VfKFgqUUlGPhIFeOaKKGLsCRCY6LAn1EaPrctJheYaKAScFUQ58F+I97Ur/+Su0vChGWdWApQRpcv+wZyUYTehoEMn9pXg2cVDtu1TVuMJ9k06wvfBPkAJmioo2L8k5SGNYprzyQ4N8+z2i8CWC+Y5O2INI7XwM5hyecRalDcLUKfTItfRKbElbHxaY979t+JAMq5laKxszvUXdc68n/avPHV/iIIXTeea7ZeVDUSe4O7mHEpLGderiIAZkW8FbMzsMB8/IwrW6hqRzGU/HoEN8Hxm2m+N3335e3k4MMQzzZ6hp6jXZSj9+gAHaIjNEMMXaLfyUaymfxKd9In6dN1a5oMM4/RlUonfwArA7wI</latexit> g 2 i G n kr f ?>\u0000r",
          "(g \u0000 f ⇧ )>kD ,H?"
        ]
      ]
    }
  ],
  "images": [
    "processed/images/2006.14512v4_page8_img0.png",
    "processed/images/2006.14512v4_page8_img1.png",
    "processed/images/2006.14512v4_page28_img0.png",
    "processed/images/2006.14512v4_page28_img1.png",
    "processed/images/2006.14512v4_page28_img2.png",
    "processed/images/2006.14512v4_page28_img3.png"
  ],
  "status": "completed"
}