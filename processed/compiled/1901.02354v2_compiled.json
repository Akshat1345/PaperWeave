{
  "metadata": {
    "title": "Geometrization of deep networks for the interpretability of deep   learning systems",
    "authors": [
      "Xiao Dong",
      "Ling Zhou"
    ],
    "abstract": "How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systems.",
    "published": "",
    "arxiv_id": "1901.02354v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.02354v2",
    "pdf_file": "data/pdfs/1901.02354v2.pdf",
    "pdf_filename": "1901.02354v2.pdf"
  },
  "processing_info": {
    "processed_at": "2025-10-28T10:14:58.237674",
    "sections_found": 3,
    "tables_found": 0,
    "images_found": 0
  },
  "sections_text": {
    "arXiv:1901.02354v2  [cs.LG]  13 Jan 2019": "1 Geometrization of deep networks for the interpretability of deep",
    "learning systems": "Xiao Dong, Ling Zhou Faculty of Computer Science and Engineering, Southeast University, Nanjing, China How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systems. Index Terms —deep networks, geometrization, physics, computation I. M OTIVATION As a general tool to solve complex problems, the thriving deep learning technology is showing its power in almost all research ﬁelds. But we are still lacking a general theoretical framework to to answer the following questions: Why does deep learning work so well? What’s the relationship between the structure of a deep network and its functionality? How to design a proper deep network structure for a given task? How can we predict and control the behaviour of a deep network during training? How can our brain construct efﬁcient network structures for different tasks with limited resources? In this work we propose a general framework to understand deep learning systems, the geometrization of deep networks. A. Why geometrization Our motivation to understand deep learning systems from a geometric perspective falls in three folds. Deep networks are physical The reason that deep learning is so powerful and universally effective in different ﬁelds is that deep networks reveal the structures of physical systems. That’s to say, deep networks are effective representations of physical systems and their evolutions. Besides the enormous examples of AI based applications on computer vision, nat- ural language processing and robot control, deep networks are also closely related with the fundamental laws of our world, for example the effective representation of many- body quantum systems, renormalization group and entan- glement renormalization, tensor networks and AdS/CFT duality. So we believe the effectiveness of deep networks has a fundamental physical origin. That is, the deep network is at least a replica of our physical world so that every physical system has a correspondent deep network representation. Deep networks may share the same structure of the physical world and they may obey the same rules. The great success of geometrization of physics inspires our idea that geometrization may also be the ultimate framework to understand deep networks and deep learning systems. Deep networks are computational programmes From the quantum computation point of view, any physical system can be regarded as being generated from an initial simple state by an unitary operation. Similarly the evolution of any physical system is also an unitary operation or equivalently a computation process. As effective descriptions of physical systems and their evolutions, deep networks are essentially computation processes and can be understood as computa- tional programmes to generate and evolve physical systems. Then the geometrization of quantum states and quantum computations also leads to the geometrization of deep learning systems. For example, quantum computation com- plexity has a clear geometric picture and concrete physi- cal meanings as discussed in complexity=action, complex- ity=volume, Hamiltonian complexity, tensor networks and the emergent spacetime structure from quantum information. Deep networks as optimal control and optimization systems Recently there are emerging efforts to formulate deep learning systems as either optimization or optimal control problems. It’s well-known that these are also closely related with geometry and physics. We will show this point with a concrete example of template image matching, which has a clear geometric picture as an optimization or an optimal control problem. All the above observations lead to the same conclusion, geometrization scheme may bring us new perspectives to understand deep networks and deep learning systems. What’s more, if the geometrization of deep networks can be accom- plished, this may also change our ways to understand the physical world, i.e. a physical world built by deep networks. Now we show how to build the geometrization of deep net- works and how this can help us to understand deep networks and deep learning. B. An abstract description of deep networks In order to establish the geometric picture of deep networks, we now give an abstract description of it. As mentioned above, deep networks are programmes or data processing systems, which can achieve a transformation from the input data space V in to the output data space V out . Normal deep learning tasks, such as feature extraction and generative models, are all mappings between different data spaces. And usually we prefer one of them to be a vector 2 space so that algebraic operations or classiﬁcations can be easily carried out on it. From the general computation point of view, a data processing or a computation system can be abstracted as a mapping C : V × G → V , where V is the space of data and G is the space of operations on data. A computation process is given by C ( v in , g ) = g ( v in ) = v out , where v i , s o ∈ V are the input and output data and g ∈ G is an operation or transformation on data. A programme or an algorithm is a realization of g , which is usually achieved by a series of simple primitive operations as in both classical and quantum computers. The structure of a deep network is essentially a parametric realization of g and the process of training is to ﬁnd the proper network parameters that achieve g . Here we would like to note that the parameters may also include part of the network structure so that network structure itself may also be learned during training. The key feature of deep networks, the deep structure means that g is realized by a discrete time sequence of transforma- tions { ¯ g n | n ∈ [0 , N ] , ¯ g 0 = Id, ¯ g N = g } , where Id stands for the identity transformation. In this transformation sequence, the n th step achieves an operation ¯ g n ◦ ¯ g − 1 n − 1 , which is usually a simple low complexity operation. To make an analytical study of deep learning networks, we introduce a continuous time ﬂow { g t | t ∈ [0 , 1] , g 0 = Id, g 1 = g } . The validity of this continuous ﬂow fundamentally lies in the continuous evolution of quantum states. This is to say, a discrete time model of quantum information processing system such as the quantum circuit model is essentially only an approximation of a continuous time quantum evolution. Similarly a discrete time deep network is only an approximation of a continuous ﬂow of transformation. Obviously the continuous time ﬂow of transformation has a geometric picture. It is a continuous curve in the space of transformation connecting the identity operation I and the target operation g . Accordingly for each input data v in , there is a curve in the data space given by { v i,t | t ∈ 0 , 1 , v 0 = v in , v 1 = v out } . The purpose of deep learning systems is to ﬁnd an optimal transformation ﬂow to realize the target transformation, where the correspondent collection of the trajectories of all the input data { v k in,t , v k in ∈ V in } should show a good shape, where a good shape means the trajectories should be smooth, stable and well distributed so that the the network has a good genearalization performace. We now focus on the continuous transformation curve g t , t ∈ [0 , 1] . If we regard the space of transformation G as a manifold, then we can build a Riemannian structure on it. We can deﬁne the time derivative of g t as ˙ g t = u t ◦ g t and a right invariant metric on the tangent space T G of G as < ˙ g t , ˙ g t > T G = < u t , u t > Id . Then we can calculate the length of the curve g t , t ∈ [0 , 1] , g 0 = I, g 1 = g as R 1 0 < u t , u t > dt , which is the algorithmic complexity of the realization g t , t ∈ [0 , 1] of g . Now we have a simple geometric picture of deep networks. The structure of a deep network and the metric determines the length of the curve. Network parameters and the metric determines the shape of the curve. The optimal realization of g under a constraint to minimize the length of the curve is the geodesic from Id to g . Of course, keen readers will argue that above geometric picture is too abstract for a quantitative or even a qualitative understanding of deep learning systems. In the remaining part of this paper, we will ﬁrstly give a solid example of the geometrization of deep networks by comparing deep networks with the geometry of image matching. Then we scratch a broader picture of the geometrization of deep networks by comparing deep networks with other physical systems in- cluding quantum information processing, quantum many-body systems, spacetime structure and general relativity. II. G EOMETRY OF IMAGE REGISTRATION Computational anatomy is a research ﬁeld to study the variability of anatomical shapes, where the comparison between shapes is the key issue. Mathematically a shape can be described by a function on a spatial space I : R n → R m , which we call an image I . Here n = 2 , 3 stands for a 2D or a 3D image. m = 1 and m > 1 mean scalar images and vector/tensor images. For two different shapes represented by correspondent images I 0 , I 1 , the task of image registration is to ﬁnd a transformation ϕ so that the difference between the target image I 1 and the transformed source image I 0 is minimized, i.e. min ϕ ∥ I 1 − I 0 ◦ ϕ ∥ . The details of the transformation I 0 ◦ ϕ depends on the type of the image I 0 . A. Diffeomorphic image registration: optimization vs opti- mal control Diffeomorphic image registration is a framework for shape comparison by modeling transformations between shapes as a smooth invertible function ϕ : R n → R n . For example the space of transformations of volumetric images can be taken as G = Diff ( R 3 ) , which is the diffeomorphism group of R 3 , and V = I ( R 3 ) as the space of volumetric images on R 3 . Deforming an image I 0 ∈ V by a transformation ϕ ∈ G is just the change of coordinate as I 0 ◦ ϕ . Following , image registration can be abstracted as a map G × V → V , where G is the group of diffeomorphic image transformations and V is the vector space of images. Large deformation diffeomorphic metric mapping (LDDMM) generates a deformation ϕ as a ﬂow ϕ u t of a time-dependent vector ﬁeld u t ∈ T e ( G ) = g so that ˙ ϕ u t = u t ◦ ϕ u t , ϕ u 0 = Id, ϕ u 1 = ϕ (1) The diffeomorphic matching of two images I 0 and I 1 with LDDMM is to ﬁnd a vector ﬁeld u t , t ∈ [0 , 1] to minimize the cost function E ( u t ) = Z 1 0 l ( u t ) 2 dt + β | I 1 − I o ◦ ϕ u 1 | 2 , ˙ ϕ u t = u t ◦ ϕ u t , ϕ u 0 = Id (2) Here the regularity on u t is a kinetic energy term l ( u t ) = 1 2 R 1 0 | u t | 2 dt with | u t | a norm on the vector ﬁeld deﬁned as | u t | 2 = ⟨ Lu t , u t ⟩ L 2 . The operator L is a positive self- adjoint differential operator, for example Lu t = u t − α 2 ∆ u t . Obviously the norm | u t | 2 = ⟨ Lu t , u t ⟩ L 2 deﬁnes a Riemannian metric on the manifold of the diffeomorphic transformation group Diff ( R n ) . The second term of E ( u t ) computes the difference between the transformed image I o ◦ ϕ u 1 and I 1 . 3 A necessary condition DE ( u t ) = 0 to minimize the cost function is that the vector ﬁeld u t should satisfy the Euler- Poincar´e (E-P) equation Lu t = − ϕ u 0 ,t I 0 ⋄ ϕ u 0 ,t ϕ u 1 , 0 π (3) where ϕ u s,t = ϕ u t ◦ ϕ u s − 1 , π := β ( ϕ u 0 ,t I 0 − I 1) ♭ ∈ V ∗ . The ♭ operator is deﬁned as ♭ : V → V ∗ , ⟨ u ♭ , v ⟩ V ∗ × V = ⟨ u, v ⟩ and ⋄ : T V ∗ → g ∗ , ⟨ I ⋄ π, u ⟩ g ∗ × g = ⟨ π, ζ u ( I ) ⟩ V ∗ × V is the momentum map. The E-P equation can also be given as d dt ∂l ( u t ) ∂u t = − ad ∗ u t ∂l ( u t ) ∂u t (4) where ∂l ( u t ) ∂u t is the momentum and ad ∗ : g → gl ( g ) is the coadjoint representation of the Lie algebra g of the Lie group G . For more details please refer to . In LDDMM framework, the curve satisfying the E-P equa- tion is found by a gradient descent algorithm, while the gradient is given by u t + Kϕ u 0 ,t I 0 ⋄ ϕ u 0 ,t ϕ u 1 , 0 π with K = L − 1 . The geometric picture of LDDMM is quite simple: LD- DMM ﬁnds a minimal length curve, i.e. a geodesic given by the E-P equation, in Diff ( R n ) connecting Id and ψ , which can transform the source I 0 to a near neighbour of the target image I 1 . Equivalently we can also induce a Riemannian structure on the image space V by the map G × V → V so that the geodesic on G leads to a geodesic on V. Here we point out that this is exactly the same as in the geometry of quantum computation that a Riemannian metric on the quantum operation group induces a Rieman- nian metric on the Hilbert space of quantum states. Another interesting observation is that the map G × V → V can also be understood as a typical computation system, where V is the data representation space and G is the data operation space. So in fact image registration and quantum computation essentially have the same abstract descriptions and geometric pictures. LDDMM based image registration is formulated as an optimization problem and solved by a gradient descent based optimization. The optimal solution ϕ t is parameterized by the time-dependent vector ﬁeld u t and the optimization procedure is a parameter estimation of u t . We can easily see this is very similar with the abstract model of deep networks we introduced above. An alternative framework of LDDMM is to formulate it as an optimal control problem, where the image registration procedure is regarded as a dynamical process. The state of the dynamical system is the transformed source image I 0 ◦ ϕ t and the vector ﬁeld u t is taken as the control signal to adjust the transformation ϕ t . The problem is then to minimize the energy function E ( u t , J 0 t , λ t , γ ) = Z 1 0 l ( u t ) + ⟨ λ t , ˙ J 0 t + ∇ I t · u t ⟩ dt (5) + < γ, J 0 0 − I 0 > + β | J 0 1 − I 1 | (6) where J 0 t = I 0 ◦ ϕ u 0 ,t , J 1 t = I 1 ◦ ϕ u 1 ,t and λ t , γ are the Lagrangian multipliers. This leads to the optimality conditions as follows ˙ J 0 t + ∇ J 0 t · u t = 0 (7) ˙ λ t + ∇· ( λ t · u t ) = 0 (8) u t + K ⋆ ∇ J 0 t λ t = 0 (9) J 0 0 = I 0 (10) λ 1 = β ( I 1 − J 0 1 ) (11) The optimization procedure is a bi-directional information ﬂow. Given the current control signal u t and initial values of J 0 0 = I 0 , the forward information ﬂow compute J 0 t for t ∈ [0 , 1] . In the backward adjoint ﬂow, we update λ t starting from λ 1 = β ( I 1 − J 0 1 ) and then u t can be updated by a gradient descent using both J 0 t and the adjoint variable λ t . We note that the gradient based update of u t here is in fact the same as the updating of u t in the optimization formulation to fulﬁll the E-P equation. But the idea of bi-directional adjoint computation is a new characteristic. This is different from the direct computation of gradient in the optimization formulation. The Lagrangian multiplier based formulation can lead to more general strategies for parameter optimization as will be shown later. B. Geodesic shooting In LDDMM, both the optimization and optimal control formulations aim to ﬁnd a geodesic by ﬁnding a vector ﬁeld u t satisfying the E-P equation. It’s well known that for a given Riemannian manifold, a geodesic is completely determined by the starting point and the initial velocity of the geodesic. So if our goal is to ﬁnd a geodesic, then the vector ﬁeld u t as a control signal is highly redundant since it can be completely determined by u 0 and the E-P equation. Geodesic shooting can ﬁnd the initial vector ﬁeld u 0 or equivalently the initial momentum Lu 0 with the E-P equation as an explicit constraint. Obviously here the geodesic shoot- ing is also formulated as an optimal control problem. The correspondent optimization procedure is also a bi-directional information ﬂow. Starting from the initial momentum and the E-P equation, the forward ﬂow updates the vector ﬁeld u t , the transformation ϕ u t and the transformed source image J 0 t = I 0 ◦ ϕ u t . The backward adjoint ﬂow updates the adjoint variables, the Lagrangian multipliers of the constraints, and ﬁnally the initial vector ﬁeld u 0 can be updated by a gradient descent. For more details of geodesic shooting and the related adjoint calculation, please refer to . The lesson we can draw from geodesic shooting is that, when the optimal conﬁguration of a subset of the parameters can be determined as a function of all the other parameters, this function can be regarded as a constraint and the optimization can be simpliﬁed as an optimal problem. Or in another word, when there exists explicit constraints among parameters, the optimization can be simpliﬁed. This may help to design the network structure in deep learning systems. This idea can be generalized to the case of a general optimal control, where explicit constraints among parameters should be respected. Then the Lagrangian multiplier based variational method will lead to a similar bi-directional information passing 4 algorithm which can be shown later to be closely related with deep learning systems. C. Semiproduct group and metamorphosis Metamorphosis is used to modify the original LDDMM or the geodesic shooting to support a second transformation ﬂow η t , v t = ϕ u t ˙ η t , which is used to change the image appearance of the template image I 0 so that the image transformation is a composition of both the coordinate transformation and the image appearance transformation. Essentially this is to replace the Lie group G with a semiproduct group. That’s to say, we are now working with a composite operation of multiple operations. Under the composite transformation, the image is trans- formed as ˙ J 0 t = v t + u t ◦ ϕ u t . This is a new constraint involving both the coordinate and image appearance transformations. Accordingly the energy function to be minimized includes both the kinetic energies of u t and v t . In another word, the Riemannian manifold of transformations is extended and a new composite metric is deﬁned. But basically the geometric picture is similar with the original LDDMM or the geodesic shooting framework. An alternative perspective of the meta- morphosis is that the transformation on the image appearance η t can be regarded as introducing noise on the image. The constraint on the kinetic energy of v t can be understood as to constrain the power of the noise. For more details of the idea of metamorphesis, please refer to . D. Summary of diffeomorphic image registration Image registration can be formalized by either energy based optimization or an optimal control problem. It has a clear geometric picture, where the optimal solution is a geodesic on the Riemannian manifold on the transformation space. The geodesic is represented by a parameterized model and is obtained by a parameter estimation procedure. What’s more, the image registration problem is closely related with the geometric mechanics in that they share lots of geometric structures. A complete description of the geometric structure of image registration is given in  and references therein. III. G EOMETRIC PICTURE OF DEEP LEARNING SYSTEMS To show the validness of the geometrization of deep net- works, we now compare the diffeomorphic image registration and deep learning systems to build a dictionary between correspondent concepts in both ﬁelds. Since image registra- tion has both a clear geometric and a physical (geometric mechanical) picture, we hope the dictionary will give us a new understanding of deep networks from both the geometric and physical points of view. Here we directly give a list of the content of the dictionary with a brief explaination. Interested readers can check the details by themselves. A. A dictionary between image registration and deep net- works (1) Network structure and G : Geometrically the network structure deﬁnes the space of possible solutions, which is a set of curves in the transformation space. Also the network struc- ture deﬁnes in which way this space is explored as explained in the relational inductive bias. Due to the limited complexity of the network and limited allowed operations, the network structure only represents a subset of all possible curves that can reach the target transformation ϕ in image registration or g in deep learning from the identical transformation Id . In fact the deep network structure deﬁnes the operation group G and the network parameters θ falls in its Lie algebra g of G . Normal CNNs are just discretisized transformation curves and the norms of network parameters θ along the curve can be roughly regarded as the non-uniform discretization step sizes. (2) Constraints and Riemannian metric : The network structure only deﬁnes the space of possible solutions. To ﬁnd the optimal solution by solving an optimization problem, we need to introduce constraints on the parameters of the network. Geometrically constraints can be regarded as Riemannian metrics on G deﬁned on the manifold of possible solutions encoded in the network structure and network parameters. Carefully adjusting constraints can change the curvature dis- tribution of the solution manifold and generally we prefer to work on a ﬂat manifold so that the optimal solution can be easily found. (3) Supervised training and landmark registration : Given the parametric description of the manifold of possible solutions and the Riemannian metric deﬁned by constraint, supervised training on a set of N labeled training data estimates the parameters u t ( θ ) to ﬁnd the optimal transformation curve g t to reach the desired target transformation. This can be understood to achieve a diffeomorphic image registration based on N pairs of landmarks on the image or to simultaneously match N images using the same diffeomorphic transformation. (4) Optimal deep networks and geodesics : In LDDMM, the optimal transformation is achieved by a geodesic on G determined by the E-P equation. In deep networks, an optimal network should also exist as a geodesic on the Riemannian manifold deﬁned by the network structure and constraints, which is the deep network with a minimal complexity. Accord- ingly, if the E-P equation of deep networks can be explicitly described as in LDDMM, then geodesic shooting can also be implemented on the optimization of deep networks. Since geodesic shooting only optimize the initial momentum of the geodesic, the training of an optimal deep network has a much smaller degree of freedom than a normal non-optimal deep network. Network distillation and network pruning are essentially both efforts to ﬁnd the optimal deep networks. (5) Back propagation and LDDMM : The back propagation based optimization of deep networks are essentially the same as the gradient descent based LDDMM optimization. (6) Neural ODE and optimal control framework : The optimal control based optimization procedure of LDDMM is essentially the same as the optimization used in neural ODE and other related works. 5 (7) Equilibrium propagation and geodesic shooting : Ben- gio’s equilibrium propagation share the same structure as geodesic shooting used in LDDMM framework. (8) Attention mechanism and semiproduct group : Es- sentially attention mechanism is a composition of multiple deep networks. It shares lots of similarity with the semiprod- uct group and metamorphoses in LDDMM framework since semiproduct group also plays with composite operations. In the semiproduct group case of LDDMM, the multiple operations are coupled and a generalized E-P equation can obtained to represent the optimal ﬂow. This indicates that theoretically we also have an optimal attention mechanism and the geodesic shooting scheme can be applied. (9) Generalization and Riemannian curve length : The generalization capability of deep networks is a key issue of the performance of deep networks. Usually generalization is described by a norm based factor, which can be understood as the complexity of the network. It has been found that deep networks have the tendency to reduce complexity during training and a lower complexity means a better generalization capability. In LDDMM, the complexity of the registration transformation is the length of the transformation curve evalu- ated using the Riemannian metric on G . In deep networks, we also have a correspondent network complexity using a special Riemannian metric, the Fisher-Rao metric. In fact this metric is closely related with general relativity, which is another evidence that deep networks have a deep physical origin. We will give more details on this point in the discussion section. (10) Batch normalization and geodesics : It’s well known that batch normalization can help the convergence of deep networks. From the geometrical point of view, since CNNs are just discretisized transformaition curves and the norms of θ are the discretization step sizes, BN can be regarded as an operation to adaptively adjust the ratio of thrown-away information (energy) along the curve. This is because CNNs are the same as the entanglement renormalization algorithm, which extracts global information by iteratively throwing away local information. By normalizing the data, BN aims to keep a constant speed of throwing away information along the network. In entanglement renormalization algorithms, this is to throw away a ﬁxed percentage of low amplitude states and keep only those high amplitude states so that the strong global information patterns are kept. This will result in a transforma- tion curve with an isometric-like property, which coincides the property of geodesics. So geometrically BN can be understood as a constraint to force the network to be a geodesic-alike curve. This geometric picture is the same as the conclusion of . In  the Hessian matrix was introduced, which is in fact the Fisher-Rao metric to evaluate the complexity of the deep network or the length of the transformation curve. If the network can be constrained by BN to form a geodesic-like curve, then it has the minimal curve length or equivalently the minimal deformation energy as in the geometry of image registration problem. Or BN forces the Fisher-Rao metric to vary smoothly along the network. Obviously a transformation with a minimal deformation energy or a smooth curve will have a smooth loss landscape, which coincides with a key conclusion of . (11) Training convergence and curvature : The conver- gence of deep networks highly depends on the back propa- gation of gradients along deep networks. From the geometric picture of LDDMM, we know that this is related with the curvature of the manifold since the curvature determines the stability of geodesics. In deep learning ﬁelds, random matrix based analysis shows that when the network has a dynamic isometric property, the forward and backward information can ﬂow freely along the network so that a better convergence can be achieved. In fact, isometry is exactly the property of a geodesic. So the dynamic isometric property of a deep network is essentially to say, the network is a geodesic on Euclidean manifold, i.e. a straight line. Similarly, batch normalization is essentially to adjust the curvature of the manifold by adjusting the Fisher-Rao metric along the network. (12) GAN and current based shape matching : The key goal of GAN is to approximate a distribution density. The main challenge of GAN is to ﬁnd a proper metric to measure the difference of distributions. This is why WGAN emerges as a break-through since it provides an efﬁcient metric for distribu- tions without one-to-one correspondence between samples of distributions. In LDDMM, there is also a way to compare two shapes without position correspondence using current based shape representation. It will be interesting to ﬁnd if there exists a correspondence between WGAN and currents. (13) Dropout and stachastic shape evolutions : As a so- lution to enhance the robustness of deep networks, dropout achieves its goal by adding perturbations on the network, either on the operation group G (dropout of neurons) or on the data space V by adding perturbation layers. In LDDMM framework, there are also similar shape registration methods by adding perturbations on either the momentum or the positions of landmarks on shapes. Obviously dropout aims to ﬁnd a curve that is robust against perturbations on either G or V . But how about a perturbation on the Lie algebra g ? We will also address this issue in the following section. (14) ResNet, Lie algebra,curvature and reparameteriza- tion of curves : ResNet as the most successful deep network structure shows a superior performance than normal CNNs. From a geometric point of view, the success of ResNet falls in that ResNet is a network running on the Lie algebra g , while normal CNNs run on G . This can be understood by taking the quantum computation as an analogue of CNNs, where normal CNNs try to construct an quantum algorithm by composing elementary unitary gates and ResNets achieve the same algorithm by ﬁnding the proper Hamiltonian. It’s well known that ResNet is essentially a differential equation, which perfectly matches the structure of LDDMM. For ResNets, the curvature along the network is much smoother than normal CNNs since the network parameters of ResNets are only weak perturbations and therefore can not lead to rough curvature change along the network. ResNets can also easily achieve reparameterization of the curve g t by just adjusting the ampli- tudes of the weights. In another word, compared with normal CNNs, ResNets run on a much smoother manifold and can approach a smooth geodesic much easier than normal CNNs. (15) Geometric structure of deep networks and Rie- 6 mannian structure on V : In deep learning ﬁelds, there are also works to explore the geometric structure of deep networks. These works are closely related with the geometrization of deep networks. But both of them are work- ing on the Riemannian geometry on V as described in  instead of on the Riemannian geometry on G . Since the geometry on V is induced by a projection from the geometry of G , a complete geometrization of deep networks should be accomplished on G and only the geometry of G can fully explore the dynamics of deep networks. This is only a partial list of the correspondence between the geometry of image registration and deep learning systems. We hope we have convinced readers to believe the geometrization of deep networks is a promising candidate for the interpreta- tion of deep learning systems. B. A concrete example Now we will give a concrete sample on how we can understand deep learning using the geometrization framework. In  the problem of how the training data will inﬂuence the prediction of a deep network was addressed. They considered a supervised training with n data points z i = x i , y i , i = 1 ....n and the cost function is L ( z, θ ) 1 n Σ n i =1 L ( z i , θ ) with θ as the network parameters. The optimal network conﬁguration is given by ˆ theta = arg min min theta L ( z, θ ) . The key results of  are two items to evaluate how the perturbation on the training data will inﬂuence the parameter ˆ theta and the loss at a test point z test given by I up,params ( z ) = − H − 1 ˆ θ ∇ θ L ( z, ˆ θ ) (12) I up,loss ( z, z test ) = −∇ θ L ( z test , ˆ θ ) H − 1 ˆ θ ∇ θ L ( z, ˆ θ ) (13) where H ˆ θ = 1 n P n i =1 ∇ 2 ˆ θ L ( z i , ˆ θ ) is the Hessian. To interpret the results using our geometrization framework, we can regard the supervised network training as an optimiza- tion problem following the formulation of image registration with a cost function E ( u t ) = Z 1 0 l ( θ ) dt + L ( z, θ ) (14) where l ( θ ) = ⟨ θ I ( θ ) θ ⟩ , I ( θ ) = P n i =1 [ ∇ θ L ( z i , θ ) ⊗ ∇ θ L ( z i , θ )] is the Fisher-Rao metric used in  to describe complexity of deep networks. Obviously the Fisher-Rao metric is essentially the same as the Hessian H ˆ θ since I ( θ ) = − H ˆ θ from information geometry. This can be understood as either to match n pairs of images simultaneously using diffeomorphic transformations on a higher dimensional space (due to the overparameterization of deep networks) or a landmark based image registration taking all the training data as paired landmarks on an image. The goal of the optimization is to ﬁnd a proper transforma- tion that can match the training data and also show good generalization performance. The Riemannian metric on the Riemannian manifold to measure the deformation energy of the transformation or the curve length or equivalently the complexity of the deep network is the Fisher-Rao metric of the deep network. From a physical point of view, it’s obvious to see that why the Fisher-Rao norm is used in  to represent the generalization capability. This is because a lower complexity network means a lower deformation energy and therefore a smoother image deformation ﬁeld. Of course for a landmark based image registration, a smooth deformation will have a better generalization performance. Comparing (14) with (2)(3)(4), we can observe that ∇ θ L ( z, ˆ θ ) in (12) is exactly the momentum in g ∗ of E-P equation and (12) is the correspondent vector in g . 13 is related with the angle between two vectors in g using the Fisher-Rao metric. We can easily draw a clear physical or a geometric picture of (12)(13). If the deep network is a geodesic under the Fisher-Rao metric, then roughly (12) indicates how the direction of the geodesic will be shifted with a perturbation of a landmark. (13) shows under a perturbation of a training landmark, how the direction of the trajectory of a test data z test transformed by the perturbed geodesic will be shifted. Of course generally deep networks are not geodesics. Then the networks in  are just normal non-geodesic curves. But still the above geometric picture holds approximately. The drawback of  is that it only consider a perturbation around the current conﬁguration ˆˆ θ so that the Fisher-Rao metric is ﬁxed by the network conﬁguration. Another more interesting work  tried to explore the complete dynamics of the Fisher-Rao metric by iteratively updating the weighting of training data and the network conﬁguration ˆ θ . Similar to  this can be formulated as an image registration problem give by E ( u t ) = Z 1 0 l ( ˆ θ ( ǫ )) dt + L ( z, θ, ) (15) with ǫ i , i = 1 , ..., n are the weights of the training data and l ( ˆ θ ( ǫ )) are deﬁned on the Fisher-Rao metric determined by the optimal network parameters ˆ θ ( ǫ ) which are ǫ dependent. What’s new here? The main difference with the image registration problem is that the Fisher-Rao metric on G is now data dependent! In another word, the Riemannian metric is not a ﬁxed background metric as in the image registration problem, instead the metric is emergent from the deep network itself. Readers with a physics background can immediately see that we have an analogue of this in physics. The data independent image registration is the Neutonian mechanics with a ﬁxed spacetime background and the data dependent deep network systems correspond to general relativity with a dynamic spacetime. What’s more, the Fisher-Rao metric used here is in fact closely related with general relativity since gravitation equation can be derived from it. So in (15) the network structure and data (information) are coupled just as spacetime and matter are coupled in general relativity. Following John Wheeler, in our physical world, spacetime tells matter how to move, matter tells spacetime how to curve. In deep networks, network tells data (information) how to move, data (information) tells network how to curve. We believe this is not just an analogue between the physical world and deep networks, this should be regarded as a general principle to design and understand deep networks. The key component of interpret deep networks is to understand how the network 7 structure and data information interact. That’s to say to ﬁnd the gravitation equation for deep networks. Here we point out, since the Fisher-Rao metric is data dependent, the optimal solution can not be written as the E-P equation with a ﬁxed Riemannian metric any more since the metric is also dynamic. In , the solution is approximated by a two-level gradient descent algorithm which updates the network parameter ˆ θ and sample weights ǫ iteratively. The ﬁnal solution is a critical point that ˆ θ is stable with respect to the perturbation of ǫ . At the critical point, the solution still satisﬁes the E-P equation with a Fisher-Rao metric determined by the network parameter ˆ θ . As a conclusion of this section, the geometrization frame- work tells us that (1)The Fisher-Rao based network complexity measure is an effective signature for the generalization prop- erty for deep networks since it’s a measure for the network complexity; (2)The network structure and data information are coupled just as matter and spacetime are coupled in general relativity and the ultimate law of deep networks is a gravitational equation of deep networks; (3)The optimal solution is a result of the competition between the two terms, the network complexity and training error, in (15). IV. D ISCUSSION Till now we have seen the validness of the geometrization framework on the interpretability of deep learning systems by showing that deep networks can correspond to geometrical mechanics and general relativity. The basic idea of geometriza- tion is that deep networks have correspondence in the physical world. Therefore we can regard deep networks as physical systems and ask the following questions: (1) Is there a GUT (grand uniﬁed theory) of deep networks? If there is a correspondence between deep networks and our physical world, then the ultimate interpretability of deep networks lies in ﬁnding the GUT of deep networks just as the interpretability of the physical world lies in the GUT of the physical world. It’s a common sense that the physical GUT is deﬁnitely a geometrical theory. So we believe geometrization should be the right roadmap for the interpretability problem of deep learning systems. Also the GUT of deep networks should have the same structure as the GUT of our physical world. Therefore even the GUT of our physical world is not available yet, exploring the similarity between physical systems and deep networks can provide guidelines for us to better understand deep networks. (2) Real physical systems obey a least action principle, is this also true for deep networks? We have seen that the geometry of image registration results in an optimal solution given by the E-P equation. But for deep networks, generally we are working with systems far from optimal. But still usually these non-optimal systems work well in practice. There are also works taking deep networks as general dynamic systems such as in neural ODE. Shall we investigate non-optimal deep networks as general information processing systems or should we stick to optimal deep networks since they are more physical? Our geometrization framework will deﬁnitely work better on the optimal deep networks. But non-optimal systems might not be properly geometrized. So maybe we should ﬁrst focus on understanding the optimal systems with clear geometric pictures. (3) What can we learn from the geometrization of physics? Till now we are only working with Riemannian structures as in the geometry of image registration. In fact the geometrization of physics is far beyond Riemannian structures. A natural extension is the ﬁbre bundle structure which plays a key role in gauge theory. Can we describe deep networks using ﬁbre bundles? Good candidates in deep networks that may be described by ﬁbre bundles are transfer learning, meta learning, neural Turing machines (NTM) and differentiable neural com- puters(DNC). They all aim to ﬁnd some kind of reconﬁgurable systems. In the language of Riemannian geometry, this usually means to reconﬁgure the metric of the system so that the optimal geodesic curves can be reconﬁgurable. A natural way to achieve this is to reconﬁgure the connection form on ﬁbre bundles. Roughly transfer learning can be understood as to transfer (part of) a geodesic to another task. Meta learning aims to ﬁnd some universal descriptions of different but similar geodesics. NTM and DNC mainly achieve the reﬁguration of systems by carefully changing their memories. We can see the memory can be understood as the ﬁbre bundle above the base space of the LSTM states. It’s interesting to check if NTM and DNC can be written as deﬁning a connection on their ﬁbre bundles. Another possibility is that ﬁbre bundles can be used to describe the coupling of multiple deep networks. It’s get- ting more and more obvious that complex tasks can only be achieved by coupling multiple deep networks just as in our human brains. In AI systems, typical coupled composite systems are GANs and attention. The coupling of systems leads to interactions between subsystems, just as interactions (forces) between physical systems. In physics, interactions are described by ﬁbre bundles. Accordingly interactions between coupled deep networks should also be described by ﬁbre bundles. The last but not the least, the coupling of multiple deep networks might be related with the existence of consciousness. We hypothesize that when multiple neural networks in our brains are coupled, the coupling may be achieved by an inde- pendent coupling system, which not only couples the multiple neural subsystems but also has its own latent state space and a stable dynamics. This independent coupling system may be the origin of our consciousness. If this is the case, then can our consciousness also be geometrized? (4) How to geometrize reinforcement (RF) learning sys- tems? Geometrically RF is essentially to learn the metric of G from the interaction with the system and then ﬁnd geodesics using the learned metric. Imitation learning can be understood to design a metric so that the expert’s action becomes a geodesic. Can we formulate a geometrization of these procedures? (5) What’s the curvature of the emergent Fisher-Rao metric in deep networks? If deep networks can be formulated as a dynamic system using emergent Fisher-Rao metric, we need to check what’s the curvature of this metric. Because the curvature will determine the stability of the geodesic. And the 8 metric is dependent on both the structure and the parameters of the network. Just as in general relativity, the solution spacetime can have either positive or negative curvature, deep networks may have the same problem. Taking CNN as an example, in quantum information ﬁeld the correspondent system is MERA or the entanglement renormalization algorithm which show a similar structure as CNN. We know that MERA builds a negative curvature geometry and is related with the famous AdS/CFT duality. Similarly the geometry of quantum computation, which is another analogues of image registration and CNNs also has an almost negative curvature, where the Riemannian metric used here is static just as in image registration. It’s reasonable to guess that CNN may also have a similar negative curvature. This might be an explaination of the existence of adversarial examples in CNN based classiﬁcation networks. (6) How to understand the overparameterization of deep networks? Overparameterization plays a key role in nowadays deep networks. It’s closely related with the training conver- gence, generalization and adversarial attacks. Geometrically this means to choose a higher dimensional group G to accom- plish the transformation. In physics we also meet overparam- eterization problems. For example, in quantum computation overparameterization means to achieve a quantum algorithm using auxiliary qubits. In tensor network representation of quantum states, overparameterization is closely related with the concepts of parent and uncle Hamiltonians. Overpa- rameterization not only brings a higher dimensional G but also a potentially more ﬂexible network structure. As we see above, the structure of deep networks will inﬂuence the curvature of the geometry built by the Fisher-Rao metric of networks. A complete understanding of the consequence of overparameterization is still needed. V. C ONCLUSIONS In this work, inspired by the geometrization of physics, we proposed a geometrization framework for the interpretabil- ity of deep learning systems. By comparing the geometry of image registration with deep networks, we showed that geometrization does bring us new pictures of deep networks. Under this framework, we also discussed some key problems for the understanding of deep learning systems. Our future work will be then to answer these questions. As a ﬁnal remark, besides the geometrization of physics to connect physics and geometry, currently there is a trend to understand physical laws from the computation point of view so that computational complexity starts to play a key role in physics. If we further bring deep networks into this game, we hope the interactions among physics, geometry, computation and deep networks may completely change our understanding of the world. A possible picture of our world may be: The world is an information processing (computation) system that generates our universe by a deep network of basic computational operators. The structure of the deep network is determined by the information structure of our universe. That’s to say the deep network is the optimal network to gen- erate the information pattern of our universe, i.e. a geodesic according to a certain Riemannian metric to measure the computational complexity. Physical laws are encoded in the correspondence between the geometric structure of the network and the information pattern of our universe. So still our world obeys a least action principle with the action is given by the computational complexity of the physical world. R EFERENCES  X. Gao and L. M. Duan. Efﬁcient representation of quantum many-body states with deep neural networks. Nature Communications , 8(1):662, 2017.  Glen Evenbly. Algorithms for tensor network renormalization. Phys.rev.b , 95(4), 2017.  Cdric Bny. Deep learning and the renormalization group. arxiv:1301.3124 , 2013.  G. Evenbly and G. Vidal. Tensor network states and geometry. Journal of Statistical Physics , 145(4):891–918, 2011.  Brian Swingle. Entanglement renormalization and holography. Physical Review D Particles and Fields , 86(6):–, 2009.  Patrick Hayden, Sepehr Nezami, Xiao Liang Qi, Nathaniel Thomas, Michael Walter, and Zhao Yang. Holographic duality from random tensor networks. Journal of High Energy Physics , 2016(11):9, 2016.  Brian Swingle. Constructing holographic spacetimes using entanglement renormalization. Physics , 2012.  Xiao Liang Qi. Exact holographic mapping and emergent space-time geometry. Physics, arXiv:1309.6282v1 , 2013.  Wen Cong Gan and Fu Wen Shu. Holography as deep learning. International Journal of Modern Physics D , 26:1743020, 2017.  J.S. Wu X. Dong and L. Zhou. How deep learning works –the geometry of deep learning. arXiv:1710.10784 , 2017.  M. Gu M. A. Nielsen, M. R. Dowling and A. C. Doherty. Quantum computation as geometry. Science 311,1133 , 2006.  H. Heydari. Geometric formulation of quantum mechanics. arXiv:1503.00238 , 2015.  Qi Chen Tian, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. arxiv:1806.07366 , 2018.  E Weinan, Jiequn Han, and Qianxiao Li. A mean-ﬁeld optimal control formulation of deep learning. arxiv:1807.01083v1 , 2018.  Laurent Younes. Shapes and Diffeomorphisms . 2010.  M. Bruveris, F. Gay-Balmaz, D. D. Holm, and T. S. Ratiu. The momentum map representation of images. Journal of Nonlinear Science , 21(1):115–150, 2011.  Martins Bruveris and Darryl D. Holm. Geometry of image registration: The diffeomorphism group and momentum maps. Fields Institute Communications , 73:19–56, 2013.  Mirza Faisal Beg, Michael I. Miller, Alain Trouve, and Laurent Younes. Computing large deformation metric mappings via geodesic ﬂows. 2004.  M. R. Dowling and M. A. Nielsen. The geometry of quantum computation. Quantum Information and Computation , 8(10):861–899, 2008.  G. L. Hart, C. Zach, and M. Niethammer. An optimal control approach for deformable registration. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops , 2013.  Vialard, FranoisXavier, Risser, Laurent, Rueckert, Daniel, Cotter, and J Colin. Diffeomorphic 3d image registration via geodesic shooting using an efﬁcient adjoint calculation. International Journal of Computer Vision , 97(2):229–241, 2012.  Darryl D. Holm, Alain Trouve, and Laurent Younes. The euler-poincare theory of metamorphosis. Quarterly of Applied Mathematics , 67(4):661– 685, 2008.  Darryl D. Holm, Tanya Schmah, and Cristina Stoica. Geometric mechanics and symmetry. Oxford University Press Oxford , (2):xvi+515, 2009.  Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vin´ıcius Flores Zambaldi, Mateusz Malinowski, Andrea Tac- chetti, David Raposo, Adam Santoro, Ryan Faulkner, C¸ aglar G¨ulc¸ehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. arxiv:1806.01261 , 2018.  Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in Computational Neuroscience , 11:24–, 2017. 9  Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. arxiv:1711.01530 , 2017.  Hiroaki Matsueda. Emergent general relativity from ﬁsher information metric. arXiv:1310.1831v2 , 2013.  Hiroaki Matsueda. Derivation of gravitational ﬁeld equation from entanglement entropy. arXiv:1408.5589v2 , 70, 2014.  Hiroaki Matsueda. Geodesic distance in ﬁsher information space and holographic entropy formula. arXiv:1408.6633v1 , 2014.  Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 31 , pages 2488–2498. Curran Associates, Inc., 2018.  Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoen- holz, and Jeffrey Pennington. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. arxiv:1806.05393 , 2018.  Stanley Durrleman, Xavier Pennec, Alain Trouv, and Nicholas Ayache. Statistical models of sets of curves and surfaces based on currents. Medical Image Analysis , 13(5):793–808, 2009.  Zhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, and Ping Wang. Adversarial noise layer: Regularize neural network by adding noise. 2018.  Alain Trouv and Franoisxavier Vialard. Shape splines and stochastic shape evolutions: A second order point of view. Quarterly of Applied Mathematics , 70(2):219–251, 2012.  Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29 , pages 3360–3368. 2016.  Michael Hauser and Asok Ray. Principles of riemannian geometry in neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 2807–2816. 2017.  Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 1885– 1894, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.  Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In Jennifer Dy and An- dreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 4334–4343, Stockholmsmssan, Stockholm Sweden, 10–",
    "Jul 2018": "C. Fernndez-Gonzlez, N. Schuch, M. M. Wolf, J. I. Cirac, and D. Prez- Garca. Frustration free gapless hamiltonians for matrix product states. Communications in Mathematical Physics , 333(1):299–333, 2015."
  },
  "tables": [],
  "images": [],
  "status": "completed"
}