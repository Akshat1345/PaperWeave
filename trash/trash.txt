End-to-End AI Research Assistant Agent (Brief Overview)

This project creates a modular, open-source AI system to help users find, analyze, and understand scientific research papers. It consists of three main components:

Scraping Agent: Automatically searches and fetches research papers and their metadata (title, authors, abstract, PDF link) from repositories like arXiv, based on the user‚Äôs topic or filters.

Compilation Agent (LLM): Parses and organizes each paper‚Äôs content, extracts key information (sections, tables, diagrams, equations), and generates intelligent, section-wise summaries using large language models.

RAG Module: Indexes all extracted content and enables interactive question-answering, retrieving relevant passages and generating accurate, source-grounded answers.

Overall, the system streamlines literature review and scientific discovery, providing accurate summaries and reliable, citation-backed answers for any research topic.



i've provided code for everything in this project

well the problem is the RAG is very very poor, it doesn't answer all the questions for eg a basic question like "compare methodology across all papers" or "what is the research gap" or anything else. how do i build an accurate rag system?

I WANT A PERFECT RAG WHICH ANSWERS ALL THE QUESTIONS ACCURATELY

also one more problem, when i research shouldn't the paper be indexed and questions to be answered corresponding the topic i research for? but on the terminal output, i see paper 73 indexed etc where i entered only 5 papers on the website

shouldn't the topics and answers should be according to the topic? correct me if i am wrong

also how do i access the knowledge graph? visualisation?



output:

Your Question

üîç Ask AI

üí° Answer:

I couldn't find relevant information in the indexed papers to answer this question. The papers may not cover this topic, or try rephrasing your question.



# requirements.txt
# Tested and optimized for Python 3.10
# Installation: pip install -r requirements.txt

# ========== CORE DEPENDENCIES ==========

# Web Framework
Flask==3.0.0
Werkzeug==3.0.1

# PDF Processing
PyMuPDF==1.23.8  # fitz
pdfplumber==0.10.3

# ArXiv API & Web
feedparser==6.0.10
requests==2.31.0
urllib3==2.1.0

# LLM Integration
ollama==0.1.6

# Utilities
python-dateutil==2.8.2

# ========== RAG & KNOWLEDGE GRAPH ==========

# Vector Database
chromadb==0.4.22

# Embeddings
sentence-transformers==2.3.1

# Knowledge Graph
networkx==3.2.1
matplotlib==3.8.2  # For graph visualization

# Text Processing
nltk==3.8.1
spacy==3.7.2

# ========== PRODUCTION (Optional) ==========
gunicorn==21.2.0

# ========== DEVELOPMENT (Optional) ==========
pytest==7.4.3
pytest-cov==4.1.0
black==23.12.1
flake8==6.1.0



# config.py - Centralized Configuration Management
import os
from dataclasses import dataclass, field
from typing import List

@dataclass
class Config:
    """Application configuration with sensible defaults."""
    
    # ========== SCRAPER SETTINGS ==========
    ARXIV_BASE_URL: str = "http://export.arxiv.org/api/query"
    ARXIV_MAX_RESULTS: int = 20
    ARXIV_RATE_LIMIT_DELAY: float = 3.0  # seconds between requests
    PDF_DOWNLOAD_TIMEOUT: int = 60  # seconds
    PDF_MAX_RETRIES: int = 3
    
    # Semantic Scholar API (for citation metrics)
    SEMANTIC_SCHOLAR_API: str = "https://api.semanticscholar.org/graph/v1"
    ENABLE_CITATION_FETCH: bool = True
    
    # ========== COMPILER SETTINGS ==========
    OLLAMA_MODEL: str = "llama3.2:latest"
    OLLAMA_TIMEOUT: int = 120  # seconds for LLM responses
    
    # PDF Processing limits
    PAGE_LIMIT: int = 30
    WORD_LIMIT: int = 20000
    
    # Feature flags
    ENABLE_EQUATIONS: bool = True
    ENABLE_REFERENCES: bool = True
    ENABLE_CAPTIONS: bool = True
    ENABLE_CACHING: bool = True
    
    # Summarization settings
    CHUNK_SIZE_WORDS: int = 500
    ENABLE_SECTION_SPECIFIC_PROMPTS: bool = True
    
    # ========== STORAGE SETTINGS ==========
    DATA_DIR: str = "data"
    PROCESSED_DIR: str = "processed"
    CACHE_DIR: str = "processed/cache"
    COMPILED_DIR: str = "processed/compiled"
    IMAGES_DIR: str = "processed/images"
    
    # Database
    DATABASE_PATH: str = "research_assistant.db"
    
    # ========== RAG SETTINGS ==========
    # Embedding Model
    EMBEDDING_MODEL: str = "all-MiniLM-L6-v2"  # Fast, 384 dimensions
    EMBEDDING_DEVICE: str = "cpu"  # or "cuda" if GPU available
    
    # Vector Database
    CHROMA_PERSIST_DIR: str = "processed/chroma_db"
    CHROMA_COLLECTION_NAME: str = "research_papers"
    
    # Chunking Strategy
    CHUNK_SIZE: int = 512  # tokens per chunk
    CHUNK_OVERLAP: int = 50  # overlap between chunks
    
    # Retrieval
    RAG_TOP_K_RESULTS: int = 10  # Number of chunks to retrieve
    RAG_SIMILARITY_THRESHOLD: float = 0.10  # Minimum similarity score (10% - filters low relevance)
    
    # Answer Generation
    RAG_MAX_CONTEXT_LENGTH: int = 3000  # Max tokens for LLM context
    RAG_TEMPERATURE: float = 0.2  # Lower = more factual
    
    # ========== KNOWLEDGE GRAPH SETTINGS ==========
    # Graph Storage
    GRAPH_DB_PATH: str = "processed/knowledge_graph.db"
    ENABLE_GRAPH_VISUALIZATION: bool = True
    GRAPH_EXPORT_DIR: str = "processed/graph_exports"
    
    # Relationship Detection
    MIN_CITATION_SIMILARITY: float = 0.7  # For linking papers
    EXTRACT_CONCEPTS: bool = True  # Extract key concepts from papers
    MAX_CONCEPTS_PER_PAPER: int = 10
    
    # Graph Analysis
    ENABLE_CENTRALITY_ANALYSIS: bool = True  # Find influential papers
    ENABLE_COMMUNITY_DETECTION: bool = True  # Find research clusters
    
    # ========== LOGGING ==========
    LOG_FILE: str = "research_assistant.log"
    LOG_MAX_BYTES: int = 10 * 1024 * 1024  # 10MB
    LOG_BACKUP_COUNT: int = 5
    LOG_LEVEL: str = "INFO"
    
    def __post_init__(self):
        """Create necessary directories on initialization."""
        for directory in [
            self.DATA_DIR,
            self.PROCESSED_DIR,
            self.CACHE_DIR,
            self.COMPILED_DIR,
            self.IMAGES_DIR,
            os.path.join(self.DATA_DIR, 'pdfs')
        ]:
            os.makedirs(directory, exist_ok=True)

# Global configuration instance
config = Config()

